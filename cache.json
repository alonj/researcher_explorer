{"43dfaad15130c8eed428e07e1f6804c2": {"authorId": "51028767", "start_year": 2021, "end_year": 2023, "papers": [{"title": "Transpose Attack: Stealing Datasets with Bidirectional Training", "authors": "Guy Amit, Mosh Levy, Yisroel Mirsky", "year": 2023, "url": "https://www.semanticscholar.org/paper/7151a1b98f519fad7fb0e481b283b158a743e29e", "abstract": "Deep neural networks are normally executed in the forward direction. However, in this work, we identify a vulnerability that enables models to be trained in both directions and on different tasks. Adversaries can exploit this capability to hide rogue models within seemingly legitimate models. In addition, in this work we show that neural networks can be taught to systematically memorize and retrieve specific samples from datasets. Together, these findings expose a novel method in which adversaries can exfiltrate datasets from protected learning environments under the guise of legitimate models. We focus on the data exfiltration attack and show that modern architectures can be used to secretly exfiltrate tens of thousands of samples with high fidelity, high enough to compromise data privacy and even train new models. Moreover, to mitigate this threat we propose a novel approach for detecting infected models.", "topic": "\"Deep Neural Network Security\""}, {"title": "Clever Hans or Neural Theory of Mind? Stress Testing Social Reasoning in Large Language Models", "authors": "Natalie Shapira, Mosh Levy, S. Alavi, Xuhui Zhou, Yejin Choi, Yoav Goldberg, Maarten Sap, Vered Shwartz", "year": 2023, "url": "https://www.semanticscholar.org/paper/ddcd2bcc809bd0c2755a4a9487473d61ac327c50", "abstract": "The escalating debate on AI\u2019s capabilities warrants developing reliable metrics to assess machine \u201cintelligence.\u201d Recently, many anecdotal examples were used to suggest that newer Large Language Models (LLMs) like ChatGPT and GPT-4 exhibit Neural Theory-of-Mind (N-ToM); however, prior work reached conflicting conclusions regarding those abilities. We investigate the extent of LLMs\u2019 N-ToM through an extensive evaluation of 6 tasks and find that while LLMs exhibit certain N-ToM abilities, this behavior is far from being robust. We further examine the factors impacting performance on N-ToM tasks and discover that LLMs struggle with adversarial examples, indicating reliance on shallow heuristics rather than robust ToM abilities. We caution against drawing conclusions from anecdotal examples, limited benchmark testing, and using human-designed psychological tests to evaluate models.", "topic": "\"Neural Theory-of-Mind in LLMs\""}, {"title": "Transferability Ranking of Adversarial Examples", "authors": "Mosh Levy, Y. Elovici, Yisroel Mirsky", "year": 2022, "url": "https://www.semanticscholar.org/paper/547323c1431e07cca6f00cc3d9927dfb9f18d0e4", "abstract": "Adversarial examples can be used to maliciously and covertly change a model\u2019s prediction. It is known that an adversarial example designed for one model can transfer to other models as well. This poses a major threat because it means that attackers can target systems in a blackbox manner. In the domain of transferability, researchers have proposed ways to make attacks more transferable and to make models more robust to transferred examples. However, to the best of our knowledge, there are no works which propose a means for ranking the transferability of an adversarial example in the perspective of a blackbox attacker. This is an important task because an attacker is likely to use only a select set of examples, and therefore will want to select the samples which are most likely to transfer. In this paper we suggest a method for ranking the transfer-ability of adversarial examples without access to the victim\u2019s model. To accomplish this, we de\ufb01ne and estimate the expected transferability of a sample given limited information about the victim. We also explore practical scenarios: where the adversary can select the best sample to attack and where the adversary must use a speci\ufb01c sample but can choose different perturbations. Through our experiments, we found that our ranking method can increase an attacker\u2019s success rate by up to 80% compared to the baseline (random selection without ranking).", "topic": "\"Adversarial Transferability Ranking\""}, {"title": "Ranking the Transferability of Adversarial Examples", "authors": "Mosh Levy, Guy Amit, Y. Elovici, Yisroel Mirsky", "year": 2022, "url": "https://www.semanticscholar.org/paper/89ea8203318f36b82ea6eb5bab9fd731fbb065f9", "abstract": "Adversarial transferability in blackbox scenarios presents a unique challenge: while attackers can employ surrogate models to craft adversarial examples, they lack assurance on whether these examples will successfully compromise the target model. Until now, the prevalent method to ascertain success has been trial and error\u2014testing crafted samples directly on the victim model. This approach, however, risks detection with every attempt, forcing attackers to either perfect their first try or face exposure. Our paper introduces a ranking strategy that refines the transfer attack process, enabling the attacker to estimate the likelihood of success without repeated trials on the victim\u2019s system. By leveraging a set of diverse surrogate models, our method can predict transferability of adversarial examples. This strategy can be used to either select the best sample to use in an attack or the best perturbation to apply to a specific sample. Using our strategy, we were able to raise the transferability of adversarial examples from a mere 20%\u2014akin to random selection\u2014up to near upper-bound levels, with some scenarios even witnessing a 100% success rate. This substantial improvement not only sheds light on the shared susceptibilities across diverse architectures but also demonstrates that attackers can forego the detectable trial-and-error tactics raising increasing the threat of surrogate-based attacks.", "topic": "\"Adversarial Transferability Ranking\""}, {"title": "The Security of Deep Learning Defences for Medical Imaging", "authors": "Mosh Levy, Guy Amit, Y. Elovici, Yisroel Mirsky", "year": 2022, "url": "https://www.semanticscholar.org/paper/abe10a9224ab825b8189ab84fdcc651d293f03b8", "abstract": "Deep learning has shown great promise in the domain of medical image analysis. Medical professionals and healthcare providers have been adopting the technology to speed up and enhance their work. These systems use deep neural networks (DNN) which are vulnerable to adversarial samples; images with imperceivable changes that can alter the model's prediction. Researchers have proposed defences which either make a DNN more robust or detect the adversarial samples before they do harm. However, none of these works consider an informed attacker which can adapt to the defence mechanism. We show that an informed attacker can evade five of the current state of the art defences while successfully fooling the victim's deep learning model, rendering these defences useless. We then suggest better alternatives for securing healthcare DNNs from such attacks: (1) harden the system's security and (2) use digital signatures.", "topic": "\"Deep Neural Network Security\""}, {"title": "FOOD: Fast Out-Of-Distribution Detector", "authors": "Guy Amit, Mosh Levy, Ishai Rosenberg, A. Shabtai, Y. Elovici", "year": 2021, "url": "https://www.semanticscholar.org/paper/80e74f5f049e9deed39568b86bdc0e00cd62504f", "abstract": "Deep neural networks (DNNs) perform well at classifying inputs associated with the classes they have been trained on, which are known as in-distribution inputs. However, out-of-distribution (OOD) inputs pose a great challenge to DNNs and consequently represent a major risk when DNNs are implemented in safety-critical systems. Extensive research has been performed in the domain of OOD detection. However, current state-of-the-art methods for OOD detection suffer from at least one of the following limitations: (1) increased inference time - this limits existing methods' applicability to many real-world applications, and (2) the need for OOD training data - such data can be difficult to acquire and may not be representative enough, thus limiting the ability of the OOD detector to generalize. In this paper, we propose FOOD - Fast Out-Of-Distribution detector - an extended DNN classifier capable of efficiently detecting OOD samples with minimal inference time overhead. Our architecture features a DNN with a final Gaussian layer combined with the log likelihood ratio statistical test and an additional output neuron for OOD detection. Instead of using real OOD data, we use a novel method to craft artificial OOD samples from in-distribution data, which are used to train our OOD detector neuron. We evaluate FOOD's detection performance on the SVHN, CIFAR-10, and CIFAR-100 datasets. Our results demonstrate that in addition to achieving state-of-the-art performance, FOOD is fast and applicable to real-world applications.", "topic": "\"Deep Neural Network Security\""}], "analysis": "1. **Adversarial Attacks on Deep Neural Networks**: Investigating methods to exploit vulnerabilities in neural networks, including data exfiltration and adversarial examples.\n\n2. **Data Exfiltration via Neural Networks**: Exploring how adversaries can use neural networks to secretly exfiltrate datasets from protected environments.\n\n3. **Transferability of Adversarial Examples**: Developing methods to rank and predict the transferability of adversarial examples in blackbox attack scenarios.\n\n4. **Adversarial Robustness in Medical Image Analysis**: Examining the vulnerabilities of deep learning systems in medical image analysis to adversarial attacks and proposing defenses.\n\n5. **Out-of-Distribution Detection**: Creating efficient methods for detecting out-of-distribution inputs in deep neural networks to enhance their reliability in safety-critical systems.", "author_name": "Mosh Levy", "coauthors_histogram": {"Guy Amit": 4, "Yisroel Mirsky": 4, "Natalie Shapira": 1, "S. Alavi": 1, "Xuhui Zhou": 1, "Yejin Choi": 1, "Yoav Goldberg": 1, "Maarten Sap": 1, "Vered Shwartz": 1, "Y. Elovici": 4, "Ishai Rosenberg": 1, "A. Shabtai": 1}, "paper_topics": {}, "cluster_trends": {"\"Deep Neural Network Security\"": {"2023": 1, "2022": 1, "2021": 1}, "\"Neural Theory-of-Mind in LLMs\"": {"2023": 1, "2021": 0, "2022": 0}, "\"Adversarial Transferability Ranking\"": {"2022": 2, "2021": 0, "2023": 0}}}, "469a898db05861c6914ef20a90f9b4a8": {"authorId": "2089067", "start_year": 2021, "end_year": 2024, "papers": [{"title": "Unsupervised Mapping of Arguments of Deverbal Nouns to Their Corresponding Verbal Labels", "authors": "A. Weinstein, Yoav Goldberg", "year": 2023, "url": "https://www.semanticscholar.org/paper/21322bfea2eaf3d439833768af5896c82c503515", "abstract": "Deverbal nouns are nominal forms of verbs commonly used in written English texts to describe events or actions, as well as their arguments. However, many NLP systems, and in particular pattern-based ones, neglect to handle such nominalized constructions. The solutions that do exist for handling arguments of nominalized constructions are based on semantic annotation and require semantic ontologies, making their applications restricted to a small set of nouns. We propose to adopt instead a more syntactic approach, which maps the arguments of deverbal nouns to the universal-dependency relations of the corresponding verbal construction. We present an unsupervised mechanism -- based on contextualized word representations -- which allows to enrich universal-dependency trees with dependency arcs denoting arguments of deverbal nouns, using the same labels as the corresponding verbal cases. By sharing the same label set as in the verbal case, patterns that were developed for verbs can be applied without modification but with high accuracy also to the nominal constructions.", "topic": "\"Advancements in NLP Systems\""}, {"title": "Conjunct Resolution in the Face of Verbal Omissions", "authors": "Royi Rassin, Yoav Goldberg, Reut Tsarfaty", "year": 2023, "url": "https://www.semanticscholar.org/paper/3b4b31f3541b3f334edf3685c17acee564f703d1", "abstract": "Verbal omissions are complex syntactic phenomena in VP coordination structures. They occur when verbs and (some of) their arguments are omitted from subsequent clauses after being explicitly stated in an initial clause. Recovering these omitted elements is necessary for accurate interpretation of the sentence, and while humans easily and intuitively fill in the missing information, state-of-the-art models continue to struggle with this task. Previous work is limited to small-scale datasets, synthetic data creation methods, and to resolution methods in the dependency-graph level. In this work we propose a conjunct resolution task that operates directly on the text and makes use of a split-and-rephrase paradigm in order to recover the missing elements in the coordination structure. To this end, we first formulate a pragmatic framework of verbal omissions which describes the different types of omissions, and develop an automatic scalable collection method. Based on this method, we curate a large dataset, containing over 10K examples of naturally-occurring verbal omissions with crowd-sourced annotations of the resolved conjuncts. We train various neural baselines for this task, and show that while our best method obtains decent performance, it leaves ample space for improvement. We propose our dataset, metrics and models as a starting point for future research on this topic.", "topic": "\"Advancements in NLP Systems\""}, {"title": "Hierarchy Builder: Organizing Textual Spans into a Hierarchy to Facilitate Navigation", "authors": "Itay Yair, Hillel Taub-Tabib, Yoav Goldberg", "year": 2023, "url": "https://www.semanticscholar.org/paper/d4fe24f56e3fceaee02b01d9c9c61d160ffaa730", "abstract": "Information extraction systems often producehundreds to thousands of strings on a specifictopic. We present a method that facilitatesbetter consumption of these strings, in an ex-ploratory setting in which a user wants to bothget a broad overview of what\u2019s available, and achance to dive deeper on some aspects. The sys-tem works by grouping similar items together,and arranging the remaining items into a hierar-chical navigable DAG structure. We apply themethod to medical information extraction.", "topic": "\"Advancements in NLP Systems\""}, {"title": "An Open-Source Gloss-Based Baseline for Spoken to Signed Language Translation", "authors": "Amit Moryossef, Mathias Muller, Anne Gohring, Zifan Jiang, Yoav Goldberg, Sarah Ebling", "year": 2023, "url": "https://www.semanticscholar.org/paper/fbbd76d71388f55ee6ac37a0b90b14d2f47f16a1", "abstract": "Sign language translation systems are complex and require many components. As a result, it is very hard to compare methods across publications. We present an open-source implementation of a text-to-gloss-to-pose-to-video pipeline approach, demonstrating conversion from German to Swiss German Sign Language, French to French Sign Language of Switzerland, and Italian to Italian Sign Language of Switzerland. We propose three different components for the text-to-gloss translation: a lemmatizer, a rule-based word reordering and dropping component, and a neural machine translation system. Gloss-to-pose conversion occurs using data from a lexicon for three different signed languages, with skeletal poses extracted from videos. To generate a sentence, the text-to-gloss system is first run, and the pose representations of the resulting signs are stitched together.", "topic": "\"Sign Language Translation and NLP Transformers\""}, {"title": "McPhraSy: Multi-Context Phrase Similarity and Clustering", "authors": "Amir D. N. Cohen, Hila Gonen, Ori Shapira, Ran Levy, Yoav Goldberg", "year": 2022, "url": "https://www.semanticscholar.org/paper/01a22ae0acf2387adb15e03cb3d91323681c0e40", "abstract": ",", "topic": "\"Abstract Analysis Techniques\""}, {"title": "LingMess: Linguistically Informed Multi Expert Scorers for Coreference Resolution", "authors": "Shon Otmazgin, Arie Cattan, Yoav Goldberg", "year": 2022, "url": "https://www.semanticscholar.org/paper/873c3846e26853d300ecf73a6e910ecf6a84e63c", "abstract": "Current state-of-the-art coreference systems are based on a single pairwise scoring component, which assigns to each pair of mention spans a score reflecting their tendency to corefer to each other. We observe that different kinds of mention pairs require different information sources to assess their score. We present LingMess, a linguistically motivated categorization of mention-pairs into 6 types of coreference decisions and learn a dedicated trainable scoring function for each category. This significantly improves the accuracy of the pairwise scorer as well as of the overall coreference performance on the English Ontonotes coreference corpus and 5 additional datasets.", "topic": "\"Advancements in NLP Systems\""}, {"title": "F-coref: Fast, Accurate and Easy to Use Coreference Resolution", "authors": "Shon Otmazgin, Arie Cattan, Yoav Goldberg", "year": 2022, "url": "https://www.semanticscholar.org/paper/c9b0fcfc9470318a56e7dedf4503186e3a343408", "abstract": "We introduce fastcoref, a python package for fast, accurate, and easy-to-use English coreference resolution. The package is pip-installable, and allows two modes: an accurate mode based on the LingMess architecture, providing state-of-the-art coreference accuracy, and a substantially faster model, F-coref, which is the focus of this work. F-coref allows to process 2.8K OntoNotes documents in 25 seconds on a V100 GPU (compared to 6 minutes for the LingMess model, and to 12 minutes of the popular AllenNLP coreference model) with only a modest drop in accuracy. The fast speed is achieved through a combination of distillation of a compact model from the LingMess model, and an efficient batching implementation using a technique we call leftover batching. https://github.com/shon-otmazgin/fastcoref", "topic": "\"Advancements in NLP Systems\""}, {"title": "On the Power of Saturated Transformers: A View from Circuit Complexity", "authors": "William Merrill, Yoav Goldberg, Roy Schwartz, Noah A. Smith", "year": 2021, "url": "https://www.semanticscholar.org/paper/1781517a91be8248dd0febad65211a1b6614e199", "abstract": "Transformers have become a standard architecture for many NLP problems. This has motivated theoretically analyzing their capabilities as models of language, in order to understand what makes them suc-cessful, and what their potential weaknesses might be. Recent work has shown that transformers with hard attention are quite limited in capacity, and in fact can be simulated by constant-depth circuits. However, hard attention is a restrictive assumption, which may complicate the relevance of these results for practical transformers. In this work, we analyze the circuit complexity of transformers with saturated attention : a generalization of hard attention that more closely captures the attention patterns learnable in practical transformers. We show that saturated transformers transcend the limitations of hard-attention transformers. With some minor assumptions, we prove that the number of bits needed to represent a saturated transformer memory vector is O (log n ) , which implies saturated transformers can be simulated by log-depth circuits. Thus, the jump from hard to saturated attention can be understood as increasing the transformer\u2019s effective circuit depth by a factor of O (log n ) .", "topic": "\"Sign Language Translation and NLP Transformers\""}], "analysis": "1. **Handling arguments of deverbal nouns using universal-dependency relations.**\n2. **Recovering omitted elements in verbal omissions within VP coordination structures.**\n3. **Categorization and scoring of mention-pairs in coreference resolution.**\n4. **Fast and efficient coreference resolution with the fastcoref package.**", "author_name": "Yoav Goldberg", "coauthors_histogram": {"A. Weinstein": 1, "Royi Rassin": 1, "Reut Tsarfaty": 1, "Itay Yair": 1, "Hillel Taub-Tabib": 1, "Amit Moryossef": 1, "Mathias Muller": 1, "Anne Gohring": 1, "Zifan Jiang": 1, "Sarah Ebling": 1, "Amir D. N. Cohen": 1, "Hila Gonen": 1, "Ori Shapira": 1, "Ran Levy": 1, "Shon Otmazgin": 2, "Arie Cattan": 2, "William Merrill": 1, "Roy Schwartz": 1, "Noah A. Smith": 1}, "paper_topics": {}, "cluster_trends": {"\"Advancements in NLP Systems\"": {"2023": 3, "2022": 2, "2021": 0, "2024": 0}, "\"Sign Language Translation and NLP Transformers\"": {"2023": 1, "2021": 1, "2022": 0, "2024": 0}, "\"Abstract Analysis Techniques\"": {"2022": 1, "2021": 0, "2023": 0, "2024": 0}}}, "a34f49f7c0468c7bb8366db1fbdd0387": {"authorId": "1699545", "start_year": 2022, "end_year": 2024, "papers": [{"title": "Don't throw away your value model! Generating more preferable text with Value-Guided Monte-Carlo Tree Search decoding", "authors": "Jiacheng Liu, Andrew Cohen, Ramakanth Pasunuru, Yejin Choi, Hannaneh Hajishirzi, Asli Celikyilmaz", "year": 2023, "url": "https://www.semanticscholar.org/paper/1671d70a135b1e28b3a9cbc830feaa9b0c57df32", "abstract": "Inference-time search algorithms such as Monte-Carlo Tree Search (MCTS) may seem unnecessary when generating natural language text based on state-of-the-art reinforcement learning such as Proximal Policy Optimization (PPO). In this paper, we demonstrate that it is possible to get extra mileage out of PPO by integrating MCTS on top. The key idea is not to throw out the value network, a byproduct of PPO training for evaluating partial output sequences, when decoding text out of the policy network. More concretely, we present a novel value-guided decoding algorithm called PPO-MCTS, which can integrate the value network from PPO to work closely with the policy network during inference-time generation. Compared to prior approaches based on MCTS for controlled text generation, the key strength of our approach is to reduce the fundamental mismatch of the scoring mechanisms of the partial outputs between training and test. Evaluation on four text generation tasks demonstrate that PPO-MCTS greatly improves the preferability of generated text compared to the standard practice of using only the PPO policy. Our results demonstrate the promise of search algorithms even on top of the aligned language models from PPO, and the under-explored benefit of the value network.", "topic": "\"Advanced Text Generation Techniques\""}, {"title": "Modular Transformers: Compressing Transformers into Modularized Layers for Flexible Efficient Inference", "authors": "Wangchunshu Zhou, Ronan Le Bras, Yejin Choi", "year": 2023, "url": "https://www.semanticscholar.org/paper/1a7763f30c97b5b052af36bcfe478f64dcb97986", "abstract": "Pre-trained Transformer models like T5 and BART have advanced the state of the art on a wide range of text generation tasks. Compressing these models into smaller ones has become critically important for practical use. Common neural network compression techniques such as knowledge distillation or quantization are limited to static compression where the compression ratio is fixed. In this paper, we introduce Modular Transformers, a modularized encoder-decoder framework for flexible sequence-to-sequence model compression. Modular Transformers train modularized layers that have the same function of two or more consecutive layers in the original model via module replacing and knowledge distillation. After training, the modularized layers can be flexibly assembled into sequence-to-sequence models that meet different performance-efficiency trade-offs. Experimental results show that after a single training phase, by simply varying the assembling strategy, Modular Transformers can achieve flexible compression ratios from 1.1x to 6x with little to moderate relative performance drop.", "topic": "\"Advancements in Language Model Compression and Reasoning\""}, {"title": "BotPercent: Estimating Twitter Bot Populations from Groups to Crowds", "authors": "Zhaoxuan Tan, Shangbin Feng, Melanie Sclar, Herun Wan, Minnan Luo, Yejin Choi, Yulia Tsvetkov", "year": 2023, "url": "https://www.semanticscholar.org/paper/1bc0dc96d745325d89ec5bee1da1541255e6d1eb", "abstract": "Twitter bot detection has become increasingly important in combating misinformation, identifying malicious online cam-paigns, and protecting the integrity of social media discourse. While existing bot detection literature mostly focuses on identifying individual bots, it remains underexplored how to estimate the proportion of bots within speci\ufb01c communities and social networks, which has great implications for both content moderators and day-to-day users. In this work, we propose community-level bot detection , a novel approach to estimating the amount of malicious interference in online communities by estimating the percentage of bot accounts. Speci\ufb01cally, we introduce BotPercent , an amalgamation of Twitter bot-detection datasets and feature, text, and graph-based models that overcome generalization issues in existing individual-level models, resulting in a more accurate community-level bot estimation. Experiments demonstrate that BotPercent achieves state-of-the-art community-level bot detection performance on the TwiBot-22 benchmark while showing great robustness towards the tampering of speci\ufb01c user features. Armed with BotPercent , we analyze bot rates in different Twitter groups and communities, such as all active Twitter users, users that interact with partisan news media, users that participate in Elon Musk\u2019s content moderation votes, and the political communities in different countries and regions. Our experimental results demonstrate that the existence of Twitter bots is not homogeneous, but rather a spatial-temporal distribution whose heterogeneity should be taken into account for content moderation, social media policy making, and more. The BotPercent implementation is available at https://github.com/TamSiuhin/BotPercent", "topic": "\"Advanced AI and NLP Research\""}, {"title": "SQuARe: A Large-Scale Dataset of Sensitive Questions and Acceptable Responses Created through Human-Machine Collaboration", "authors": "Hwaran Lee, Seokhee Hong, Joonsuk Park, Takyoung Kim, M. Cha, Yejin Choi, Byoung Pil Kim, Gunhee Kim, Eun-Ju Lee, Yong Lim, Alice Oh, San-hee Park, Jung-Woo Ha", "year": 2023, "url": "https://www.semanticscholar.org/paper/1bcca45111ae6c7a1e30f1372b0584cc9d24031f", "abstract": "The potential social harms that large language models pose, such as generating offensive content and reinforcing biases, are steeply rising. Existing works focus on coping with this concern while interacting with ill-intentioned users, such as those who explicitly make hate speech or elicit harmful responses. However, discussions on sensitive issues can become toxic even if the users are well-intentioned. For safer models in such scenarios, we present the Sensitive Questions and Acceptable Response (SQuARe) dataset, a large-scale Korean dataset of 49k sensitive questions with 42k acceptable and 46k non-acceptable responses. The dataset was constructed leveraging HyperCLOVA in a human-in-the-loop manner based on real news headlines. Experiments show that acceptable response generation significantly improves for HyperCLOVA and GPT-3, demonstrating the efficacy of this dataset.", "topic": "\"Advanced Text Generation Techniques\""}, {"title": "Commonsense Knowledge Transfer for Pre-trained Language Models", "authors": "Wangchunshu Zhou, Ronan Le Bras, Yejin Choi", "year": 2023, "url": "https://www.semanticscholar.org/paper/1c6c6a26d23e8343c6de06187818f0402c994812", "abstract": "Despite serving as the foundation models for a wide range of NLP benchmarks, pre-trained language models have shown limited capabilities of acquiring implicit commonsense knowledge from self-supervision alone, compared to learning linguistic and factual knowledge that appear more explicitly in the surface patterns in text. In this work, we introduce commonsense knowledge transfer, a framework to transfer the commonsense knowledge stored in a neural commonsense knowledge model to a general-purpose pre-trained language model. It first exploits general texts to form queries for extracting commonsense knowledge from the neural commonsense knowledge model and then refines the language model with two self-supervised objectives: commonsense mask infilling and commonsense relation prediction, which align human language with the underlying commonsense knowledge. Empirical results show that our approach consistently improves the model's performance on downstream tasks that require commonsense reasoning. Moreover, we find that the improvement is more significant in the few-shot setting. This suggests that our approach helps language models better transfer to downstream tasks without extensive supervision by injecting commonsense knowledge into their parameters.", "topic": "\"Commonsense Knowledge in AI\""}, {"title": "Master Computer Science A Multilingual Solution for Mental Health Evaluation with Full Context Components", "authors": "Kailai Sun, Dian Yu, Jianshu Chen, Dong Yu, Yejin Choi, Zhixing Tian, Yuanzhe Zhang, Kang Liu, Jun Zhao, S\u00f8ren Christian Winther Topp, D. \u00d8stergaard, A. Vaidyam, Hannah Wisniewski, J. Halamka, M. S. Kashavan, Shuailiang Zhang, Hai Zhao, Zhuosheng Yuwei Wu", "year": 2023, "url": "https://www.semanticscholar.org/paper/4d54bcc738b270c9a8736520107b1c7cf286b917", "abstract": "Text-based models for multi-purpose inspection and evaluation are absent in the mental health detection field. We propose a multilingual solution in the form of Multi-choice Machine Reading Comprehension (MMRC) tasks combined with modification of the network architecture, input format and training scheme for such a scenario. In our solution, we 1) introduce extra tokens for the identification of different components of an MMRC sample and reconstruct the whole sample as one input sequence, 2) insert a small transformer encoder network to fuse the information from every option and 3) transfer the modelling of relationships among all components obtained from high-resource language to low-resource languages. Through the above modifications, we input the text records, evaluation, and corresponding criteria in text form, and train the neural network to output a valid assessment. Experiment results suggest our methods can be efficient in the demand for adaptability and cross-language transfer.", "topic": "\"Commonsense Knowledge in AI\""}, {"title": "ArK: Augmented Reality with Knowledge Interactive Emergent Ability", "authors": "Qiuyuan Huang, J. Park, Abhinav Gupta, Pan Lu, Paul N. Bennett, Ran Gong, Subhojit Som, Baolin Peng, O. Mohammed, C. Pal, Yejin Choi, Jianfeng Gao", "year": 2023, "url": "https://www.semanticscholar.org/paper/5b678b4c4737c7d2e8ba98e211fed4834dd43732", "abstract": "Despite the growing adoption of mixed reality and interactive AI agents, it remains challenging for these systems to generate high quality 2D/3D scenes in unseen environments. The common practice requires deploying an AI agent to collect large amounts of data for model training for every new task. This process is costly, or even impossible, for many domains. In this study, we develop an infinite agent that learns to transfer knowledge memory from general foundation models (e.g. GPT4, DALLE) to novel domains or scenarios for scene understanding and generation in the physical or virtual world. The heart of our approach is an emerging mechanism, dubbed Augmented Reality with Knowledge Inference Interaction (ArK), which leverages knowledge-memory to generate scenes in unseen physical world and virtual reality environments. The knowledge interactive emergent ability (Figure 1) is demonstrated as the observation learns i) micro-action of cross-modality: in multi-modality models to collect a large amount of relevant knowledge memory data for each interaction task (e.g., unseen scene understanding) from the physical reality; and ii) macro-behavior of reality-agnostic: in mix-reality environments to improve interactions that tailor to different characterized roles, target variables, collaborative information, and so on. We validate the effectiveness of ArK on the scene generation and editing tasks. We show that our ArK approach, combined with large foundation models, significantly improves the quality of generated 2D/3D scenes, compared to baselines, demonstrating the potential benefit of incorporating ArK in generative AI for applications such as metaverse and gaming simulation.", "topic": "\"Advanced AI and NLP Research\""}, {"title": "Symbolic Chain-of-Thought Distillation: Small Models Can Also \u201cThink\u201d Step-by-Step", "authors": "Liunian Harold Li, Jack Hessel, Youngjae Yu, Xiang Ren, Kai-Wei Chang, Yejin Choi", "year": 2023, "url": "https://www.semanticscholar.org/paper/7a6a298efb965ce9a351a3212f6f536e94dbbb03", "abstract": "Chain-of-thought prompting (e.g., \u201cLet\u2019s think step-by-ste\u201d) primes large language models to verbalize rationalization for their predictions. While chain-of-thought can lead to dramatic performance gains, benefits appear to emerge only for sufficiently large models (beyond 50B parameters). We show that orders-of-magnitude smaller models (125M\u20141.3B parameters) can still benefit from chain-of-thought prompting. To achieve this, we introduce Symbolic Chain-of-Thought Distillation (SCoTD), a method to train a smaller student model on rationalizations sampled from a significantly larger teacher model. Experiments across several commonsense benchmarks show that: 1) SCoTD enhances the performance of the student model in both supervised and few-shot settings, and especially for challenge sets; 2) sampling many reasoning chains per instance from the teacher is paramount; and 3) after distillation, student chain-of-thoughts are judged by humans as comparable to the teacher, despite orders of magnitude fewer parameters. We test several hypotheses regarding what properties of chain-of-thought samples are important, e.g., diversity vs. teacher likelihood vs. open-endedness. We release our corpus of chain-of-thought samples and code.", "topic": "\"Advancements in Language Model Compression and Reasoning\""}, {"title": "Faith and Fate: Limits of Transformers on Compositionality", "authors": "Nouha Dziri, Ximing Lu, Melanie Sclar, Xiang Lorraine Li, Liwei Jian, Bill Yuchen Lin, Peter West, Chandra Bhagavatula, Ronan Le Bras, Jena D. Hwang, Soumya Sanyal, Sean Welleck, Xiang Ren, Allyson Ettinger, Za\u00efd Harchaoui, Yejin Choi", "year": 2023, "url": "https://www.semanticscholar.org/paper/7d97c17a75beb89f938eaac1d3ca60ac2245fb2e", "abstract": "Transformer large language models (LLMs) have sparked admiration for their exceptional performance on tasks that demand intricate multi-step reasoning. Yet, these models simultaneously show failures on surprisingly trivial problems. This begs the question: Are these errors incidental, or do they signal more substantial limitations? In an attempt to demystify transformer LLMs, we investigate the limits of these models across three representative compositional tasks -- multi-digit multiplication, logic grid puzzles, and a classic dynamic programming problem. These tasks require breaking problems down into sub-steps and synthesizing these steps into a precise answer. We formulate compositional tasks as computation graphs to systematically quantify the level of complexity, and break down reasoning steps into intermediate sub-procedures. Our empirical findings suggest that transformer LLMs solve compositional tasks by reducing multi-step compositional reasoning into linearized subgraph matching, without necessarily developing systematic problem-solving skills. To round off our empirical study, we provide theoretical arguments on abstract multi-step reasoning problems that highlight how autoregressive generations' performance can rapidly decay with\\,increased\\,task\\,complexity.", "topic": "\"Advancements in Language Model Compression and Reasoning\""}, {"title": "Champagne: Learning Real-world Conversation from Large-Scale Web Videos", "authors": "Seungju Han, Jack Hessel, Nouha Dziri, Yejin Choi, Youngjae Yu", "year": 2023, "url": "https://www.semanticscholar.org/paper/8c236be5cb8073cb3db317919ceb55130ab66dbe", "abstract": "Visual information is central to conversation: body gestures and physical behaviour, for example, contribute to meaning that transcends words alone. To date, however, most neural conversational models are limited to just text. We introduce Champagne, a generative model of conversations that can account for visual contexts. To train Champagne, we collect and release YTD-18M, a large-scale corpus of 18M video-based dialogues. YTD-18M is constructed from web videos: crucial to our data collection pipeline is a pretrained language model that converts error-prone automatic transcripts to a cleaner dialogue format while maintaining meaning.Human evaluation reveals that YTD-18M is more sensible and specific than prior resources (MMDialog [17], 1M dialogues), while maintaining visual-groundedness. Experiments demonstrate that 1) Champagne learns to conduct conversation from YTD-18M; and 2) when fine-tuned, it achieves state-of-the-art results on four vision-language tasks focused on real-world conversations. We release data, models, and code at https://seungjuhan.me/champagne.", "topic": "\"Advanced AI and NLP Research\""}, {"title": "Common Sense: The Dark Matter of Language and Intelligence", "authors": "Yejin Choi", "year": 2023, "url": "https://www.semanticscholar.org/paper/8e1ad6685630822baad5e28b8e63218d7ee9b6ef", "abstract": "Scale appears to be the winning recipe in today's leaderboards. And yet, extreme-scale neural models are (un)surprisingly brittle and make errors that are often nonsensical and even counterintuitive. In this talk, I will argue for the importance of knowledge, especially commonsense knowledge, as well as inference-time reasoning algorithms, and demonstrate how smaller models developed in academia can still have an edge over larger industry-scale models, if powered with knowledge and/or reasoning algorithms.", "topic": "\"Commonsense Knowledge in AI\""}, {"title": "ArK: Augmented Reality with Knowledge Emergent Infrastructure", "authors": "Qiuyuan Huang, J. Park, Abhinav Gupta, Pan Lu, Paul N. Bennett, Ran Gong, Subhojit Som, Baolin Peng, O. Mohammed, C. Pal, Yejin Choi, Jianfeng Gao", "year": 2023, "url": "https://www.semanticscholar.org/paper/8e6394886157578aae839f72bac933414eaa8b1b", "abstract": "Despite the growing adoption of mixed reality and interactive AI, it remains challenging to generate high-quality 2D/3D scenes in unseen environments. Typically, an AI agent requires collecting extensive training data for every new task, which can be costly or impossible for many domains. In this study, we develop an infinite agent that learns to transfer knowledge memory from general foundation models (e.g., GPT4, DALLE) to novel domains or scenarios for scene understanding and generation in physical or virtual worlds. Central to our approach is the interactive emerging mechanism, dubbed Augmented Reality with Knowledge Emergent Infrastructure (ArK) , which leverages knowledge-memory to generate scenes in unseen physical worlds and virtual reality environments. The knowledge interactive emergent ability (Figure 1) is demonstrated through i) micro-action of cross-modality : in multi-modality models to collect a large amount of relevant knowledge-memory data for each interaction task (e.g., unseen scene understanding) from the physical reality; and ii) macro-behavior of reality-agnostic : in mix-reality environments to improve interactions that tailor to different characterized roles, target variables, collaborative information, and so on. We validate ArK\u2019s effectiveness in scene generation and editing tasks and show that our ArK approach, combined with large foundation models, significantly improves the quality of generated 2D/3D scenes, highlighting its potential in applications such as metaverse and gaming simulation.", "topic": "\"Advanced AI and NLP Research\""}, {"title": "From Dogwhistles to Bullhorns: Unveiling Coded Rhetoric with Language Models", "authors": "Julia Mendelsohn, Ronan Le Bras, Yejin Choi, Maarten Sap", "year": 2023, "url": "https://www.semanticscholar.org/paper/a5731b32060909bfc8848fa5f7e1e14ca3b53240", "abstract": "Dogwhistles are coded expressions that simultaneously convey one meaning to a broad audience and a second, often hateful or provocative, meaning to a narrow in-group; they are deployed to evade both political repercussions and algorithmic content moderation. For example, the word \u201ccosmopolitan\u201d in a sentence such as \u201cwe need to end the cosmopolitan experiment\u201d can mean \u201cworldly\u201d to many but also secretly mean \u201cJewish\u201d to a select few. We present the first large-scale computational investigation of dogwhistles. We develop a typology of dogwhistles, curate the largest-to-date glossary of over 300 dogwhistles with rich contextual information and examples, and analyze their usage in historical U.S. politicians\u2019 speeches. We then assess whether a large language model (GPT-3) can identify dogwhistles and their meanings, and find that GPT-3\u2019s performance varies widely across types of dogwhistles and targeted groups. Finally, we show that harmful content containing dogwhistles avoids toxicity detection, highlighting online risks presented by such coded language. This work sheds light on the theoretical and applied importance of dogwhistles in both NLP and computational social science, and provides resources to facilitate future research in modeling dogwhistles and mitigating their online harms.", "topic": "\"Advanced AI and NLP Research\""}, {"title": "Are Machine Rationales (Not) Useful to Humans? Measuring and Improving Human Utility of Free-text Rationales", "authors": "Brihi Joshi, Ziyi Liu, Sahana Ramnath, Aaron Chan, Zhewei Tong, Shaoliang Nie, Qifan Wang, Yejin Choi, Xiang Ren", "year": 2023, "url": "https://www.semanticscholar.org/paper/a9ae921054e363c52dde5673fb3f9cef747febfb", "abstract": "Among the remarkable emergent capabilities of large language models (LMs) is free-text rationalization; beyond certain scale, large LMs are capable of generating seemingly useful rationalizations, which in turn, can dramatically enhance their performances on leaderboards. This phenomenon raises a question: can machine generated rationales also be useful for humans, especially when lay humans try to answer questions based on those machine rationales? We observe that human utility of existing rationales is far from satisfactory and expensive to estimate with human studies. Existing metrics like task performance of the LM generating the rationales or similarity between generated and gold rationales are not good indicators of their human utility. While we observe that certain properties of rationales like conciseness and novelty are correlated with their human utility, estimating them without human involvement is challenging. We show that, by estimating a rationale\u2019s helpfulness in answering similar unseen instances, we can measure its human utility to a better extent. We also translate this finding into an automated score, Gen-U, that we propose, which can help improve LMs\u2019 ability to generate rationales with better human utility, while maintaining most of its task performance. Lastly, we release all code and collected data with this project.", "topic": "\"Advanced AI and NLP Research\""}, {"title": "Impossible Distillation: from Low-Quality Model to High-Quality Dataset & Model for Summarization and Paraphrasing", "authors": "Jaehun Jung, Peter West, Liwei Jiang, Faeze Brahman, Ximing Lu, Jillian R. Fisher, Taylor Sorensen, Yejin Choi", "year": 2023, "url": "https://www.semanticscholar.org/paper/ac9d5e5cd77a4f36d9bfd4ee9d4c8089f89990a0", "abstract": "It is commonly perceived that the strongest language models (LMs) rely on a combination of massive scale, instruction data, and human feedback to perform specialized tasks \u2013 e.g. summarization and paraphrasing, without supervision. In this paper, we propose that language models can learn to summarize and paraphrase sentences, with none of these 3 factors. We present I MPOSSIBLE D ISTILLATION , a framework that distills a task-specific dataset directly from an off-the-shelf LM, even when it is impossible for the LM itself to reliably solve the task. By training a student model on the generated dataset and amplifying its capability through self-distillation, our method yields a high-quality model and dataset from a low-quality teacher model, without the need for scale or supervision. Using I MPOSSIBLE D ISTILLATION , we are able to distill an order of magnitude smaller model (with only 770M parameters) that outperforms 175B parameter GPT-3, in both quality and controllability, as confirmed by automatic and human evaluations. Furthermore, as a useful byproduct of our approach, we obtain D IM S UM +, a high-quality dataset with 3.4M sentence summaries and paraphrases. Our analyses show that this dataset, as a purely LM-generated corpus, is more diverse and more effective for generalization to unseen domains than all human-authored datasets \u2013 including Gigaword with 4M samples.", "topic": "\"Advancements in Language Model Compression and Reasoning\""}, {"title": "Vera: A General-Purpose Plausibility Estimation Model for Commonsense Statements", "authors": "Jiacheng Liu, Wenya Wang, Dianzhuo Wang, Noah A. Smith, Yejin Choi, Hannaneh Hajishirzi", "year": 2023, "url": "https://www.semanticscholar.org/paper/b953a1527c8878698f9adb2691e425d5d206cae4", "abstract": "Despite the much discussed capabilities of today's language models, they are still prone to silly and unexpected commonsense failures. We consider a retrospective verification approach that reflects on the correctness of LM outputs, and introduce Vera, a general-purpose model that estimates the plausibility of declarative statements based on commonsense knowledge. Trained on ~7M commonsense statements created from 19 QA datasets and two large-scale knowledge bases, and with a combination of three training objectives, Vera is a versatile model that effectively separates correct from incorrect statements across diverse commonsense domains. When applied to solving commonsense problems in the verification format, Vera substantially outperforms existing models that can be repurposed for commonsense verification, and it further exhibits generalization capabilities to unseen tasks and provides well-calibrated outputs. We find that Vera excels at filtering LM-generated commonsense knowledge and is useful in detecting erroneous commonsense statements generated by models like ChatGPT in real-world settings.", "topic": "\"Commonsense Knowledge in AI\""}, {"title": "Fusing Pre-Trained Language Models with Multimodal Prompts through Reinforcement Learning", "authors": "Youngjae Yu, Jiwan Chung, Heeseung Yun, Jack Hessel, J. Park, Ximing Lu, Rowan Zellers, Prithviraj Ammanabrolu, Ronan Le Bras, Gunhee Kim, Yejin Choi", "year": 2023, "url": "https://www.semanticscholar.org/paper/bee79110f7b89292955984c7110ed0de8ae719a1", "abstract": "Language models are capable of commonsense reasoning: while domain-specific models can learn from explicit knowledge (e.g. commonsense graphs [6] ethical norms [25]), and larger models like GPT-3 [7] mani-fest broad commonsense reasoning capacity. Can their knowledge be extended to multimodal inputs such as images and audio without paired domain data? In this work, we propose \u2021ESPER (Extending Sensory PErception with Reinforcement learning) which enables text-only pretrained models to address multimodal tasks such as visual commonsense reasoning. Our key novelty is to use rein-forcement learning to align multimodal inputs to language model generations without direct supervision: for example, our reward optimization relies only on cosine similarity derived from CLIP [52] and requires no additional paired (image, text) data. Experiments demonstrate that ESPER outperforms baselines and prior work on a variety of multimodal text generation tasks ranging from captioning to commonsense reasoning; these include a new benchmark we collect and release, the ESP dataset, which tasks models with generating the text of several different domains for each image. Our code and data are publicly released at https://github.com/JiwanChung/esper.", "topic": "\"Multimodal Language Models and Reinforcement Learning\""}, {"title": "Synergistic shape selectivity of H-Beta and H-ZSM-5 for Xylene-rich BTX production by hydrocracking of heavy-aromatic compounds", "authors": "Jinho Oh, Yejin Choi, Jaeuk Shin, Kyutae Kim, Jung Kyoo Lee", "year": 2023, "url": "https://www.semanticscholar.org/paper/c189f7e9debe65923020608984a62c9a845d955f", "abstract": null, "topic": "Unknown"}, {"title": "Inference-Time Policy Adapters (IPA): Tailoring Extreme-Scale LMs without Fine-tuning", "authors": "Ximing Lu, Faeze Brahman, Peter West, Jaehun Jang, Khyathi Raghavi Chandu, Abhilasha Ravichander, Lianhui Qin, Prithviraj Ammanabrolu, Liwei Jiang, Sahana Ramnath, Nouha Dziri, Jillian R. Fisher, Bill Yuchen Lin, Skyler Hallinan, Xiang Ren, Sean Welleck, Yejin Choi", "year": 2023, "url": "https://www.semanticscholar.org/paper/ca991e0283a1c30a46eb585d9eb499fc0ec8ecc2", "abstract": "While extreme-scale language models have demonstrated exceptional performance on a variety of language tasks, the degree of control over these language models through pure prompting can often be limited. Directly fine-tuning such language models can be effective for tailoring them, but it can be either extremely costly (e.g., GPT-3) or not even feasible for the broader community (e.g., GPT-4). We propose Inference-time Policy Adapters (IPA), which efficiently tailors a language model such as GPT-3 without fine-tuning it. IPA guides a large base model during decoding time through a lightweight policy adapter trained to optimize an arbitrary user objective with reinforcement learning. On five challenging text generation tasks, such as toxicity reduction and lexically constrained generation, IPA consistently brings significant improvements over off-the-shelf language models. It outperforms competitive baseline methods, sometimes even including expensive fine-tuning. In particular, tailoring GPT-2 with IPA can outperform GPT-3, while tailoring GPT-3 with IPA brings a major performance boost over GPT-3 (and sometimes even over GPT-4). Our promising results highlight the potential of IPA as a lightweight alternative to tailoring extreme-scale language models.", "topic": "\"Multimodal Language Models and Reinforcement Learning\""}, {"title": "PlaSma: Making Small Language Models Better Procedural Knowledge Models for (Counterfactual) Planning", "authors": "Faeze Brahman, Chandra Bhagavatula, Valentina Pyatkin, Jena D. Hwang, Xiang Lorraine Li, Hirona J. Arai, Soumya Sanyal, Keisuke Sakaguchi, Xiang Ren, Yejin Choi", "year": 2023, "url": "https://www.semanticscholar.org/paper/d3637f23d15e71bc1b16948a4c29f08c90b8c772", "abstract": "Procedural planning, which entails decomposing a high-level goal into a sequence of temporally ordered steps, is an important yet intricate task for machines. It involves integrating common-sense knowledge to reason about complex contextualized situations that are often counterfactual, e.g.\"scheduling a doctor's appointment without a phone\". While current approaches show encouraging results using large language models (LLMs), they are hindered by drawbacks such as costly API calls and reproducibility issues. In this paper, we advocate planning using smaller language models. We present PlaSma, a novel two-pronged approach to endow small language models with procedural knowledge and (counterfactual) planning capabilities. More concretely, we develop symbolic procedural knowledge distillation to enhance the implicit knowledge in small language models and an inference-time algorithm to facilitate more structured and accurate reasoning. In addition, we introduce a novel task, Counterfactual Planning, that requires a revision of a plan to cope with a counterfactual situation. In both the original and counterfactual setting, we show that orders-of-magnitude smaller models (770M-11B parameters) can compete and often surpass their larger teacher models' capabilities.", "topic": "\"Advancements in Language Model Compression and Reasoning\""}, {"title": "Value Kaleidoscope: Engaging AI with Pluralistic Human Values, Rights, and Duties", "authors": "Taylor Sorensen, Liwei Jiang, Jena D. Hwang, Sydney Levine, Valentina Pyatkin, Peter West, Nouha Dziri, Ximing Lu, Kavel Rao, Chandra Bhagavatula, Maarten Sap, J. Tasioulas, Yejin Choi", "year": 2023, "url": "https://www.semanticscholar.org/paper/d655f652d02251b45db43181c5e3c73dfc59cd51", "abstract": "Human values are crucial to human decision-making. Value pluralism is the view that multiple correct values may be held in tension with one another (e.g., when considering lying to a friend to protect their feelings, how does one balance honesty with friendship?). As statistical learners, AI systems fit to averages by default, washing out these potentially irreducible value conflicts. To improve AI systems to better reflect value pluralism, the first-order challenge is to explore the extent to which AI systems can model pluralistic human values, rights, and duties as well as their interaction.\n\nWe introduce ValuePrism, a large-scale dataset of 218k values, rights, and duties connected to 31k human-written situations. ValuePrism\u2019s contextualized values are generated by GPT-4 and deemed high-quality by human annotators 91% of the time. We conduct a large-scale study with annotators across diverse social and demographic backgrounds to try to understand whose values are represented.\n\nWith ValuePrism, we build Value Kaleidoscope (or Kaleido), an open, light-weight, and structured language-based multi-task model that generates, explains, and assesses the relevance and valence (i.e., support or oppose) of human values, rights, and duties within a specific context. Humans prefer the sets of values output by our system over the teacher GPT- 4, finding them more accurate and with broader coverage. In addition, we demonstrate that Kaleido can help explain variability in human decision-making by outputting contrasting values. Finally, we show that Kaleido\u2019s representations transfer to other philosophical frameworks and datasets, confirming the benefit of an explicit, modular, and interpretable approach to value pluralism. We hope that our work will serve as a step to making more explicit the implicit values behind human decision-making and to steering AI systems to make decisions that are more in accordance with them.", "topic": "\"Advanced AI and NLP Research\""}, {"title": "SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks", "authors": "Bill Yuchen Lin, Yicheng Fu, Karina Yang, Prithviraj Ammanabrolu, Faeze Brahman, Shiyu Huang, Chandra Bhagavatula, Yejin Choi, Xiang Ren", "year": 2023, "url": "https://www.semanticscholar.org/paper/d671d62a1eb4d57343e4a0928297266dffc0c118", "abstract": "We introduce SwiftSage, a novel agent framework inspired by the dual-process theory of human cognition, designed to excel in action planning for complex interactive reasoning tasks. SwiftSage integrates the strengths of behavior cloning and prompting large language models (LLMs) to enhance task completion performance. The framework comprises two primary modules: the Swift module, representing fast and intuitive thinking, and the Sage module, emulating deliberate thought processes. The Swift module is a small encoder-decoder LM fine-tuned on the oracle agent's action trajectories, while the Sage module employs LLMs such as GPT-4 for subgoal planning and grounding. We develop a heuristic method to harmoniously integrate the two modules, resulting in a more efficient and robust problem-solving process. In 30 tasks from the ScienceWorld benchmark, SwiftSage significantly outperforms other methods such as SayCan, ReAct, and Reflexion, demonstrating its effectiveness in solving complex interactive tasks.", "topic": "\"Commonsense Knowledge in AI\""}, {"title": "Minding Language Models\u2019 (Lack of) Theory of Mind: A Plug-and-Play Multi-Character Belief Tracker", "authors": "Melanie Sclar, Sachin Kumar, Peter West, Alane Suhr, Yejin Choi, Yulia Tsvetkov", "year": 2023, "url": "https://www.semanticscholar.org/paper/d7a3f5c612930a3c08f1632b88934252edc66d67", "abstract": "Theory of Mind (ToM)\u2014the ability to reason about the mental states of other people\u2014is a key element of our social intelligence. Yet, despite their ever more impressive performance, large-scale neural language models still lack basic theory of mind capabilities out-of-the-box. We posit that simply scaling up models will not imbue them with theory of mind due to the inherently symbolic and implicit nature of the phenomenon, and instead investigate an alternative: can we design a decoding-time algorithm that enhances theory of mind of off-the-shelf neural language models without explicit supervision? We present SymbolicToM, a plug-and-play approach to reason about the belief states of multiple characters in reading comprehension tasks via explicit symbolic representation. More concretely, our approach tracks each entity\u2019s beliefs, their estimation of other entities\u2019 beliefs, and higher-order levels of reasoning, all through graphical representations, allowing for more precise and interpretable reasoning than previous approaches. Empirical results on the well-known ToMi benchmark (Le et al., 2019) demonstrate that SymbolicToM dramatically enhances off-the-shelf neural networks\u2019 theory of mind in a zero-shot setting while showing robust out-of-distribution performance compared to supervised baselines. Our work also reveals spurious patterns in existing theory of mind benchmarks, emphasizing the importance of out-of-distribution evaluation and methods that do not overfit a particular dataset.", "topic": "\"Advancements in Language Model Compression and Reasoning\""}, {"title": "Clever Hans or Neural Theory of Mind? Stress Testing Social Reasoning in Large Language Models", "authors": "Natalie Shapira, Mosh Levy, S. Alavi, Xuhui Zhou, Yejin Choi, Yoav Goldberg, Maarten Sap, Vered Shwartz", "year": 2023, "url": "https://www.semanticscholar.org/paper/ddcd2bcc809bd0c2755a4a9487473d61ac327c50", "abstract": "The escalating debate on AI\u2019s capabilities warrants developing reliable metrics to assess machine \u201cintelligence.\u201d Recently, many anecdotal examples were used to suggest that newer Large Language Models (LLMs) like ChatGPT and GPT-4 exhibit Neural Theory-of-Mind (N-ToM); however, prior work reached conflicting conclusions regarding those abilities. We investigate the extent of LLMs\u2019 N-ToM through an extensive evaluation of 6 tasks and find that while LLMs exhibit certain N-ToM abilities, this behavior is far from being robust. We further examine the factors impacting performance on N-ToM tasks and discover that LLMs struggle with adversarial examples, indicating reliance on shallow heuristics rather than robust ToM abilities. We caution against drawing conclusions from anecdotal examples, limited benchmark testing, and using human-designed psychological tests to evaluate models.", "topic": "\"Language Models and Narrative Analysis\""}, {"title": "Multimodal C4: An Open, Billion-scale Corpus of Images Interleaved With Text", "authors": "Wanrong Zhu, Jack Hessel, Anas Awadalla, S. Gadre, Jesse Dodge, Alex Fang, Youngjae Yu, Ludwig Schmidt, William Yang Wang, Yejin Choi", "year": 2023, "url": "https://www.semanticscholar.org/paper/df958800014d310b6df34ad83d771314d68fbb2d", "abstract": "In-context vision and language models like Flamingo support arbitrarily interleaved sequences of images and text as input. This format not only enables few-shot learning via interleaving independent supervised (image, text) examples, but also, more complex prompts involving interaction between images, e.g.,\"What do image A and image B have in common?\"To support this interface, pretraining occurs over web corpora that similarly contain interleaved images+text. To date, however, large-scale data of this form have not been publicly available. We release Multimodal C4, an augmentation of the popular text-only C4 corpus with images interleaved. We use a linear assignment algorithm to place images into longer bodies of text using CLIP features, a process that we show outperforms alternatives. Multimodal C4 spans everyday topics like cooking, travel, technology, etc. A manual inspection of a random sample of documents shows that a vast majority (88%) of images are topically relevant, and that linear assignment frequently selects individual sentences specifically well-aligned with each image (80%). After filtering NSFW images, ads, etc., the resulting corpus consists of 101.2M documents with 571M images interleaved in 43B English tokens.", "topic": "\"Multimodal Language Models and Reinforcement Learning\""}, {"title": "We're Afraid Language Models Aren't Modeling Ambiguity", "authors": "Alisa Liu, Zhaofeng Wu, Julian Michael, Alane Suhr, Peter West, Alexander Koller, Swabha Swayamdipta, Noah A. Smith, Yejin Choi", "year": 2023, "url": "https://www.semanticscholar.org/paper/e877d295ca425faf33f0c8e4d8c410c2e9c8a26d", "abstract": "Ambiguity is an intrinsic feature of natural language. Managing ambiguity is a key part of human language understanding, allowing us to anticipate misunderstanding as communicators and revise our interpretations as listeners. As language models (LMs) are increasingly employed as dialogue interfaces and writing aids, handling ambiguous language is critical to their success. We characterize ambiguity in a sentence by its effect on entailment relations with another sentence, and collect AmbiEnt, a linguist-annotated benchmark of 1,645 examples with diverse kinds of ambiguity. We design a suite of tests based on AmbiEnt, presenting the first evaluation of pretrained LMs to recognize ambiguity and disentangle possible meanings. We find that the task remains extremely challenging, including for GPT-4, whose generated disambiguations are considered correct only 32% of the time in human evaluation, compared to 90% for disambiguations in our dataset. Finally, to illustrate the value of ambiguity-sensitive tools, we show that a multilabel NLI model can flag political claims in the wild that are misleading due to ambiguity. We encourage the field to rediscover the importance of ambiguity for NLP.", "topic": "\"Advanced AI and NLP Research\""}, {"title": "NeuroComparatives: Neuro-Symbolic Distillation of Comparative Knowledge", "authors": "Phillip Howard, Junlin Wang, Vasudev Lal, Gadi Singer, Yejin Choi, Swabha Swayamdipta", "year": 2023, "url": "https://www.semanticscholar.org/paper/eba9a4a57939eedc58cd66e610ccb7e09f1c3628", "abstract": "Comparative knowledge (e.g., steel is stronger and heavier than styrofoam) is an essential component of our world knowledge, yet understudied in prior literature. In this paper, we harvest the dramatic improvements in knowledge capabilities of language models into a large-scale comparative knowledge base. While the ease of acquisition of such comparative knowledge is much higher from extreme-scale models like GPT-4, compared to their considerably smaller and weaker counterparts such as GPT-2, not even the most powerful models are exempt from making errors. We thus ask: to what extent are models at different scales able to generate valid and diverse comparative knowledge? We introduce NeuroComparatives, a novel framework for comparative knowledge distillation overgenerated from language models such as GPT-variants and LLaMA, followed by stringent filtering of the generated knowledge. Our framework acquires comparative knowledge between everyday objects, producing a corpus of up to 8.8M comparisons over 1.74M entity pairs - 10X larger and 30% more diverse than existing resources. Moreover, human evaluations show that NeuroComparatives outperform existing resources in terms of validity (up to 32% absolute improvement). Our acquired NeuroComparatives leads to performance improvements on five downstream tasks. We find that neuro-symbolic manipulation of smaller models offers complementary benefits to the currently dominant practice of prompting extreme-scale language models for knowledge distillation.", "topic": "\"Commonsense Knowledge in AI\""}, {"title": "Do Embodied Agents Dream of Pixelated Sheep?: Embodied Decision Making using Language Guided World Modelling", "authors": "Kolby Nottingham, Prithviraj Ammanabrolu, Alane Suhr, Yejin Choi, Hannaneh Hajishirzi, Sameer Singh, Roy Fox", "year": 2023, "url": "https://www.semanticscholar.org/paper/fc918d6f8e2523696c34fa1be5aabdb42e9648d2", "abstract": "Reinforcement learning (RL) agents typically learn tabula rasa, without prior knowledge of the world. However, if initialized with knowledge of high-level subgoals and transitions between subgoals, RL agents could utilize this Abstract World Model (AWM) for planning and exploration. We propose using few-shot large language models (LLMs) to hypothesize an AWM, that will be verified through world experience, to improve sample efficiency of RL agents. Our DECKARD agent applies LLM-guided exploration to item crafting in Minecraft in two phases: (1) the Dream phase where the agent uses an LLM to decompose a task into a sequence of subgoals, the hypothesized AWM; and (2) the Wake phase where the agent learns a modular policy for each subgoal and verifies or corrects the hypothesized AWM. Our method of hypothesizing an AWM with LLMs and then verifying the AWM based on agent experience not only increases sample efficiency over contemporary methods by an order of magnitude but is also robust to and corrects errors in the LLM, successfully blending noisy internet-scale information from LLMs with knowledge grounded in environment dynamics.", "topic": "\"Multimodal Language Models and Reinforcement Learning\""}, {"title": "Quark: Controllable Text Generation with Reinforced Unlearning", "authors": "Ximing Lu, Sean Welleck, Liwei Jiang, Jack Hessel, Lianhui Qin, Peter West, Prithviraj Ammanabrolu, Yejin Choi", "year": 2022, "url": "https://www.semanticscholar.org/paper/023edab4738690444e3924e224c2641017a0d794", "abstract": "Large-scale language models often learn behaviors that are misaligned with user expectations. Generated text may contain offensive or toxic language, contain significant repetition, or be of a different sentiment than desired by the user. We consider the task of unlearning these misalignments by fine-tuning the language model on signals of what not to do. We introduce Quantized Reward Konditioning (Quark), an algorithm for optimizing a reward function that quantifies an (un)wanted property, while not straying too far from the original model. Quark alternates between (i) collecting samples with the current language model, (ii) sorting them into quantiles based on reward, with each quantile identified by a reward token prepended to the language model's input, and (iii) using a standard language modeling loss on samples from each quantile conditioned on its reward token, while remaining nearby the original language model via a KL-divergence penalty. By conditioning on a high-reward token at generation time, the model generates text that exhibits less of the unwanted property. For unlearning toxicity, negative sentiment, and repetition, our experiments show that Quark outperforms both strong baselines and state-of-the-art reinforcement learning methods like PPO (Schulman et al. 2017), while relying only on standard language modeling primitives.", "topic": "\"Advanced Text Generation Techniques\""}, {"title": "Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks", "authors": "Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Anjana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, Eshaan Pathak, Giannis Karamanolakis, H. Lai, I. Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi, Maitreya Patel, Kuntal Kumar Pal, M. Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri, Rushang Karia, Shailaja Keyur Sampat, Savan Doshi, Siddhartha Mishra, Sujan Reddy, Sumanta Patro, Tanay Dixit, Xudong Shen, Chitta Baral, Yejin Choi, Noah A. Smith, Hannaneh Hajishirzi, Daniel Khashabi", "year": 2022, "url": "https://www.semanticscholar.org/paper/06d7cb8c8816360feb33c3367073e0ef66d7d0b0", "abstract": "How well can NLP models generalize to a variety of unseen tasks when provided with task instructions? To address this question, we first introduce Super-NaturalInstructions, a benchmark of 1,616 diverse NLP tasks and their expert-written instructions. Our collection covers 76 distinct task types, including but not limited to classification, extraction, infilling, sequence tagging, text rewriting, and text composition. This large and diverse collection of tasks enables rigorous benchmarking of cross-task generalization under instructions\u2014training models to follow instructions on a subset of tasks and evaluating them on the remaining unseen ones.Furthermore, we build Tk-Instruct, a transformer model trained to follow a variety of in-context instructions (plain language task definitions or k-shot examples). Our experiments show that Tk-Instruct outperforms existing instruction-following models such as InstructGPT by over 9% on our benchmark despite being an order of magnitude smaller. We further analyze generalization as a function of various scaling parameters, such as the number of observed tasks, the number of instances per task, and model sizes. We hope our dataset and model facilitate future progress towards more general-purpose NLP models.", "topic": "\"Advanced AI and NLP Research\""}, {"title": "Detoxifying Text with MaRCo: Controllable Revision with Experts and Anti-Experts", "authors": "Skyler Hallinan, Alisa Liu, Yejin Choi, Maarten Sap", "year": 2022, "url": "https://www.semanticscholar.org/paper/0cb79553e576c49dc89504101c75367196841142", "abstract": "Text detoxification has the potential to mitigate the harms of toxicity by rephrasing text to remove offensive meaning, but subtle toxicity remains challenging to tackle. We introduce MaRCo, a detoxification algorithm that combines controllable generation and text rewriting methods using a Product of Experts with autoencoder language models (LMs). MaRCo uses likelihoods under a non-toxic LM (expert) and a toxic LM (anti-expert) to find candidate words to mask and potentially replace. We evaluate our method on several subtle toxicity and microaggressions datasets, and show that it not only outperforms baselines on automatic metrics, but MaRCo\u2019s rewrites are preferred 2.1 times more in human evaluation. Its applicability to instances of subtle toxicity is especially promising, demonstrating a path forward for addressing increasingly elusive online hate.", "topic": "\"Advanced Text Generation Techniques\""}, {"title": "Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs", "authors": "Maarten Sap, Ronan Le Bras, Daniel Fried, Yejin Choi", "year": 2022, "url": "https://www.semanticscholar.org/paper/311fd5f6f114ae51f8cbd95a0da69d7b556d25f1", "abstract": "Social intelligence and Theory of Mind (TOM), i.e., the ability to reason about the different mental states, intents, and reactions of all people involved, allows humans to effectively navigate and understand everyday social interactions. As NLP systems are used in increasingly complex social situations, their ability to grasp social dynamics becomes crucial.In this work, we examine the open question of social intelligence and Theory of Mind in modern NLP systems from an empirical and theorybased perspective. We show that one of today\u2019s largest language models (GPT-3; Brown et al., 2020) lacks this kind of social intelligence out-of-the box, using two tasks: SocialIQa (Sap et al., 2019), which measure models\u2019 ability to understand intents and reactions of participants of social interactions, and ToMi (Le, Boureau, and Nickel, 2019), which measures whether models can infer mental states and realities of participants of situations.Our results show that models struggle substantially at these Theory of Mind tasks, with well-below-human accuracies of 55% and 60% on SocialIQa and ToMi, respectively. To conclude, we draw on theories from pragmatics to contextualize this shortcoming of large language models, by examining the limitations stemming from their data, neural architecture, and training paradigms. Challenging the prevalent narrative that only scale is needed, we posit that person-centric NLP approaches might be more effective towards neural Theory of Mind.", "topic": "\"Advanced AI and NLP Research\""}, {"title": "NaturalAdversaries: Can Naturalistic Adversaries Be as Effective as Artificial Adversaries?", "authors": "Saadia Gabriel, Hamid Palangi, Yejin Choi", "year": 2022, "url": "https://www.semanticscholar.org/paper/337ad42b01d0ff764aa46a1ac9e3a93e2830e06c", "abstract": "While a substantial body of prior work has explored adversarial example generation for natural language understanding tasks, these examples are often unrealistic and diverge from the real-world data distributions. In this work, we introduce a two-stage adversarial example generation framework (NaturalAdversaries), for designing adversaries that are effective at fooling a given classifier and demonstrate natural-looking failure cases that could plausibly occur during in-the-wild deployment of the models. At the first stage a token attribution method is used to summarize a given classifier's behaviour as a function of the key tokens in the input. In the second stage a generative model is conditioned on the key tokens from the first stage. NaturalAdversaries is adaptable to both black-box and white-box adversarial attacks based on the level of access to the model parameters. Our results indicate these adversaries generalize across domains, and offer insights for future research on improving robustness of neural text classification models.", "topic": "\"Advanced AI and NLP Research\""}, {"title": "ProsocialDialog: A Prosocial Backbone for Conversational Agents", "authors": "Hyunwoo Kim, Youngjae Yu, Liwei Jiang, Ximing Lu, Daniel Khashabi, Gunhee Kim, Yejin Choi, Maarten Sap", "year": 2022, "url": "https://www.semanticscholar.org/paper/36c50e6638dddc8324eef9bfa064bfcab80cbef4", "abstract": "Most existing dialogue systems fail to respond properly to potentially unsafe user utterances by either ignoring or passively agreeing with them. To address this issue, we introduce ProsocialDialog, the first large-scale multi-turn dialogue dataset to teach conversational agents to respond to problematic content following social norms. Covering diverse unethical, problematic, biased, and toxic situations, ProsocialDialog contains responses that encourage prosocial behavior, grounded in commonsense social rules (i.e., rules-of-thumb, RoTs). Created via a human-AI collaborative framework, ProsocialDialog consists of 58K dialogues, with 331K utterances, 160K unique RoTs, and 497K dialogue safety labels accompanied by free-form rationales.With this dataset, we introduce a dialogue safety detection module, Canary, capable of generating RoTs given conversational context, and a socially-informed dialogue agent, Prost. Empirical results show that Prost generates more socially acceptable dialogues compared to other state-of-the-art language and dialogue models in both in-domain and out-of-domain settings. Additionally, Canary effectively guides conversational agents and off-the-shelf language models to generate significantly more prosocial responses. Our work highlights the promise and importance of creating and steering conversational AI to be socially responsible.", "topic": "\"Socially Responsible AI Systems\""}, {"title": "NeuroCounterfactuals: Beyond Minimal-Edit Counterfactuals for Richer Data Augmentation", "authors": "Phillip Howard, Gadi Singer, Vasudev Lal, Yejin Choi, Swabha Swayamdipta", "year": 2022, "url": "https://www.semanticscholar.org/paper/39880b887c19ae80d71643b37d6fc89aba8ec0c4", "abstract": "While counterfactual data augmentation offers a promising step towards robust generalization in natural language processing, producing a set of counterfactuals that offer valuable inductive bias for models remains a challenge. Most existing approaches for producing counterfactuals, manual or automated, rely on small perturbations via minimal edits, resulting in simplistic changes. We introduce NeuroCounterfactuals, designed as loose counterfactuals, allowing for larger edits which result in naturalistic generations containing linguistic diversity, while still bearing similarity to the original document. Our novel generative approach bridges the benefits of constrained decoding, with those of language model adaptation for sentiment steering. Training data augmentation with our generations results in both in-domain and out-of-domain improvements for sentiment classification, outperforming even manually curated counterfactuals, under select settings. We further present detailed analyses to show the advantages of NeuroCounterfactuals over approaches involving simple, minimal edits.", "topic": "\"Advanced AI and NLP Research\""}, {"title": "MERLOT RESERVE: Neural Script Knowledge through Vision and Language and Sound", "authors": "Rowan Zellers, Jiasen Lu, Ximing Lu, Youngjae Yu, Yanpeng Zhao, Mohammadreza Salehi, Aditya Kusupati, Jack Hessel, Ali Farhadi, Yejin Choi", "year": 2022, "url": "https://www.semanticscholar.org/paper/400d619cbabeb669115bb7281a889ab869829ef5", "abstract": "As humans, we navigate a multimodal world, building a holistic understanding from all our senses. We introduce @MERLOT RESERVE, a model that represents videos jointly over time - through a new training objective that learns from audio, subtitles, and video frames. Given a video, we replace snippets of text and audio with a MASK token; the model learns by choosing the correct masked-out snippet. Our objective learns faster than alternatives, and performs well at scale: we pretrain on 20 million YouTube videos. Empirical results show that @MERLOT RESERVE learns strong multimodal representations. When finetuned, it sets state-of-the-art on Visual Commonsense Reasoning (VCR), TVQA, and Kinetics-600; outperforming prior work by 5%, 7%, and 1.5% respectively. Ablations show that these tasks benefit from audio pretraining - even VCR, a QA task centered around images (without sound). Moreover, our objective enables out-of-the-box prediction, revealing strong multimodal commonsense understanding. In a fully zero-shot setting, our model obtains competitive results on four video tasks, even outperforming supervised approaches on the recently proposed Situated Reasoning (STAR) benchmark. We analyze why audio enables better vision-language representations, suggesting significant opportunities for future research. We conclude by discussing ethical and societal implications of multimodal pretraining.", "topic": "\"Multimodal Language Models and Reinforcement Learning\""}, {"title": "Multimodal Knowledge Alignment with Reinforcement Learning", "authors": "Youngjae Yu, Jiwan Chung, Heeseung Yun, Jack Hessel, J. Park, Ximing Lu, Prithviraj Ammanabrolu, Rowan Zellers, Ronan Le Bras, Gunhee Kim, Yejin Choi", "year": 2022, "url": "https://www.semanticscholar.org/paper/43c81cd19ffd3170be14407e10e0229f879a8647", "abstract": "Large language models readily adapt to novel settings, even without task-specific training data. Can their zero-shot capacity be extended to multimodal inputs? In this work, we propose ESPER which extends language-only zero-shot models to unseen multimodal tasks, like image and audio captioning. Our key novelty is to use reinforcement learning to align multimodal inputs to language model generations without direct supervision: for example, in the image case our reward optimization relies only on cosine similarity derived from CLIP, and thus requires no additional explicitly paired (image, caption) data. Because the parameters of the language model are left unchanged, the model maintains its capacity for zero-shot generalization. Experiments demonstrate that ESPER outperforms baselines and prior work on a variety of zero-shot tasks; these include a new benchmark we collect+release, ESP dataset, which tasks models with generating several diversely-styled captions for each image.", "topic": "\"Multimodal Language Models and Reinforcement Learning\""}, {"title": "MAUVE Scores for Generative Models: Theory and Practice", "authors": "Krishna Pillutla, Lang Liu, John Thickstun, Sean Welleck, Swabha Swayamdipta, Rowan Zellers, Sewoong Oh, Yejin Choi, Za\u00efd Harchaoui", "year": 2022, "url": "https://www.semanticscholar.org/paper/447ddcb2b16c2cacabeec2933273918bd6fe1d79", "abstract": "Generative artificial intelligence has made significant strides, producing text indistinguishable from human prose and remarkably photorealistic images. Automatically measuring how close the generated data distribution is to the target distribution is central to diagnosing existing models and developing better ones. We present MAUVE, a family of comparison measures between pairs of distributions such as those encountered in the generative modeling of text or images. These scores are statistical summaries of divergence frontiers capturing two types of errors in generative modeling. We explore three approaches to statistically estimate these scores: vector quantization, non-parametric estimation, and classifier-based estimation. We provide statistical bounds for the vector quantization approach. Empirically, we find that the proposed scores paired with a range of $f$-divergences and statistical estimation methods can quantify the gaps between the distributions of human-written text and those of modern neural language models by correlating with human judgments and identifying known properties of the generated texts. We demonstrate in the vision domain that MAUVE can identify known properties of generated images on par with or better than existing metrics. In conclusion, we present practical recommendations for using MAUVE effectively with language and image modalities.", "topic": "\"AI Model Evaluation Metrics\""}, {"title": "COLD Decoding: Energy-based Constrained Text Generation with Langevin Dynamics", "authors": "Lianhui Qin, Sean Welleck, Daniel Khashabi, Yejin Choi", "year": 2022, "url": "https://www.semanticscholar.org/paper/4a6a65968a8eb8c09ffb57a7774ddabb596565b1", "abstract": "Many applications of text generation require incorporating different constraints to control the semantics or style of generated text. These constraints can be hard (e.g., ensuring certain keywords are included in the output) and soft (e.g., contextualizing the output with the left- or right-hand context). In this paper, we present Energy-based Constrained Decoding with Langevin Dynamics (COLD), a decoding framework which unifies constrained generation as specifying constraints through an energy function, then performing efficient differentiable reasoning over the constraints through gradient-based sampling. COLD decoding is a flexible framework that can be applied directly to off-the-shelf left-to-right language models without the need for any task-specific fine-tuning, as demonstrated through three challenging text generation applications: lexically-constrained generation, abductive reasoning, and counterfactual reasoning. Our experiments on these constrained generation tasks point to the effectiveness of our approach, both in terms of automatic and human evaluation.", "topic": "\"Advanced Text Generation Techniques\""}, {"title": "The Curious Case of Commonsense Intelligence", "authors": "Yejin Choi", "year": 2022, "url": "https://www.semanticscholar.org/paper/4fa63e4b65a13f12440ba976c84535141b6e8332", "abstract": "Abstract Commonsense intelligence is a long-standing puzzle in AI. Despite considerable advances in deep learning, AI continues to be narrow and brittle due to its lack of common sense. Why is common sense so trivial for humans but so hard for machines? In this essay, I map the twists and turns in recent research adventures toward commonsense AI. As we will see, the latest advances on common sense are riddled with new, potentially counterintuitive perspectives and questions. In particular, I discuss the significance of language for modeling intuitive reasoning, the fundamental limitations of logic formalisms despite their intellectual appeal, the case for on-the-fly generative reasoning through language, the continuum between knowledge and reasoning, and the blend between symbolic and neural knowledge representations.", "topic": "\"Commonsense Knowledge in AI\""}, {"title": "Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations", "authors": "Jaehun Jung, Lianhui Qin, Sean Welleck, Faeze Brahman, Chandra Bhagavatula, Ronan Le Bras, Yejin Choi", "year": 2022, "url": "https://www.semanticscholar.org/paper/50b0c6ee2b3d53ba5af69d6c00b5d60888a9026f", "abstract": "Pre-trained language models (LMs) struggle with consistent reasoning; recently, prompting LMs to generate explanations that self-guide the inference has emerged as a promising direction to amend this. However, these approaches are fundamentally bounded by the correctness of explanations, which themselves are often noisy and inconsistent. In this work, we develop Maieutic Prompting, which aims to infer a correct answer to a question even from the unreliable generations of LM. Maieutic Prompting induces a tree of explanations abductively (e.g. X is true, because ...) and recursively, then frames the inference as a satisfiability problem over these explanations and their logical relations. We test Maieutic Prompting for true/false QA on three challenging benchmarks that require complex commonsense reasoning. Maieutic Prompting achieves up to 20% better accuracy than state-of-the-art prompting methods, and as a fully unsupervised approach, performs competitively with supervised models. We also show that Maieutic Prompting improves robustness in inference while providing interpretable rationales.", "topic": "\"Commonsense Knowledge in AI\""}, {"title": "Generating Sequences by Learning to Self-Correct", "authors": "Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, T. Shen, Daniel Khashabi, Yejin Choi", "year": 2022, "url": "https://www.semanticscholar.org/paper/538288d24bdad73d831dfed44b706958287ed318", "abstract": "Sequence generation applications require satisfying semantic constraints, such as ensuring that programs are correct, using certain keywords, or avoiding undesirable content. Language models, whether fine-tuned or prompted with few-shot demonstrations, frequently violate these constraints, and lack a mechanism to iteratively revise their outputs. Moreover, some powerful language models are of extreme scale or inaccessible, making it inefficient, if not infeasible, to update their parameters for task-specific adaptation. We present Self-Correction, an approach that decouples an imperfect base generator (an off-the-shelf language model or supervised sequence-to-sequence model) from a separate corrector that learns to iteratively correct imperfect generations. To train the corrector, we propose an online training procedure that can use either scalar or natural language feedback on intermediate imperfect generations. We show that Self-Correction improves upon the base generator in three diverse generation tasks - mathematical program synthesis, lexically-constrained generation, and toxicity control - even when the corrector is much smaller than the base generator.", "topic": "\"Advanced Text Generation Techniques\""}, {"title": "Quantifying the narrative flow of imagined versus autobiographical stories", "authors": "Maarten Sap, A. Jafarpour, Yejin Choi, Noah A. Smith, J. Pennebaker, Eric Horvitz", "year": 2022, "url": "https://www.semanticscholar.org/paper/56f83667ad90d7f2d5266cc80a914f0a1d0b7abc", "abstract": "Significance We explore the open question about differences in the narrative flow of stories generated from memory versus imagination. We introduce sequentiality, a computational measure of narrative flow of events that compares the influence of preceding sentences versus story topic on story sentences, using a cutting-edge large language model (GPT-3). Applying sequentiality to thousands of stories, we find that the narrative flows of imagined stories have greater reliance on preceding sentences than for autobiographical stories and that autobiographical narratives become more similar to imagined stories when retold several months later. Furthermore, we uncover a link between events perceived as salient and sequentiality. The methods provide a window into cognitive processes of storytelling that breaks away from traditional approaches to analyzing narratives.", "topic": "\"Language Models and Narrative Analysis\""}, {"title": "Imagined versus Remembered Stories: Quantifying Differences in Narrative Flow", "authors": "Maarten Sap, A. Jafarpour, Yejin Choi, Noah A. Smith, J. Pennebaker, E. Horvitz", "year": 2022, "url": "https://www.semanticscholar.org/paper/5b49f4a3b4cd6c3bed3441a927eaa721fd8d36dd", "abstract": "Lifelong experiences and learned knowledge lead to shared expectations about how common situations tend to unfold. Such knowledge of narrative event flow enables people to weave together a story. However, comparable computational tools to evaluate the flow of events in narratives are limited. We quantify the differences between autobiographical and imagined stories by introducing sequentiality , a measure of narrative flow of events, drawing probabilistic inferences from a cutting-edge large language model (GPT-3). Sequentiality captures the flow of a narrative by comparing the probability of a sentence with and without its preceding story context. We applied our measure to study thousands of diary-like stories, collected from crowdworkers about either a recent remembered experience or an imagined story on the same topic. The results show that imagined stories have higher sequentiality than autobiographical stories and that the sequentiality of autobiographical stories increases when the memories are retold several months later. In pur-suit of deeper understandings of how sequentiality measures the flow of narratives, we explore proportions of major and minor events in story sentences, as annotated by crowdworkers. We find that lower sequentiality is associated with higher proportions of major events. The methods and results highlight opportunities to use cutting-edge computational analyses, such as sequentiality, on large corpora of matched imagined and autobiographical stories to investigate the influences of memory and reasoning on language generation processes.", "topic": "\"Language Models and Narrative Analysis\""}, {"title": "WANLI: Worker and AI Collaboration for Natural Language Inference Dataset Creation", "authors": "Alisa Liu, Swabha Swayamdipta, Noah A. Smith, Yejin Choi", "year": 2022, "url": "https://www.semanticscholar.org/paper/5e8d3c2dc0fc53949794fc00600e25558c4a2441", "abstract": "A recurring challenge of crowdsourcing NLP datasets at scale is that human writers often rely on repetitive patterns when crafting examples, leading to a lack of linguistic diversity. We introduce a novel approach for dataset creation based on worker and AI collaboration, which brings together the generative strength of language models and the evaluative strength of humans. Starting with an existing dataset, MultiNLI for natural language inference (NLI), our approach uses dataset cartography to automatically identify examples that demonstrate challenging reasoning patterns, and instructs GPT-3 to compose new examples with similar patterns. Machine generated examples are then automatically filtered, and finally revised and labeled by human crowdworkers. The resulting dataset, WANLI, consists of 107,885 NLI examples and presents unique empirical strengths over existing NLI datasets. Remarkably, training a model on WANLI improves performance on eight out-of-domain test sets we consider, including by 11% on HANS and 9% on Adversarial NLI, compared to training on the 4x larger MultiNLI. Moreover, it continues to be more effective than MultiNLI augmented with other NLI datasets. Our results demonstrate the promise of leveraging natural language generation techniques and re-imagining the role of humans in the dataset creation process.", "topic": "\"Advanced AI and NLP Research\""}, {"title": "ClarifyDelphi: Reinforced Clarification Questions with Defeasibility Rewards for Social and Moral Situations", "authors": "Valentina Pyatkin, Jena D. Hwang, Vivek Srikumar, Ximing Lu, Liwei Jiang, Yejin Choi, Chandra Bhagavatula", "year": 2022, "url": "https://www.semanticscholar.org/paper/66e1e4ac804be19e7be931a3b999128529bb41a6", "abstract": "Context is everything, even in commonsense moral reasoning. Changing contexts can flip the moral judgment of an action; Lying to a friend is wrong in general, but may be morally acceptable if it is intended to protect their life.We present ClarifyDelphi, an interactive system that learns to ask clarification questions (e.g., why did you lie to your friend?) in order to elicit additional salient contexts of a social or moral situation. We posit that questions whose potential answers lead to diverging moral judgments are the most informative. Thus, we propose a reinforcement learning framework with a defeasibility reward that aims to maximize the divergence between moral judgments of hypothetical answers to a question. Human evaluation demonstrates that our system generates more relevant, informative and defeasible questions compared to competitive baselines. Our work is ultimately inspired by studies in cognitive science that have investigated the flexibility in moral cognition (i.e., the diverse contexts in which moral rules can be bent), and we hope that research in this direction can assist both cognitive and computational investigations of moral judgments.", "topic": "\"Socially Responsible AI Systems\""}, {"title": "Referee: Reference-Free Sentence Summarization with Sharper Controllability through Symbolic Knowledge Distillation", "authors": "Melanie Sclar, Peter West, Sachin Kumar, Yulia Tsvetkov, Yejin Choi", "year": 2022, "url": "https://www.semanticscholar.org/paper/78d3955e30d99650d078ba3ce1a523745da31040", "abstract": "We present Referee, a novel framework for sentence summarization that can be trained reference-free (i.e., requiring no gold summaries for supervision), while allowing direct control for compression ratio. Our work is the first to demonstrate that reference-free, controlled sentence summarization is feasible via the conceptual framework of Symbolic Knowledge Distillation (West et al., 2022), where latent knowledge in pre-trained language models is distilled via explicit examples sampled from the teacher models, further purified with three types of filters: length, fidelity, and Information Bottleneck. Moreover, we uniquely propose iterative distillation of knowledge, where student models from the previous iteration of distillation serve as teacher models in the next iteration. Starting off from a relatively modest set of GPT3-generated summaries, we demonstrate how iterative knowledge distillation can lead to considerably smaller, but better summarizers with sharper controllability. A useful by-product of this iterative distillation process is a high-quality dataset of sentence-summary pairs with varying degrees of compression ratios. Empirical results demonstrate that the final student models vastly outperform the much larger GPT3-Instruct model in terms of the controllability of compression ratios, without compromising the quality of resulting summarization.", "topic": "\"Advancements in Language Model Compression and Reasoning\""}, {"title": "Do Androids Laugh at Electric Sheep? Humor \u201cUnderstanding\u201d Benchmarks from The New Yorker Caption Contest", "authors": "Jack Hessel, Ana Marasovi\u0107, Jena D. Hwang, Lillian Lee, Jeff Da, Rowan Zellers, Robert Mankoff, Yejin Choi", "year": 2022, "url": "https://www.semanticscholar.org/paper/79914b8f022a2e4b522d890cbaf001842a116df0", "abstract": "Large neural networks can now generate jokes, but do they really \u201cunderstand\u201d humor? We challenge AI models with three tasks derived from the New Yorker Cartoon Caption Contest: matching a joke to a cartoon, identifying a winning caption, and explaining why a winning caption is funny. These tasks encapsulate progressively more sophisticated aspects of \u201cunderstanding\u201d a cartoon; key elements are the complex, often surprising relationships between images and captions and the frequent inclusion of indirect and playful allusions to human experience and culture. We investigate both multimodal and language-only models: the former are challenged with the cartoon images directly, while the latter are given multifaceted descriptions of the visual scene to simulate human-level visual understanding. We find that both types of models struggle at all three tasks. For example, our best multimodal models fall 30 accuracy points behind human performance on the matching task, and, even when provided ground-truth visual scene descriptors, human-authored explanations are preferred head-to-head over the best machine-authored ones (few-shot GPT-4) in more than 2/3 of cases. We release models, code, leaderboard, and corpus, which includes newly-gathered annotations describing the image\u2019s locations/entities, what\u2019s unusual in the scene, and an explanation of the joke.", "topic": "\"Advanced AI and NLP Research\""}, {"title": "Faking Fake News for Real Fake News Detection: Propaganda-Loaded Training Data Generation", "authors": "Kung-Hsiang Huang, Preslav Nakov, Yejin Choi, Heng Ji", "year": 2022, "url": "https://www.semanticscholar.org/paper/79ec50cd1320697819fd44c4ac6570c48a312349", "abstract": "Despite recent advances in detecting fake news generated by neural models, their results are not readily applicable to effective detection of human-written disinformation. What limits the successful transfer between them is the sizable gap between machine-generated fake news and human-authored ones, including the notable differences in terms of style and underlying intent. With this in mind, we propose a novel framework for generating training examples that are informed by the known styles and strategies of human-authored propaganda. Specifically, we perform self-critical sequence training guided by natural language inference to ensure the validity of the generated articles, while also incorporating propaganda techniques, such as appeal to authority and loaded language. In particular, we create a new training dataset, PropaNews, with 2,256 examples, which we release for future use. Our experimental results show that fake news detectors trained on PropaNews are better at detecting human-written disinformation by 3.62\u20137.69% F1 score on two public datasets.", "topic": "\"Advanced AI and NLP Research\""}, {"title": "I2D2: Inductive Knowledge Distillation with NeuroLogic and Self-Imitation", "authors": "Chandra Bhagavatula, Jena D. Hwang, Doug Downey, Ronan Le Bras, Ximing Lu, Keisuke Sakaguchi, Swabha Swayamdipta, Peter West, Yejin Choi", "year": 2022, "url": "https://www.semanticscholar.org/paper/83562af413d730b9321efe8bea24058514ac940b", "abstract": "Commonsense capabilities of pre-trained language models dramatically improve with scale, leading many to believe that scale is the only winning recipe. But is it? Here, we investigate an alternative that a priori seems impossible: can smaller language models (e.g., GPT-2) win over models that are orders of magnitude larger and better (e.g., GPT-3), if powered with novel commonsense distillation algorithms?The key intellectual challenge is to design a learning algorithm that achieve a competitive level of commonsense acquisition, without relying on the benefits of scale. In particular, we study generative models of commonsense knowledge, focusing on the task of generating generics, statements of commonsense facts about everyday concepts, e.g., birds can fly.We introduce I2D2, a novel commonsense distillation framework that loosely follows the Symbolic Knowledge Distillation of West et al. but breaks the dependence on the extreme-scale teacher model with two innovations: (1) the novel adaptation of NeuroLogic Decoding to enhance the generation quality of the weak, off-the-shelf language models, and (2) self-imitation learning to iteratively learn from the model\u2019s own enhanced commonsense acquisition capabilities. Empirical results suggest that scale is not the only way, as novel algorithms can be a promising alternative. Moreover, our study leads to a new corpus of generics, Gen-A-tomic, that is the largest and highest quality available to date.", "topic": "\"Commonsense Knowledge in AI\""}, {"title": "Knowledge is Power: Symbolic Knowledge Distillation, Commonsense Morality, & Multimodal Script Knowledge", "authors": "Yejin Choi", "year": 2022, "url": "https://www.semanticscholar.org/paper/8703985fd545ba7498a75fcbcc27845a3b549833", "abstract": "Scale appears to be the winning recipe in today's AI leaderboards. And yet, extreme-scale neural models are still brittle to make errors that are often nonsensical and even counterintuitive. In this talk, I will argue for the importance of knowledge, especially commonsense knowledge, and demonstrate how smaller models developed in academia can still have an edge over larger industry-scale models, if powered with knowledge. First, I will introduce \"symbolic knowledge distillation\", a new framework to distill larger neural language models into smaller commonsense models, which leads to a machine-authored KB that wins, for the first time, over a human-authored KB in all criteria: scale, accuracy, and diversity. Next, I will present an experimental conceptual framework toward computational social norms and commonsense morality, so that neural language models can learn to reason that \"helping a friend\" is generally a good thing to do, but \"helping a friend spread fake news\" is not. Finally, I will discuss an approach to multimodal script knowledge demonstrating the power of complex raw data, which leads to new SOTA performances on a dozen leaderboards that require grounded, temporal, and causal commonsense reasoning.", "topic": "\"Commonsense Knowledge in AI\""}, {"title": "Aligning to Social Norms and Values in Interactive Narratives", "authors": "Prithviraj Ammanabrolu, Liwei Jiang, Maarten Sap, Hannaneh Hajishirzi, Yejin Choi", "year": 2022, "url": "https://www.semanticscholar.org/paper/871037ca207f36ffc5322d7815a2dd58951a6227", "abstract": "We focus on creating agents that act in alignment with socially beneficial norms and values in interactive narratives or text-based games\u2014environments wherein an agent perceives and interacts with a world through natural language. Such interactive agents are often trained via reinforcement learning to optimize task performance, even when such rewards may lead to agent behaviors that violate societal norms\u2014causing harm either to the agent itself or other entities in the environment. Social value alignment refers to creating agents whose behaviors conform to expected moral and social norms for a given context and group of people\u2014in our case, it means agents that behave in a manner that is less harmful and more beneficial for themselves and others.We build on the Jiminy Cricket benchmark (Hendrycks et al. 2021), a set of 25 annotated interactive narratives containing thousands of morally salient scenarios covering everything from theft and bodily harm to altruism. We introduce the GALAD (Game-value ALignment through Action Distillation) agent that uses the social commonsense knowledge present in specially trained language models to contextually restrict its action space to only those actions that are aligned with socially beneficial values. An experimental study shows that the GALAD agent makes decisions efficiently enough to improve state-of-the-art task performance by 4% while reducing the frequency of socially harmful behaviors by 25% compared to strong contemporary value alignment approaches.", "topic": "\"Socially Responsible AI Systems\""}, {"title": "Rainier: Reinforced Knowledge Introspector for Commonsense Question Answering", "authors": "Jiacheng Liu, Skyler Hallinan, Ximing Lu, Pengfei He, Sean Welleck, Hannaneh Hajishirzi, Yejin Choi", "year": 2022, "url": "https://www.semanticscholar.org/paper/87b05afde92ec1f2c4ac4dffe2ab5c27ae36ea0c", "abstract": "Knowledge underpins reasoning. Recent research demonstrates that when relevant knowledge is provided as additional context to commonsense question answering (QA), it can substantially enhance the performance even on top of state-of-the-art. The fundamental challenge is where and how to find such knowledge that is high quality and on point with respect to the question; knowledge retrieved from knowledge bases are incomplete and knowledge generated from language models are inconsistent.We present Rainier, or Reinforced Knowledge Introspector, that learns to generate contextually relevant knowledge in response to given questions. Our approach starts by imitating knowledge generated by GPT-3, then learns to generate its own knowledge via reinforcement learning where rewards are shaped based on the increased performance on the resulting question answering. Rainier demonstrates substantial and consistent performance gains when tested over 9 different commonsense benchmarks: including 5 datasets that are seen during model training, as well as 4 datasets that are kept unseen. Our work is the first to report that knowledge generated by models that are orders of magnitude smaller than GPT-3, even without direct supervision on the knowledge itself, can exceed the quality of commonsense knowledge elicited from GPT-3.", "topic": "\"Socially Responsible AI Systems\""}, {"title": "The Abduction of Sherlock Holmes: A Dataset for Visual Abductive Reasoning", "authors": "Jack Hessel, Jena D. Hwang, J. Park, Rowan Zellers, Chandra Bhagavatula, Anna Rohrbach, Kate Saenko, Yejin Choi", "year": 2022, "url": "https://www.semanticscholar.org/paper/909c74269cbaf7384df48c17cd4245570ff87b13", "abstract": null, "topic": "Unknown"}, {"title": "Is Reinforcement Learning (Not) for Natural Language Processing?: Benchmarks, Baselines, and Building Blocks for Natural Language Policy Optimization", "authors": "Rajkumar Ramamurthy, Prithviraj Ammanabrolu, Kiant\u00e9 Brantley, Jack Hessel, R. Sifa, C. Bauckhage, Hannaneh Hajishirzi, Yejin Choi", "year": 2022, "url": "https://www.semanticscholar.org/paper/912a39c2e0e4a35747531669cfa952d2c5627729", "abstract": "We tackle the problem of aligning pre-trained large language models (LMs) with human preferences. If we view text generation as a sequential decision-making problem, reinforcement learning (RL) appears to be a natural conceptual framework. However, using RL for LM-based generation faces empirical challenges, including training instability due to the combinatorial action space, as well as a lack of open-source libraries and benchmarks customized for LM alignment. Thus, a question rises in the research community: is RL a practical paradigm for NLP? To help answer this, we first introduce an open-source modular library, RL4LMs (Reinforcement Learning for Language Models), for optimizing language generators with RL. The library consists of on-policy RL algorithms that can be used to train any encoder or encoder-decoder LM in the HuggingFace library (Wolf et al. 2020) with an arbitrary reward function. Next, we present the GRUE (General Reinforced-language Understanding Evaluation) benchmark, a set of 6 language generation tasks which are supervised not by target strings, but by reward functions which capture automated measures of human preference. GRUE is the first leaderboard-style evaluation of RL algorithms for NLP tasks. Finally, we introduce an easy-to-use, performant RL algorithm, NLPO (Natural Language Policy Optimization) that learns to effectively reduce the combinatorial action space in language generation. We show 1) that RL techniques are generally better than supervised methods at aligning LMs to human preferences; and 2) that NLPO exhibits greater stability and performance than previous policy gradient methods (e.g., PPO (Schulman et al. 2017)), based on both automatic and human evaluations.", "topic": "\"Multimodal Language Models and Reinforcement Learning\""}, {"title": "Reinforced Clarification Question Generation with Defeasibility Rewards for Disambiguating Social and Moral Situations", "authors": "Valentina Pyatkin, Jena D. Hwang, Vivek Srikumar, Ximing Lu, Liwei Jiang, Yejin Choi, Chandra Bhagavatula", "year": 2022, "url": "https://www.semanticscholar.org/paper/9a0ffb6156763be319a1ea39c9fac4080bc61182", "abstract": "Context is vital for commonsense moral reasoning. \u201cLying to a friend\" is wrong if it is meant to deceive them, but may be morally okay if it is intended to protect them. Such nuanced but salient contextual information can potentially \ufb02ip the moral judgment of an action. Thus, we present C LARIFY D ELPHI , an interactive system that elicits missing contexts of a moral situation by generating clari\ufb01cation questions such as \u201cWhy did you lie to your friend?\". Our approach is inspired by the observation that questions whose potential answers lead to diverging moral judgments are the most informative. We learn to generate questions using Reinforcement Learning, by maximizing the divergence between moral judgements of hypothetical answers to a question. Human evaluation shows that our system generates more relevant , informative and defeasible questions compared to other question generation baselines. C LARIFY D ELPHI assists informed moral reasoning processes by seeking additional morally consequential context to disambiguate social and moral situations.", "topic": "\"Socially Responsible AI Systems\""}, {"title": "Exposing the Limits of Video-Text Models through Contrast Sets", "authors": "J. Park, Sheng Shen, Ali Farhadi, Trevor Darrell, Yejin Choi, Anna Rohrbach", "year": 2022, "url": "https://www.semanticscholar.org/paper/aab700c342796155ff10e6607c3667e6e29fffb0", "abstract": "Recent video-text models can retrieve relevant videos based on text with a high accuracy, but to what extent do they comprehend the semantics of the text? Can they discriminate between similar entities and actions? To answer this, we propose an evaluation framework that probes video-text models with hard negatives. We automatically build contrast sets, where true textual descriptions are manipulated in ways that change their semantics while maintaining plausibility. Specifically, we leverage a pre-trained language model and a set of heuristics to create verb and person entity focused contrast sets. We apply these in the multiple choice video to-text classification setting. We test the robustness of recent methods on the proposed automatic contrast sets, and compare them to additionally collected human-generated counterparts, to assess their effectiveness. We see that model performance suffers across all methods, erasing the gap between recent CLIP-based methods vs. the earlier methods.", "topic": "\"Advanced Text Generation Techniques\""}, {"title": "INK : Intensive-Neural-Knowledge Aligned Image Text Retrieval", "authors": "J. Sung, Qiuyuan Huang, Yonatan Bisk, Subhojit Som, Ali Farhadi, Yejin Choi, Jianfeng Gao", "year": 2022, "url": "https://www.semanticscholar.org/paper/c9d98db9931689cfdbe5cf77fe5b887560891448", "abstract": "Knowledge-based vision language systems are increasingly ubiquitous in our everyday lives. However, despite the introduction of numerous benchmarks, the community has siloed models of different types of knowledge rather than building general knowledge-intensive models that encompass both commonsense and factoid knowledge. We introduce INK \u2013 I ntensive N eural K nowledge \u2013 a new task that involves extracting the necessary knowledge to accurately perform image and text retrieval 1 . In particular, INK leverages existing resources to require understanding of factoid, object-commonsense, or social-consciousness knowledge to successfully perform retrieval. Finally, we provide a set of competitive baseline models whose weak performance motivates the need to develop new knowledge understanding models and systems.", "topic": "\"Commonsense Knowledge in AI\""}, {"title": "NaturalProver: Grounded Mathematical Proof Generation with Language Models", "authors": "Sean Welleck, Jiacheng Liu, Ximing Lu, Hannaneh Hajishirzi, Yejin Choi", "year": 2022, "url": "https://www.semanticscholar.org/paper/d593b9b8d63426f0d6a795dd7f2294619bc03610", "abstract": "Theorem proving in natural mathematical language - the mixture of symbolic and natural language used by humans - plays a central role in mathematical advances and education, and tests aspects of reasoning that are core to intelligence. Yet it has remained underexplored with modern generative models. We study large-scale language models on two new generation tasks: suggesting the next step in a mathematical proof, and full proof generation. We develop NaturalProver, a language model that generates proofs by conditioning on background references (e.g. theorems and definitions that are either retrieved or human-provided), and optionally enforces their presence with constrained decoding. On theorems from the NaturalProofs benchmark, NaturalProver improves the quality of next-step suggestions and generated proofs over fine-tuned GPT-3, according to human evaluations from university-level mathematics students. NaturalProver is capable of proving some theorems that require short (2-6 step) proofs, and providing next-step suggestions that are rated as correct and useful over 40% of the time, which is to our knowledge the first demonstration of these capabilities using neural language models.", "topic": "\"Advanced Text Generation Techniques\""}, {"title": "Probing Factually Grounded Content Transfer with Factual Ablation", "authors": "Peter West, Chris Quirk, Michel Galley, Yejin Choi", "year": 2022, "url": "https://www.semanticscholar.org/paper/d77909e83a6456a22cc3653bca3316f6e21691ac", "abstract": "Despite recent success, large neural models often generate factually incorrect text. Compounding this is the lack of a standard automatic evaluation for factuality\u2013it cannot be meaningfully improved if it cannot be measured. Grounded generation promises a path to solving both of these problems: models draw on a reliable external document (grounding) for factual information, simplifying the challenge of factuality. Measuring factuality is also simplified\u2013to factual consistency, testing whether the generation agrees with the grounding, rather than all facts. Yet, without a standard automatic metric for factual consistency, factually grounded generation remains an open problem. We study this problem for content transfer, in which generations extend a prompt, using information from factual grounding. Particularly, this domain allows us to introduce the notion of factual ablation for automatically measuring factual consistency: this captures the intuition that the model should be less likely to produce an output given a less relevant grounding document. In practice, we measure this by presenting a model with two grounding documents, and the model should prefer to use the more factually relevant one. We contribute two evaluation sets to measure this. Applying our new evaluation, we propose multiple novel methods improving over strong baselines.", "topic": "\"Advanced Text Generation Techniques\""}, {"title": "Penguins Don\u2019t Fly: Reasoning about Generics through Instantiations and Exceptions", "authors": "Emily Allaway, Jena D. Hwang, Chandra Bhagavatula, K. McKeown, Doug Downey, Yejin Choi", "year": 2022, "url": "https://www.semanticscholar.org/paper/d89cee8ab8a8b775b49044aae112b3dd910d7338", "abstract": "Generics express generalizations about the world (e.g., birds can fly) that are not universally true (e.g., newborn birds and penguins cannot fly). Commonsense knowledge bases, used extensively in NLP, encode some generic knowledge but rarely enumerate such exceptions and knowing when a generic statement holds or does not hold true is crucial for developing a comprehensive understanding of generics. We present a novel framework informed by linguistic theory to generate exemplars\u2014specific cases when a generic holds true or false. We generate ~19k exemplars for ~650 generics and show that our framework outperforms a strong GPT-3 baseline by 12.8 precision points. Our analysis highlights the importance of linguistic theory-based controllability for generating exemplars, the insufficiency of knowledge bases as a source of exemplars, and the challenges exemplars pose for the task of natural language inference.", "topic": "\"Advanced AI and NLP Research\""}, {"title": "RealTime QA: What's the Answer Right Now?", "authors": "Jungo Kasai, Keisuke Sakaguchi, Yoichi Takahashi, Ronan Le Bras, Akari Asai, Xinyan Velocity Yu, Dragomir R. Radev, Noah A. Smith, Yejin Choi, Kentaro Inui", "year": 2022, "url": "https://www.semanticscholar.org/paper/dc5518e1db565a8c52084f27353461df474403d8", "abstract": "We introduce REALTIME QA, a dynamic question answering (QA) platform that announces questions and evaluates systems on a regular basis (weekly in this version). REALTIME QA inquires about the current world, and QA systems need to answer questions about novel events or information. It therefore challenges static, conventional assumptions in open-domain QA datasets and pursues instantaneous applications. We build strong baseline models upon large pretrained language models, including GPT-3 and T5. Our benchmark is an ongoing effort, and this paper presents real-time evaluation results over the past year. Our experimental results show that GPT-3 can often properly update its generation results, based on newly-retrieved documents, highlighting the importance of up-to-date information retrieval. Nonetheless, we find that GPT-3 tends to return outdated answers when retrieved documents do not provide sufficient information to find an answer. This suggests an important avenue for future research: can an open-domain QA system identify such unanswerable cases and communicate with the user or even the retrieval module to modify the retrieval results? We hope that REALTIME QA will spur progress in instantaneous applications of question answering and beyond.", "topic": "\"Advanced AI and NLP Research\""}, {"title": "Twist Decoding: Diverse Generators Guide Each Other", "authors": "Jungo Kasai, Keisuke Sakaguchi, Ronan Le Bras, Hao Peng, Ximing Lu, Dragomir R. Radev, Yejin Choi, Noah A. Smith", "year": 2022, "url": "https://www.semanticscholar.org/paper/ded0cd920c145ca0ae68acf306d54926cad5388c", "abstract": "Many language generation models are now available for a wide range of generation tasks, including machine translation and summarization. Combining such diverse models may lead to further progress, but ensembling generation models is challenging during inference: conventional ensembling methods (e.g., shallow fusion) require that the models share vocabulary/tokenization schemes. We introduce Twist decoding, a simple and general text generation algorithm that benefits from diverse models at inference time. Our method does not assume the vocabulary, tokenization or even generation order is shared. Our extensive evaluations on machine translation and scientific paper summarization demonstrate that Twist decoding substantially outperforms each model decoded in isolation over various scenarios, including cases where domain-specific and general-purpose models are both available. Twist decoding also consistently outperforms the popular reranking heuristic where output candidates from one model are rescored by another. We hope that our work will encourage researchers and practitioners to examine generation models collectively, not just independently, and to seek out models with complementary strengths to the currently available models.", "topic": "\"Advanced Text Generation Techniques\""}, {"title": "REV: Information-Theoretic Evaluation of Free-Text Rationales", "authors": "Hanjie Chen, Faeze Brahman, Xiang Ren, Yangfeng Ji, Yejin Choi, Swabha Swayamdipta", "year": 2022, "url": "https://www.semanticscholar.org/paper/e1d66f654fa056660988e1f40f06e97b792b6a59", "abstract": "Generating free-text rationales is a promising step towards explainable NLP, yet evaluating such rationales remains a challenge. Existing metrics have mostly focused on measuring the association between the rationale and a given label. We argue that an ideal metric should focus on the new information uniquely provided in the rationale that is otherwise not provided in the input or the label. We investigate this research problem from an information-theoretic perspective using conditional V-information (Hewitt et al., 2021). More concretely, we propose a metric called REV (Rationale Evaluation with conditional V-information), to quantify the amount of new, label-relevant information in a rationale beyond the information already available in the input or the label. Experiments across four benchmarks with reasoning tasks, including chain-of-thought, demonstrate the effectiveness of REV in evaluating rationale-label pairs, compared to existing metrics. We further demonstrate REV is consistent with human judgments on rationale evaluations and provides more sensitive measurements of new information in free-text rationales. When used alongside traditional performance metrics, REV provides deeper insights into models\u2019 reasoning and prediction processes.", "topic": "\"Socially Responsible AI Systems\""}, {"title": "Computational Lens on Cognition: Study Of Autobiographical Versus Imagined Stories With Large-Scale Language Models", "authors": "Maarten Sap, A. Jafarpour, Yejin Choi, Noah A. Smith, J. Pennebaker, E. Horvitz", "year": 2022, "url": "https://www.semanticscholar.org/paper/e40b2ae363fbe383e6823a75827c87846268133b", "abstract": "Lifelong experiences and learned knowledge lead to shared expectations about how common situations tend to unfold. Such knowledge enables people to interpret story narratives and identify salient events effortlessly. We study differences in the narrative flow of events in autobiographical versus imagined stories using GPT-3, one of the largest neural language models created to date. The diary-like stories were written by crowdworkers about either a recently experienced event or an imagined event on the same topic. To analyze the narrative flow of events of these stories, we measured sentence sequentiality , which compares the probability of a sentence with and without its preceding story context. We found that imagined stories have higher sequentiality than autobiographical stories, and that the sequentiality of autobiographical stories is higher when they are retold than when freshly recalled. Through an annotation of events in story sentences, we found that the story types contain similar proportions of major salient events, but that the autobiographical stories are denser in factual minor events. Furthermore, in comparison to imagined stories, autobiographical stories contain more concrete words and words related to the first person, cognitive processes, time, space, numbers, social words, and core drives and needs. Our findings highlight the opportunity to investigate memory and cognition with large-scale statistical language models", "topic": "\"Language Models and Narrative Analysis\""}, {"title": "Benchmarking Generalization via In-Context Instructions on 1, 600+ Language Tasks", "authors": "Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Anjana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, Eshaan Pathak, Giannis Karamanolakis, H. Lai, I. Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi, Maitreya Patel, Kuntal Kumar Pal, M. Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri, Rushang Karia, Shailaja Keyur Sampat, Savan Doshi, Siddhartha Mishra, Sujan Reddy, Sumanta Patro, Tanay Dixit, Xudong Shen, Chitta Baral, Yejin Choi, Hannaneh Hajishirzi, Noah A. Smith, Daniel Khashabi", "year": 2022, "url": "https://www.semanticscholar.org/paper/ec64e324ce1210fe5245dfd0fb5a92058732e5b9", "abstract": "How can we measure the generalization of models to a variety of unseen tasks when provided with their language instructions? To facilitate progress in this goal, we introduce N ATURAL -I NSTRUCTIONS v 2 , a collection of 1,600+ diverse language tasks and their expert written instructions. More impor-tantly, the benchmark covers 70+ distinct task types, such as tagging, in-\ufb01lling, and rewriting. This benchmark is collected with contributions", "topic": "\"Advanced AI and NLP Research\""}, {"title": "A Call for Clarity in Beam Search: How It Works and When It Stops", "authors": "Jungo Kasai, Keisuke Sakaguchi, Ronan Le Bras, Dragomir R. Radev, Yejin Choi, Noah A. Smith", "year": 2022, "url": "https://www.semanticscholar.org/paper/f290bc1c70688b7985b7cca863e4670398875595", "abstract": "Text generation with beam search has proven successful in a wide range of applications. We point out that, though largely overlooked in the literature, the commonly-used implementation of beam decoding (e.g., Hugging Face Transformers and fairseq) uses a first come, first served heuristic: it keeps a set of already completed sequences over time steps and stops when the size of this set reaches the beam size. Based on this finding, we introduce a patience factor, a simple modification to this beam decoding implementation, that generalizes the stopping criterion and provides flexibility to the depth of search. Empirical results demonstrate that adjusting this patience factor improves decoding performance of strong pretrained models on news text summarization and machine translation over diverse language pairs, with a negligible inference slowdown. Our approach only modifies one line of code and can be thus readily incorporated in any implementation. Further, we find that different versions of beam decoding result in large performance differences in summarization, demonstrating the need for clarity in specifying the beam search implementation in research work. Our code will be available upon publication.", "topic": "\"Advanced Text Generation Techniques\""}, {"title": "SODA: Million-scale Dialogue Distillation with Social Commonsense Contextualization", "authors": "Hyunwoo Kim, Jack Hessel, Liwei Jiang, Peter West, Ximing Lu, Youngjae Yu, Pei Zhou, Ronan Le Bras, Malihe Alikhani, Gunhee Kim, Maarten Sap, Yejin Choi", "year": 2022, "url": "https://www.semanticscholar.org/paper/f78fe02f681a0a9a6867b007bd39e3884de64a91", "abstract": "Data scarcity has been a long standing issue in the field of open-domain social dialogue. To quench this thirst, we present SODA: the first publicly available, million-scale high-quality social dialogue dataset. By contextualizing social commonsense knowledge from a knowledge graph, we are able to distill an exceptionally broad spectrum of social interactions from a large language model. Human evaluation shows that conversations in SODA are more consistent, specific, and (surprisingly) natural than those in prior human-authored datasets. Using SODA, we train COSMO: a generalizable conversation model that is significantly more natural and consistent on unseen datasets than best-performing conversation models (e.g., GODEL, BlenderBot-1, Koala, Vicuna). Experiments reveal COSMO is sometimes even preferred to the original human-written gold responses. Additionally, our results shed light on the distinction between knowledge-enriched conversations and natural social chitchats. We plan to make our data, model, and code public.", "topic": "\"Commonsense Knowledge in AI\""}], "analysis": "1. **Controlled Text Generation with Monte-Carlo Tree Search (MCTS) and Proximal Policy Optimization (PPO)**\n2. **Flexible Sequence-to-Sequence Model Compression using Modular Transformers**\n3. **Sensitive Question and Acceptable Response Dataset for Safer Language Models**\n4. **Commonsense Knowledge Transfer to Pre-trained Language Models**\n5. **Symbolic Chain-of-Thought Distillation for Smaller Language Models**\n6. **Compositional Reasoning in Transformer Language Models**\n7. **Visual Contexts in Neural Conversational Models**\n8. **Commonsense Knowledge and Inference-Time Reasoning Algorithms**\n9. **Human Utility of Machine-Generated Rationales**\n10. **Task-Specific Dataset Distillation from Off-the-Shelf Language Models**\n11. **Commonsense Verification with Retrospective Reasoning**\n12. **Multimodal Commonsense Reasoning with Reinforcement Learning**\n13. **Inference-Time Policy Adapters for Tailoring Language Models**", "author_name": "Yejin Choi", "coauthors_histogram": {"Jiacheng Liu": 4, "Andrew Cohen": 1, "Ramakanth Pasunuru": 1, "Hannaneh Hajishirzi": 9, "Asli Celikyilmaz": 1, "Wangchunshu Zhou": 2, "Ronan Le Bras": 13, "Zhaoxuan Tan": 1, "Shangbin Feng": 1, "Melanie Sclar": 4, "Herun Wan": 1, "Minnan Luo": 1, "Yulia Tsvetkov": 3, "Hwaran Lee": 1, "Seokhee Hong": 1, "Joonsuk Park": 1, "Takyoung Kim": 1, "M. Cha": 1, "Byoung Pil Kim": 1, "Gunhee Kim": 5, "Eun-Ju Lee": 1, "Yong Lim": 1, "Alice Oh": 1, "San-hee Park": 1, "Jung-Woo Ha": 1, "Kailai Sun": 1, "Dian Yu": 1, "Jianshu Chen": 1, "Dong Yu": 1, "Zhixing Tian": 1, "Yuanzhe Zhang": 1, "Kang Liu": 1, "Jun Zhao": 1, "S\u00f8ren Christian Winther Topp": 1, "D. \u00d8stergaard": 1, "A. Vaidyam": 1, "Hannah Wisniewski": 1, "J. Halamka": 1, "M. S. Kashavan": 1, "Shuailiang Zhang": 1, "Hai Zhao": 1, "Zhuosheng Yuwei Wu": 1, "Qiuyuan Huang": 3, "J. Park": 6, "Abhinav Gupta": 2, "Pan Lu": 2, "Paul N. Bennett": 2, "Ran Gong": 2, "Subhojit Som": 3, "Baolin Peng": 2, "O. Mohammed": 2, "C. Pal": 2, "Jianfeng Gao": 3, "Liunian Harold Li": 1, "Jack Hessel": 11, "Youngjae Yu": 8, "Xiang Ren": 7, "Kai-Wei Chang": 1, "Nouha Dziri": 4, "Ximing Lu": 17, "Xiang Lorraine Li": 2, "Liwei Jian": 1, "Bill Yuchen Lin": 3, "Peter West": 12, "Chandra Bhagavatula": 10, "Jena D. Hwang": 9, "Soumya Sanyal": 2, "Sean Welleck": 9, "Allyson Ettinger": 1, "Za\u00efd Harchaoui": 2, "Seungju Han": 1, "Julia Mendelsohn": 1, "Maarten Sap": 11, "Brihi Joshi": 1, "Ziyi Liu": 1, "Sahana Ramnath": 2, "Aaron Chan": 1, "Zhewei Tong": 1, "Shaoliang Nie": 1, "Qifan Wang": 1, "Jaehun Jung": 2, "Liwei Jiang": 9, "Faeze Brahman": 7, "Jillian R. Fisher": 2, "Taylor Sorensen": 2, "Wenya Wang": 1, "Dianzhuo Wang": 1, "Noah A. Smith": 11, "Jiwan Chung": 2, "Heeseung Yun": 2, "Rowan Zellers": 6, "Prithviraj Ammanabrolu": 8, "Jinho Oh": 1, "Jaeuk Shin": 1, "Kyutae Kim": 1, "Jung Kyoo Lee": 1, "Jaehun Jang": 1, "Khyathi Raghavi Chandu": 1, "Abhilasha Ravichander": 1, "Lianhui Qin": 4, "Skyler Hallinan": 3, "Valentina Pyatkin": 4, "Hirona J. Arai": 1, "Keisuke Sakaguchi": 5, "Sydney Levine": 1, "Kavel Rao": 1, "J. Tasioulas": 1, "Yicheng Fu": 1, "Karina Yang": 1, "Shiyu Huang": 1, "Sachin Kumar": 2, "Alane Suhr": 3, "Natalie Shapira": 1, "Mosh Levy": 1, "S. Alavi": 1, "Xuhui Zhou": 1, "Yoav Goldberg": 1, "Vered Shwartz": 1, "Wanrong Zhu": 1, "Anas Awadalla": 1, "S. Gadre": 1, "Jesse Dodge": 1, "Alex Fang": 1, "Ludwig Schmidt": 1, "William Yang Wang": 1, "Alisa Liu": 3, "Zhaofeng Wu": 1, "Julian Michael": 1, "Alexander Koller": 1, "Swabha Swayamdipta": 7, "Phillip Howard": 2, "Junlin Wang": 1, "Vasudev Lal": 2, "Gadi Singer": 2, "Kolby Nottingham": 1, "Sameer Singh": 1, "Roy Fox": 1, "Yizhong Wang": 2, "Swaroop Mishra": 2, "Pegah Alipoormolabashi": 2, "Yeganeh Kordi": 2, "Amirreza Mirzaei": 2, "Anjana Arunkumar": 2, "Arjun Ashok": 2, "Arut Selvan Dhanasekaran": 2, "Atharva Naik": 2, "David Stap": 2, "Eshaan Pathak": 2, "Giannis Karamanolakis": 2, "H. Lai": 2, "I. Purohit": 2, "Ishani Mondal": 2, "Jacob Anderson": 2, "Kirby Kuznia": 2, "Krima Doshi": 2, "Maitreya Patel": 2, "Kuntal Kumar Pal": 2, "M. Moradshahi": 2, "Mihir Parmar": 2, "Mirali Purohit": 2, "Neeraj Varshney": 2, "Phani Rohitha Kaza": 2, "Pulkit Verma": 2, "Ravsehaj Singh Puri": 2, "Rushang Karia": 2, "Shailaja Keyur Sampat": 2, "Savan Doshi": 2, "Siddhartha Mishra": 2, "Sujan Reddy": 2, "Sumanta Patro": 2, "Tanay Dixit": 2, "Xudong Shen": 2, "Chitta Baral": 2, "Daniel Khashabi": 5, "Daniel Fried": 1, "Saadia Gabriel": 1, "Hamid Palangi": 1, "Hyunwoo Kim": 2, "Jiasen Lu": 1, "Yanpeng Zhao": 1, "Mohammadreza Salehi": 1, "Aditya Kusupati": 1, "Ali Farhadi": 3, "Krishna Pillutla": 1, "Lang Liu": 1, "John Thickstun": 1, "Sewoong Oh": 1, "T. Shen": 1, "A. Jafarpour": 3, "J. Pennebaker": 3, "Eric Horvitz": 1, "E. Horvitz": 2, "Vivek Srikumar": 2, "Ana Marasovi\u0107": 1, "Lillian Lee": 1, "Jeff Da": 1, "Robert Mankoff": 1, "Kung-Hsiang Huang": 1, "Preslav Nakov": 1, "Heng Ji": 1, "Doug Downey": 2, "Pengfei He": 1, "Anna Rohrbach": 2, "Kate Saenko": 1, "Rajkumar Ramamurthy": 1, "Kiant\u00e9 Brantley": 1, "R. Sifa": 1, "C. Bauckhage": 1, "Sheng Shen": 1, "Trevor Darrell": 1, "J. Sung": 1, "Yonatan Bisk": 1, "Chris Quirk": 1, "Michel Galley": 1, "Emily Allaway": 1, "K. McKeown": 1, "Jungo Kasai": 3, "Yoichi Takahashi": 1, "Akari Asai": 1, "Xinyan Velocity Yu": 1, "Dragomir R. Radev": 3, "Kentaro Inui": 1, "Hao Peng": 1, "Hanjie Chen": 1, "Yangfeng Ji": 1, "Pei Zhou": 1, "Malihe Alikhani": 1}, "paper_topics": {}, "cluster_trends": {"\"Advanced Text Generation Techniques\"": {"2023": 2, "2022": 9, "2024": 0}, "\"Advancements in Language Model Compression and Reasoning\"": {"2023": 6, "2022": 1, "2024": 0}, "\"Advanced AI and NLP Research\"": {"2023": 8, "2022": 10, "2024": 0}, "\"Commonsense Knowledge in AI\"": {"2023": 6, "2022": 6, "2024": 0}, "\"Multimodal Language Models and Reinforcement Learning\"": {"2023": 4, "2022": 3, "2024": 0}, "\"Language Models and Narrative Analysis\"": {"2023": 1, "2022": 3, "2024": 0}, "\"Socially Responsible AI Systems\"": {"2022": 6, "2023": 0, "2024": 0}, "\"AI Model Evaluation Metrics\"": {"2022": 1, "2023": 0, "2024": 0}}}, "5870be96021671ec152015efcd5b8904": {"authorId": "143643017", "start_year": 2022, "end_year": 2024, "papers": [{"title": "The 2023 ACR/EULAR Antiphospholipid Syndrome Classification Criteria", "authors": "M. Barbhaiya, S. Zuily, Ray Naden, Alison M Hendry, F. Manneville, Mary-Carmen Amigo, Zahir Amoura, D. Andrade, L. Andreoli, B. Artim-Esen, T. Atsumi, T. Av\u010din, H. Belmont, M. Bertolaccini, D. W. Branch, G. Carvalheiras, Alessandro Casini, Ricard Cervera, Hannah Cohen, N. Costedoat-Chalumeau, M. Crowther, G. Jes\u00fas, A. Delluc, Sheetal Desai, M. D. De Sancho, K. Devreese, R. Diz\u2010Kucukkaya, A. Duarte-Garc\u00eda, Camille Franc\u00e8s, David Garcia, Jean-Christophe Gris, Natasha Jordan, R. K. Leaf, N. Kello, J. Knight, Carl A. Laskin, Alfred I. Lee, K. Legault, Steve R Levine, R. Levy, M. Limper, M. Lockshin, K. Mayer-Pickel, J. Musial, Marcus John Beasley, G. Orsolini, T. Ortel, V. Pengo, Michelle A Petri, G. Pons-Estel, J. G\u00f3mez-Puerta, Quentin Raimboug, R. Roubey, G. Sanna, S. Seshan, S. Sciascia, M. Tektonidou, A. Tincani, Denis Wahl, R. Willis, C. Yelnik, Catherine Zuily, F. Guillemin, K. Costenbader, D. Erkan", "year": 2023, "url": "https://www.semanticscholar.org/paper/6854b260d54a529519f01e02887bf51f7c73bb46", "abstract": "To develop new antiphospholipid syndrome (APS) classification criteria with high specificity for use in observational studies and trials, jointly supported by the American College of Rheumatology (ACR) and EULAR.", "topic": "\"APS Classification Criteria\""}, {"title": "2023 ACR/EULAR antiphospholipid syndrome classification criteria", "authors": "M. Barbhaiya, S. Zuily, R. Naden, Alison M Hendry, F. Manneville, M. Amigo, Z. Amoura, D. Andrade, L. Andreoli, B. Artim-Esen, T. Atsumi, T. Av\u010din, M. Belmont, M. Bertolaccini, D. Branch, G. Carvalheiras, A. Casini, R. Cervera, Hannah Cohen, N. Costedoat-Chalumeau, M. Crowther, G. D. de Jes\u00fas, A. Delluc, S. Desai, M. D. De Sancho, K. Devreese, R. Diz\u2010Kucukkaya, A. Duarte-Garc\u00eda, C. France\u0300s, David A. Garcia, J. Gris, N. Jordan, R. K. Leaf, N. Kello, J. Knight, C. Laskin, A. Lee, K. Legault, Steve R Levine, R. Levy, M. Limper, M. Lockshin, K. Mayer-Pickel, J. Musial, P. Meroni, G. Orsolini, T. Ortel, V. Pengo, M. Petri, G. Pons-Estel, J. G\u00f3mez-Puerta, Quentin Raimboug, R. Roubey, G. Sanna, S. Seshan, S. Sciascia, M. Tektonidou, A. Tincani, D. Wahl, R. Willis, C. Yelnik, Catherine Zuily, F. Guillemin, K. Costenbader, D. Erkan", "year": 2023, "url": "https://www.semanticscholar.org/paper/b44f9cfc69e20cc26fd18943e4ca41244a86e6bd", "abstract": "Objective To develop new antiphospholipid syndrome (APS) classification criteria with high specificity for use in observational studies and trials, jointly supported by the American College of Rheumatology (ACR) and EULAR. Methods This international multidisciplinary initiative included four phases: (1) Phase I, criteria generation by surveys and literature review; (2) Phase II, criteria reduction by modified Delphi and nominal group technique exercises; (3) Phase III, criteria definition, further reduction with the guidance of real-world patient scenarios, and weighting via consensus-based multicriteria decision analysis, and threshold identification; and (4) Phase IV, validation using independent adjudicators\u2019 consensus as the gold standard. Results The 2023 ACR/EULAR APS classification criteria include an entry criterion of at least one positive antiphospholipid antibody (aPL) test within 3 years of identification of an aPL-associated clinical criterion, followed by additive weighted criteria (score range 1\u20137 points each) clustered into six clinical domains (macrovascular venous thromboembolism, macrovascular arterial thrombosis, microvascular, obstetric, cardiac valve, and hematologic) and two laboratory domains (lupus anticoagulant functional coagulation assays, and solid-phase enzyme-linked immunosorbent assays for IgG/IgM anticardiolipin and/or IgG/IgM anti\u2013\u03b22-glycoprotein I antibodies). Patients accumulating at least three points each from the clinical and laboratory domains are classified as having APS. In the validation cohort, the new APS criteria vs the 2006 revised Sapporo classification criteria had a specificity of 99% vs 86%, and a sensitivity of 84% vs 99%. Conclusion These new ACR/EULAR APS classification criteria were developed using rigorous methodology with multidisciplinary international input. Hierarchically clustered, weighted, and risk-stratified criteria reflect the current thinking about APS, providing high specificity and a strong foundation for future APS research.", "topic": "\"APS Classification Criteria\""}, {"title": "Global comment on the use of hydroxychloroquine during the periconception period and pregnancy in women with autoimmune diseases.", "authors": "K. Schreiber, I. Giles, N. Costedoat-Chalumeau, C. Nelson-Piercy, R. Dolhain, M. Mosca, F. F\u00f6rger, R. Fischer-Betz, A. Molt\u00f3, A. Tincani, E. Pasquier, B. Marin, \u00c9. \u00c9lefant, Jane Salmon, B. Bermas, L. Sammaritano, M. Clowse, Christina Chambers, J. Buyon, S. Inoue, N. Agmon-Levin, Silvia Aguilera, S. Emadi, J. Andersen, D. Andrade, A. Antovic, L. Arnaud, A. Christiansen, T. Av\u010din, Sara Badreh-Wirstr\u00f6m, G. Bertsias, I. Bini, A. Bobirc\u0103, W. Branch, Antonio Brucato, I. Bultink, S. Capela, I. Cecchi, R. Cervera, C. Chighizola, C. Cobilinschi, M. Cuadrado, D. Dey, O. Etomi, G. Espinosa, J. Flint, J. Fonseca, R. Fritsch-Stork, M. Gerosa, B. Glintborg, C. Skorpen, B. Goulden, C. Graversgaard, I. Gunnarsson, Latika Gupta, M. Hetland, Kenneth J. Hodson, B. Hunt, D. Isenberg, S. Jacobsen, M. Khamashta, R. Levy, L. Linde, J. Lykke, Y. Meissner, L. Moore, Eric F Morand, S. Navarra, D. Opri\u0219-Belinski, M. \u00d8stensen, H. Ozawa, L. F. Perez-Garcia, Michelle Petri, G. Pons-Estel, M. Radin, L. Raio, A. Rottenstreich, G. Ruiz\u2010Irastorza, Sla\u0111ana Rumpl Tunji\u0107, M. Rygg, S. Sciascia, A. Strangfeld, E. Svenungsson, M. Tektonidou, A. Troldborg, \u00c9. Vinet, J. Vojinovi\u0107, A. Voss, M. Wallenius, L. Andreoli", "year": 2023, "url": "https://www.semanticscholar.org/paper/fb7fba867c0fb331718696c31fd49ff3e2a4b483", "abstract": null, "topic": "Unknown"}, {"title": "Unsupervised Discontinuous Constituency Parsing with Mildly Context-Sensitive Grammars", "authors": "Songlin Yang, R. Levy, Yoon Kim", "year": 2022, "url": "https://www.semanticscholar.org/paper/52fa59615baf31e1085450cd31d1c7c23968dbd0", "abstract": "We study grammar induction with mildly context-sensitive grammars for unsupervised discontinuous parsing. Using the probabilistic linear context-free rewriting system (LCFRS) formalism, our approach fixes the rule structure in advance and focuses on parameter learning with maximum likelihood. To reduce the computational complexity of both parsing and parameter estimation, we restrict the grammar formalism to LCFRS-2 (i.e., binary LCFRS with fan-out two) and further discard rules that require O(l6) time to parse, reducing inference to O(l5). We find that using a large number of nonterminals is beneficial and thus make use of tensor decomposition-based rank-space dynamic programming with an embedding-based parameterization of rule probabilities to scale up the number of nonterminals. Experiments on German and Dutch show that our approach is able to induce linguistically meaningful trees with continuous and discontinuous structures.", "topic": "\"Advanced Computational Linguistics\""}, {"title": "Letter to the Editor: indirect treatment comparison of anifrolumab efficacy versus belimumab in adults with systemic lupus erythematosus.", "authors": "Nick Ballew, A. Mian, R. Levy, Matt Bradley", "year": 2022, "url": "https://www.semanticscholar.org/paper/deaad7ca615b6956dcad964893a8496ffae16a39", "abstract": null, "topic": "Unknown"}, {"title": "Trading off Utility, Informativeness, and Complexity in Emergent Communication", "authors": "Mycal Tucker, R. Levy, J. Shah, Noga Zaslavsky", "year": 2022, "url": "https://www.semanticscholar.org/paper/e9ad792984d9d1076b264219933d093010a022db", "abstract": "Emergent communication (EC) research often focuses on optimizing task-specific utility as a driver for communication. However, there is increasing evidence that human languages are shaped by task-general communicative constraints and evolve under pressure to optimize the Information Bottleneck (IB) tradeoff between the informativeness and complexity of the lexicon. Here, we integrate these two approaches by trading off utility, informativeness, and complexity in EC. To this end, we propose Vector-Quantized Variational Information Bottleneck (VQ-VIB), a method for training neural agents to encode inputs into discrete signals embedded in a continuous space. We evaluate our approach in multi-agent reinforcement learning settings and in color reference games and show that: (1) VQ-VIB agents can continuously adapt to changing communicative needs and, in the color domain, align with human languages; (2) the emergent VQ-VIB embedding spaces are semantically meaningful and perceptually grounded; and (3) encouraging informativeness leads to faster convergence rates and improved utility, both in VQ-VIB and in prior neural architectures for symbolic EC, with VQ-VIB achieving higher utility for any given complexity. This work offers a new framework for EC that is grounded in information-theoretic principles that are believed to characterize human language evolution and that may facilitate human-agent interaction.", "topic": "\"Advanced Computational Linguistics\""}, {"title": "Which presuppositions are subject to contextual felicity constraints? *", "authors": "Ethan Gotlieb Wilcox, R. Levy, Kathryn Davidson", "year": 2022, "url": "https://www.semanticscholar.org/paper/f4be90c5c58079b659584a4db8e6cd405be45b93", "abstract": "Some sentences with presupposition triggers can be felicitously uttered when their presuppositions are not entailed by the context, whereas others are infelicitous in such environments, a phenomenon known as Missing Accommodation / Informative Presupposition or varying Contextual Felicity Constraints (CFCs). Despite an abundance of recent quantitative work on presuppositions, this aspect of their behavior has received less attention via experimentation. Here, we present the results from a semantic rating study testing the relative CFC strength of thirteen presupposition triggers, making this the largest cross-trigger comparison reported in the literature to date. The results support a three-way categorical analysis of presupposition triggers, based on imposing strong, weak, or no CFCs. We observe that strong CFC triggers are all focus-associating, suggesting that (at least some of the) variation in behavior arises due to naturally-occurring semantic classes. We compare our results to three previous proposals for CFC variation and argue that none yet account for the full empirical picture.", "topic": "\"Presupposition Triggers in Language\""}], "analysis": "1. Grammar induction with mildly context-sensitive grammars for unsupervised discontinuous parsing.\n2. Use of probabilistic linear context-free rewriting systems (LCFRS) for parameter learning in grammar induction.\n3. Tensor decomposition-based rank-space dynamic programming for scaling nonterminals in grammar induction.\n4. Emergent communication in multi-agent reinforcement learning settings.\n5. Vector-Quantized Variational Information Bottleneck (VQ-VIB) for encoding inputs into discrete signals.\n6. Information-theoretic principles in emergent communication and human language evolution.\n7. Semantic rating study on presupposition triggers and Contextual Felicity Constraints (CFCs).\n8. Cross-trigger comparison of presupposition triggers based on CFC strength.", "author_name": "R. Levy", "coauthors_histogram": {"M. Barbhaiya": 2, "S. Zuily": 2, "Ray Naden": 1, "Alison M Hendry": 2, "F. Manneville": 2, "Mary-Carmen Amigo": 1, "Zahir Amoura": 1, "D. Andrade": 3, "L. Andreoli": 3, "B. Artim-Esen": 2, "T. Atsumi": 2, "T. Av\u010din": 3, "H. Belmont": 1, "M. Bertolaccini": 2, "D. W. Branch": 1, "G. Carvalheiras": 2, "Alessandro Casini": 1, "Ricard Cervera": 1, "Hannah Cohen": 2, "N. Costedoat-Chalumeau": 3, "M. Crowther": 2, "G. Jes\u00fas": 1, "A. Delluc": 2, "Sheetal Desai": 1, "M. D. De Sancho": 2, "K. Devreese": 2, "R. Diz\u2010Kucukkaya": 2, "A. Duarte-Garc\u00eda": 2, "Camille Franc\u00e8s": 1, "David Garcia": 1, "Jean-Christophe Gris": 1, "Natasha Jordan": 1, "R. K. Leaf": 2, "N. Kello": 2, "J. Knight": 2, "Carl A. Laskin": 1, "Alfred I. Lee": 1, "K. Legault": 2, "Steve R Levine": 2, "M. Limper": 2, "M. Lockshin": 2, "K. Mayer-Pickel": 2, "J. Musial": 2, "Marcus John Beasley": 1, "G. Orsolini": 2, "T. Ortel": 2, "V. Pengo": 2, "Michelle A Petri": 1, "G. Pons-Estel": 3, "J. G\u00f3mez-Puerta": 2, "Quentin Raimboug": 2, "R. Roubey": 2, "G. Sanna": 2, "S. Seshan": 2, "S. Sciascia": 3, "M. Tektonidou": 3, "A. Tincani": 3, "Denis Wahl": 1, "R. Willis": 2, "C. Yelnik": 2, "Catherine Zuily": 2, "F. Guillemin": 2, "K. Costenbader": 2, "D. Erkan": 2, "R. Naden": 1, "M. Amigo": 1, "Z. Amoura": 1, "M. Belmont": 1, "D. Branch": 1, "A. Casini": 1, "R. Cervera": 2, "G. D. de Jes\u00fas": 1, "S. Desai": 1, "C. France\u0300s": 1, "David A. Garcia": 1, "J. Gris": 1, "N. Jordan": 1, "C. Laskin": 1, "A. Lee": 1, "P. Meroni": 1, "M. Petri": 1, "D. Wahl": 1, "K. Schreiber": 1, "I. Giles": 1, "C. Nelson-Piercy": 1, "R. Dolhain": 1, "M. Mosca": 1, "F. F\u00f6rger": 1, "R. Fischer-Betz": 1, "A. Molt\u00f3": 1, "E. Pasquier": 1, "B. Marin": 1, "\u00c9. \u00c9lefant": 1, "Jane Salmon": 1, "B. Bermas": 1, "L. Sammaritano": 1, "M. Clowse": 1, "Christina Chambers": 1, "J. Buyon": 1, "S. Inoue": 1, "N. Agmon-Levin": 1, "Silvia Aguilera": 1, "S. Emadi": 1, "J. Andersen": 1, "A. Antovic": 1, "L. Arnaud": 1, "A. Christiansen": 1, "Sara Badreh-Wirstr\u00f6m": 1, "G. Bertsias": 1, "I. Bini": 1, "A. Bobirc\u0103": 1, "W. Branch": 1, "Antonio Brucato": 1, "I. Bultink": 1, "S. Capela": 1, "I. Cecchi": 1, "C. Chighizola": 1, "C. Cobilinschi": 1, "M. Cuadrado": 1, "D. Dey": 1, "O. Etomi": 1, "G. Espinosa": 1, "J. Flint": 1, "J. Fonseca": 1, "R. Fritsch-Stork": 1, "M. Gerosa": 1, "B. Glintborg": 1, "C. Skorpen": 1, "B. Goulden": 1, "C. Graversgaard": 1, "I. Gunnarsson": 1, "Latika Gupta": 1, "M. Hetland": 1, "Kenneth J. Hodson": 1, "B. Hunt": 1, "D. Isenberg": 1, "S. Jacobsen": 1, "M. Khamashta": 1, "L. Linde": 1, "J. Lykke": 1, "Y. Meissner": 1, "L. Moore": 1, "Eric F Morand": 1, "S. Navarra": 1, "D. Opri\u0219-Belinski": 1, "M. \u00d8stensen": 1, "H. Ozawa": 1, "L. F. Perez-Garcia": 1, "Michelle Petri": 1, "M. Radin": 1, "L. Raio": 1, "A. Rottenstreich": 1, "G. Ruiz\u2010Irastorza": 1, "Sla\u0111ana Rumpl Tunji\u0107": 1, "M. Rygg": 1, "A. Strangfeld": 1, "E. Svenungsson": 1, "A. Troldborg": 1, "\u00c9. Vinet": 1, "J. Vojinovi\u0107": 1, "A. Voss": 1, "M. Wallenius": 1, "Songlin Yang": 1, "Yoon Kim": 1, "Nick Ballew": 1, "A. Mian": 1, "Matt Bradley": 1, "Mycal Tucker": 1, "J. Shah": 1, "Noga Zaslavsky": 1, "Ethan Gotlieb Wilcox": 1, "Kathryn Davidson": 1}, "paper_topics": {}, "cluster_trends": {"\"APS Classification Criteria\"": {"2023": 2, "2022": 0, "2024": 0}, "\"Advanced Computational Linguistics\"": {"2022": 2, "2023": 0, "2024": 0}, "\"Presupposition Triggers in Language\"": {"2022": 1, "2023": 0, "2024": 0}}}, "f14aa616ed32087e2ee24aafaf5b5d6d": {"authorId": "2121801234", "start_year": 2021, "end_year": 2024, "papers": [{"title": "Human-Guided Complexity-Controlled Abstractions", "authors": "Andi Peng, Mycal Tucker, Eoin M. Kenny, Noga Zaslavsky, Pulkit Agrawal, Julie A. Shah", "year": 2023, "url": "https://www.semanticscholar.org/paper/9ada63c92c53af1d13813a182180e910efc0d3c0", "abstract": "Neural networks often learn task-specific latent representations that fail to generalize to novel settings or tasks. Conversely, humans learn discrete representations (i.e., concepts or words) at a variety of abstraction levels (e.g.,\"bird\"vs.\"sparrow\") and deploy the appropriate abstraction based on task. Inspired by this, we train neural models to generate a spectrum of discrete representations, and control the complexity of the representations (roughly, how many bits are allocated for encoding inputs) by tuning the entropy of the distribution over representations. In finetuning experiments, using only a small number of labeled examples for a new task, we show that (1) tuning the representation to a task-appropriate complexity level supports the highest finetuning performance, and (2) in a human-participant study, users were able to identify the appropriate complexity level for a downstream task using visualizations of discrete representations. Our results indicate a promising direction for rapid model finetuning by leveraging human insight.", "topic": "Emergent Communication Systems"}, {"title": "Artificial Neural Network Language Models Predict Human Brain Responses to Language Even After a Developmentally Realistic Amount of Training", "authors": "Eghbal A. Hosseini, Martin Schrimpf, Yian Zhang, Samuel R. Bowman, Noga Zaslavsky, Evelina Fedorenko", "year": 2023, "url": "https://www.semanticscholar.org/paper/ac70fb2c74aa87420878c441c3d24969947e0294", "abstract": "Artificial neural networks have emerged as computationally plausible models of human language processing. A major criticism of these models is that the amount of training data they receive far exceeds that of humans during language learning. Here, we use two complementary approaches to ask how the models\u2019 ability to capture human fMRI responses to sentences is affected by the amount of training data. First, we evaluate GPT-2 models trained on 1 million, 10 million, 100 million, or 1 billion words against an fMRI benchmark. We consider the 100-million-word model to be developmentally plausible in terms of the amount of training data given that this amount is similar to what children are estimated to be exposed to during the first 10 years of life. Second, we test the performance of a GPT-2 model trained on a 9-billion-token dataset to reach state-of-the-art next-word prediction performance on the human benchmark at different stages during training. Across both approaches, we find that (i) the models trained on a developmentally plausible amount of data already achieve near-maximal performance in capturing fMRI responses to sentences. Further, (ii) lower perplexity\u2014a measure of next-word prediction performance\u2014is associated with stronger alignment with human data, suggesting that models that have received enough training to achieve sufficiently high next-word prediction performance also acquire representations of sentences that are predictive of human fMRI responses. In tandem, these findings establish that although some training is necessary for the models\u2019 predictive ability, a developmentally realistic amount of training (\u223c100 million words) may suffice.", "topic": "Neural Networks & Language Processing"}, {"title": "Teasing apart the representational spaces of ANN language models to discover key axes of model-to-brain alignment", "authors": "Eghbal A. Hosseini, Noga Zaslavsky, Colton Casto, Evelina Fedorenko", "year": 2023, "url": "https://www.semanticscholar.org/paper/cac6135acc25e4ac84fc3d9961fad2eea3143e80", "abstract": null, "topic": "Unknown"}, {"title": "Evidence for a language-independent conceptual representation of pronominal referents", "authors": "Mora Maldonado, Noga Zaslavsky, Jennifer Culbertson", "year": 2023, "url": "https://www.semanticscholar.org/paper/d3c22b78f6e48414f2976d59b614e4fc86b5b2dc", "abstract": null, "topic": "Unknown"}, {"title": "The information geometry of pragmatic reasoning", "authors": "Noga Zaslavsky", "year": 2022, "url": "https://www.semanticscholar.org/paper/232756033cf58d27a4bf17685e8b67b4fc93652e", "abstract": null, "topic": "Unknown"}, {"title": "Towards Human-Agent Communication via the Information Bottleneck Principle", "authors": "Mycal Tucker, J. Shah, R. Levy, Noga Zaslavsky", "year": 2022, "url": "https://www.semanticscholar.org/paper/272de5c8dabceee66e8bdb5804f944da8375cd2f", "abstract": "\u2014Emergent communication research often focuses on optimizing task-speci\ufb01c utility as a driver for communication. However, human languages appear to evolve under pressure to ef\ufb01ciently compress meanings into communication signals by optimizing the Information Bottleneck tradeoff between informa- tiveness and complexity. In this work, we study how trading off these three factors \u2014 utility, informativeness, and complexity \u2014 shapes emergent communication, including compared to human communication. To this end, we propose Vector-Quantized Vari- ational Information Bottleneck (VQ-VIB), a method for training neural agents to compress inputs into discrete signals embedded in a continuous space. We train agents via VQ-VIB and compare their performance to previously proposed neural architectures in grounded environments and in a Lewis reference game. Across all neural architectures and settings, taking into account com- municative informativeness bene\ufb01ts communication convergence rates, and penalizing communicative complexity leads to human- like lexicon sizes while maintaining high utility. Additionally, we \ufb01nd that VQ-VIB outperforms other discrete communication methods. This work demonstrates how fundamental principles that are believed to characterize human language evolution may inform emergent communication in arti\ufb01cial agents.", "topic": "Emergent Communication Systems"}, {"title": "Teasing apart models of pragmatics using optimal reference game design", "authors": "Irene Zhou, Jennifer Hu, Roger Levy, Noga Zaslavsky", "year": 2022, "url": "https://www.semanticscholar.org/paper/277ed57913a4f273335845db61def9a86829d95e", "abstract": null, "topic": "Unknown"}, {"title": "The emergence of discrete and systematic communication in a continuous signal-meaning space", "authors": "Alicia M. Chen, Matthias Hofer, Moshe Poliak, Roger Levy, Noga Zaslavsky", "year": 2022, "url": "https://www.semanticscholar.org/paper/51d5df146007299a1c9ecd8d1381e5fcdc996df1", "abstract": null, "topic": "Unknown"}, {"title": "Artificial neural network language models align neurally and behaviorally with humans even after a developmentally realistic amount of training", "authors": "Eghbal A. Hosseini, Martin Schrimpf, Yian Zhang, Samuel R. Bowman, Noga Zaslavsky, Evelina Fedorenko", "year": 2022, "url": "https://www.semanticscholar.org/paper/d461aef562bd9c13dc0f972f147df8be1fcca63c", "abstract": "Artificial neural networks have emerged as computationally plausible models of human language processing. A major criticism of these models is that the amount of training data they receive far exceeds that of humans during language learning. Here, we use two complementary approaches to ask how the models\u2019 ability to capture human neural and behavioral responses to language is affected by the amount of training data. First, we evaluate GPT-2 models trained on 1 million, 10 million, 100 million, or 1 billion tokens against two fMRI benchmarks and one behavioral (reading times) benchmark. Because children are exposed to approximately 100 million words during the first 10 years of life, we consider the 100-million-token model developmentally plausible. Second, we test the performance of a GPT-2 model that is trained on a 9-billion dataset to reach state-of-the-art next-word prediction performance against the same human benchmarks at different stages during training. Across both approaches, we find that (i) the models trained on a developmentally plausible amount of data already achieve near-maximal performance in capturing neural and behavioral responses to language. Further, (ii) lower perplexity\u2014a measure of next-word prediction performance\u2014is associated with stronger alignment with the human benchmarks, suggesting that models that have received enough training to achieve sufficiently high next-word prediction performance also acquire human-like representations of the linguistic input. In tandem, these findings establish that although some training is necessary for the models\u2019 ability to predict human responses to language, a developmentally realistic amount of training (~100 million tokens) may suffice.", "topic": "Neural Networks & Language Processing"}, {"title": "Trading off Utility, Informativeness, and Complexity in Emergent Communication", "authors": "Mycal Tucker, R. Levy, J. Shah, Noga Zaslavsky", "year": 2022, "url": "https://www.semanticscholar.org/paper/e9ad792984d9d1076b264219933d093010a022db", "abstract": "Emergent communication (EC) research often focuses on optimizing task-specific utility as a driver for communication. However, there is increasing evidence that human languages are shaped by task-general communicative constraints and evolve under pressure to optimize the Information Bottleneck (IB) tradeoff between the informativeness and complexity of the lexicon. Here, we integrate these two approaches by trading off utility, informativeness, and complexity in EC. To this end, we propose Vector-Quantized Variational Information Bottleneck (VQ-VIB), a method for training neural agents to encode inputs into discrete signals embedded in a continuous space. We evaluate our approach in multi-agent reinforcement learning settings and in color reference games and show that: (1) VQ-VIB agents can continuously adapt to changing communicative needs and, in the color domain, align with human languages; (2) the emergent VQ-VIB embedding spaces are semantically meaningful and perceptually grounded; and (3) encouraging informativeness leads to faster convergence rates and improved utility, both in VQ-VIB and in prior neural architectures for symbolic EC, with VQ-VIB achieving higher utility for any given complexity. This work offers a new framework for EC that is grounded in information-theoretic principles that are believed to characterize human language evolution and that may facilitate human-agent interaction.", "topic": "Emergent Communication Systems"}, {"title": "Generalization and Translatability in Emergent Communication via Informational Constraints", "authors": "Mycal Tucker, J. Shah, R. Levy, Noga Zaslavsky", "year": 2022, "url": "https://www.semanticscholar.org/paper/f6ee73a9f201c3c5e56d27501fe5d9dd043aa36e", "abstract": "Traditional emergent communication (EC) methods often fail to generalize to novel settings or align with representations of natural language. Here, we show how controlling the Information Bottleneck (IB) tradeoff between complexity and informativeness (a principle thought to guide human languages) helps to address both of these problems in EC. Using VQ-VIB, a recent method for training EC agents while controlling the IB tradeoff, we find that: (1) increasing pressure for informativeness, which encourages agents to develop a shared understanding beyond task-specific needs, leads to better generalization to more challenging tasks and novel inputs; (2) VQ-VIB agents develop an EC space that encodes some semantic similarities and facilitates open-domain communication, similar to word embeddings in natural language; and (3) when translating between English and EC, greater complexity leads to improved performance of teams of simulated English speakers and trained VQ-VIB listeners, but only up to a threshold corresponding to the English complexity. These results indicate the importance of informational constraints for improving self-play performance and human-agent interaction.", "topic": "Emergent Communication Systems"}, {"title": "Competition from novel features drives scalar inferences in reference games", "authors": "Jennifer Hu, Noga Zaslavsky, Roger Levy", "year": 2021, "url": "https://www.semanticscholar.org/paper/0571d12ca605d56f623456da8a92bbfce853b219", "abstract": null, "topic": "Unknown"}, {"title": "Scalable pragmatic communication via self-supervision", "authors": "Jennifer Hu, R. Levy, Noga Zaslavsky", "year": 2021, "url": "https://www.semanticscholar.org/paper/35f8975878b19ca81f70bc84be0eb72134fc0b06", "abstract": "Models of context-sensitive communication often use the Rational Speech Act framework (RSA; Frank & Goodman, 2012), which formulates listeners and speakers in a cooperative reasoning process. However, the standard RSA formulation can only be applied to small domains, and large-scale applications have relied on imitating human behavior. Here, we propose a new approach to scalable pragmatics, building upon recent theoretical results (Zaslavsky et al., 2020) that characterize pragmatic reasoning in terms of general information-theoretic principles. Specifically, we propose an architecture and learning process in which agents acquire pragmatic policies via self-supervision instead of imitating human data. This work suggests a new principled approach for equipping artificial agents with pragmatic skills via self-supervision, which is grounded both in pragmatic theory and in information theory.", "topic": "Emergent Communication Systems"}, {"title": "Bayesian Approaches to Color Category Learning", "authors": "T. Griffiths, Noga Zaslavsky", "year": 2021, "url": "https://www.semanticscholar.org/paper/701f2e84cd645e27ab96c006a0adcc30a494db1f", "abstract": null, "topic": "Unknown"}, {"title": "The evolution of color naming reflects pressure for efficiency: Evidence from the recent past", "authors": "Noga Zaslavsky, Karee Garvin, Charles Kemp, Naftali Tishby, T. Regier", "year": 2021, "url": "https://www.semanticscholar.org/paper/761c298b58964c4f5933d0b5bb9fc69e36ce5ab3", "abstract": "It has been proposed that semantic systems evolve under pressure for efficiency. This hypothesis has so far been supported largely indirectly, by synchronic cross-language comparison, rather than directly by diachronic data. Here, we directly test this hypothesis in the domain of color naming, by analyzing recent diachronic data from Nafaanra, a language of Ghana and C\u00f4te d\u2019Ivoire, and comparing it with quantitative predictions derived from the mathematical theory of efficient data compression. We show that color naming in Nafaanra has changed over the past four decades while remaining near-optimally efficient, and that this outcome would be unlikely under a random drift process that maintains structured color categories without pressure for efficiency. To our knowledge, this finding provides the first direct evidence that color naming evolves under pressure for efficiency, supporting the hypothesis that efficiency shapes the evolution of the lexicon.", "topic": "\"Efficient Communication and Cognitive Models\""}, {"title": "The forms and meanings of grammatical markers support efficient communication", "authors": "Francis Mollica, Geoff Bacon, Noga Zaslavsky, Yang Xu, T. Regier, Charles Kemp", "year": 2021, "url": "https://www.semanticscholar.org/paper/76d54ac3f6c1fe70444a0902623813bcb1f112b5", "abstract": "Significance Grammatical marking of features such as number, tense, and evidentiality varies widely across languages. Despite this variation, we show that grammatical markers support efficient information transfer from speakers to listeners. We apply a formal model of communication to data from dozens of languages and find that grammatical marking achieves a near-optimal balance between maximizing informativeness and minimizing code lengths. Our approach shows how general information-theoretic principles can capture variation in both form and meaning across languages. Functionalist accounts of language suggest that forms are paired with meanings in ways that support efficient communication. Previous work on grammatical marking suggests that word forms have lengths that enable efficient production, and work on the semantic typology of the lexicon suggests that word meanings represent efficient partitions of semantic space. Here we establish a theoretical link between these two lines of work and present an information-theoretic analysis that captures how communicative pressures influence both form and meaning. We apply our approach to the grammatical features of number, tense, and evidentiality and show that the approach explains both which systems of feature values are attested across languages and the relative lengths of the forms for those feature values. Our approach shows that general information-theoretic principles can capture variation in both form and meaning across languages.", "topic": "\"Efficient Communication and Cognitive Models\""}, {"title": "Beyond linear regression: mapping models in cognitive neuroscience should align with research goals", "authors": "Anna A. Ivanova, Martin Schrimpf, Stefano Anzellotti, Noga Zaslavsky, Evelina Fedorenko, Leyla Isik", "year": 2021, "url": "https://www.semanticscholar.org/paper/b1d4730af386ab3ff452775ef5a7f3be1129e26b", "abstract": "Many cognitive neuroscience studies use large feature sets to predict and interpret brain activity patterns. Feature sets take many forms, from human stimulus annotations to representations in deep neural networks. Of crucial importance in all these studies is the mapping model, which defines the space of possible relationships between features and neural data. Until recently, most encoding and decoding studies have used linear mapping models. Increasing availability of large datasets and computing resources has recently allowed some researchers to employ more flexible nonlinear mapping models instead; however, the question of whether nonlinear mapping models can yield meaningful scientific insights remains debated. Here, we discuss the choice of a mapping model in the context of three overarching desiderata: predictive accuracy, interpretability, and biological plausibility. We show that, contrary to popular intuition, these desiderata do not map cleanly onto the linear/nonlinear divide; instead, each desideratum can refer to multiple research goals, each of which imposes its own constraints on the mapping model. Moreover, we argue that, instead of categorically treating the mapping models as linear or nonlinear, we should instead aim to estimate the complexity of these models. We show that, in many cases, complexity provides a more accurate reflection of restrictions imposed by various research goals. Finally, we outline several complexity metrics that can be used to effectively evaluate mapping models.", "topic": "\"Efficient Communication and Cognitive Models\""}, {"title": "Let's talk (efficiently) about us: Person systems achieve near-optimal compression", "authors": "Noga Zaslavsky, Mora Maldonado, J. Culbertson", "year": 2021, "url": "https://www.semanticscholar.org/paper/d46cc155e033b09c530061afecebebc77542a0a2", "abstract": "Systems of personal pronouns (e.g., 'you' and 'I') vary widely across languages, but at the same time not all possible systems are attested. Linguistic theories have generally accounted for this in terms of strong grammatical constraints, but recent experimental work challenges this view. Here, we take a novel approach to understanding personal pronoun systems by invoking a recent information-theoretic framework for semantic systems that predicts that languages efficiently compress meanings into forms. We find that a test set of cross-linguistically attested personal pronoun systems achieves near-optimal compression, supporting the hypothesis that efficient compression shapes semantic systems. Further, our best-fitting model includes an egocentric bias that favors a salient speaker representation, accounting for a well-known typological generalization of person systems ('Zwicky's Generalization') without the need for a hard grammatical constraint.", "topic": "\"Efficient Communication and Cognitive Models\""}, {"title": "Empirical Support for a Rate-Distortion Account of Pragmatic Reasoning", "authors": "Irene Zhou, Jennifer Hu, Roger Levy, Noga Zaslavsky", "year": 2021, "url": "https://www.semanticscholar.org/paper/dbc89496a04251e6e13b578ca95a9c24b1f0f249", "abstract": "Iterated models of pragmatic reasoning, such as the Rational Speech Act model (RSA; Frank & Goodman, 2012), aim to explain howmeaning is understood in context. We propose an optimal experiment design approach for teasing apart such models, in which contexts are optimized for differentiating model predictions in reference games. We use this approach to compare RSA with RD-RSA (Zaslavsky et al., 2020), a recent variant of RSA grounded in Rate-Distortion theory. First, we show that our optimal experiment design approach finds cases in which the two models yield qualitatively different predictions, in contrast to previous experimental settings for which these models generate similar predictions. Next, we test the models on newly collected experimental data using our optimal design. Our results show that in this experimental setting RD-RSA robustly outperforms the standard RSA model. This finding supports the idea that Rate-Distortion theory may characterize human pragmatic reasoning.", "topic": "\"Efficient Communication and Cognitive Models\""}], "analysis": "1. **Task-specific latent representations in neural networks**\n2. **Human-like discrete representations in neural models**\n3. **Entropy tuning for representation complexity in neural models**\n4. **Training data requirements for neural language models**\n5. **Alignment of neural models with human fMRI responses**\n6. **Information Bottleneck tradeoff in emergent communication**\n7. **Vector-Quantized Variational Information Bottleneck (VQ-VIB) in neural agents**\n8. **Pragmatic reasoning in artificial agents**\n9. **Iterated models of pragmatic reasoning and Rate-Distortion theory**", "author_name": "Noga Zaslavsky", "coauthors_histogram": {"Andi Peng": 1, "Mycal Tucker": 4, "Eoin M. Kenny": 1, "Pulkit Agrawal": 1, "Julie A. Shah": 1, "Eghbal A. Hosseini": 3, "Martin Schrimpf": 3, "Yian Zhang": 2, "Samuel R. Bowman": 2, "Evelina Fedorenko": 4, "Colton Casto": 1, "Mora Maldonado": 2, "Jennifer Culbertson": 1, "J. Shah": 3, "R. Levy": 4, "Irene Zhou": 2, "Jennifer Hu": 4, "Roger Levy": 4, "Alicia M. Chen": 1, "Matthias Hofer": 1, "Moshe Poliak": 1, "T. Griffiths": 1, "Karee Garvin": 1, "Charles Kemp": 2, "Naftali Tishby": 1, "T. Regier": 2, "Francis Mollica": 1, "Geoff Bacon": 1, "Yang Xu": 1, "Anna A. Ivanova": 1, "Stefano Anzellotti": 1, "Leyla Isik": 1, "J. Culbertson": 1}, "paper_topics": {}, "cluster_trends": {"Emergent Communication Systems": {"2023": 1, "2022": 3, "2021": 1, "2024": 0}, "Neural Networks & Language Processing": {"2023": 1, "2022": 1, "2021": 0, "2024": 0}, "\"Efficient Communication and Cognitive Models\"": {"2021": 5, "2022": 0, "2023": 0, "2024": 0}}}, "142da7429b6d5ef85ebcdd78a5e3c72a": {"authorId": "2949185", "start_year": 2022, "end_year": 2024, "papers": [{"title": "Human Curriculum Effects Emerge with In-Context Learning in Neural Networks", "authors": "Jacob Russin, Ellie Pavlick, Michael J. Frank", "year": 2024, "url": "https://www.semanticscholar.org/paper/038fc046f3b7b7a3adefeea172e4b94a214e0c2a", "abstract": "Human learning is sensitive to rule-like structure and the curriculum of examples used for training. In tasks governed by succinct rules, learning is more robust when related examples are blocked across trials, but in the absence of such rules, interleaving is more effective. To date, no neural model has simultaneously captured these seemingly contradictory effects. Here we show that this same tradeoff spontaneously emerges with \u201cin-context learning\u201d (ICL) both in neural networks trained with metalearning and in large language models (LLMs). ICL is the ability to learn new tasks \u201cin context\u201d \u2014 without weight changes \u2014 via an inner-loop algorithm implemented in activation dynamics. Experiments with pretrained LLMs and metalearning transformers show that ICL exhibits the blocking advantage demonstrated in humans on a task involving rule-like structure, and conversely, that concurrent in-weight learning reproduces the interleaving advantage observed in humans on tasks lacking such structure.", "topic": "\"Neural Network Interpretability and Learning\""}, {"title": "Transformer Mechanisms Mimic Frontostriatal Gating Operations When Trained on Human Working Memory Tasks", "authors": "Aaron Traylor, Jack Merullo, Michael J. Frank, Ellie Pavlick", "year": 2024, "url": "https://www.semanticscholar.org/paper/8f48f5b22fe0e71227d002fac0bff1ee937c45bc", "abstract": "Models based on the Transformer neural network architecture have seen success on a wide variety of tasks that appear to require complex\"cognitive branching\"-- or the ability to maintain pursuit of one goal while accomplishing others. In cognitive neuroscience, success on such tasks is thought to rely on sophisticated frontostriatal mechanisms for selective \\textit{gating}, which enable role-addressable updating -- and later readout -- of information to and from distinct\"addresses\"of memory, in the form of clusters of neurons. However, Transformer models have no such mechanisms intentionally built-in. It is thus an open question how Transformers solve such tasks, and whether the mechanisms that emerge to help them to do so bear any resemblance to the gating mechanisms in the human brain. In this work, we analyze the mechanisms that emerge within a vanilla attention-only Transformer trained on a simple sequence modeling task inspired by a task explicitly designed to study working memory gating in computational cognitive neuroscience. We find that, as a result of training, the self-attention mechanism within the Transformer specializes in a way that mirrors the input and output gating mechanisms which were explicitly incorporated into earlier, more biologically-inspired architectures. These results suggest opportunities for future research on computational similarities between modern AI architectures and models of the human brain.", "topic": "\"Transformer Models and Cognitive Mechanisms\""}, {"title": "Compositional Reasoning in Vision-Language Models", "authors": "Jessica Li, Apoorv Khandelwal, Ellie Pavlick", "year": 2023, "url": "https://www.semanticscholar.org/paper/110ccbe25193fa3df9ae073205dee9da8b05cd01", "abstract": "Vision\u2013language models (VLMs) have achieved impressive performance on long-standing visual question-answering (VQA) benchmarks by utilizing large-scale image\u2013caption pre-training on web datasets. These models are able to perform a variety of downstream tasks out of the box, including zero-shot VQA, but have also been shown to struggle with understanding concepts such as object attributes, relations, and compositional reasoning. In this paper, we test whether such multi-modal models are able to reason compositionally. To do so, we decompose \u201ccom-positional\u201d questions from the CLEVR dataset and confirm that state-of-the-art VLMs struggle with composition. We reveal that these models can accurately identify pieces of information but struggle to relate them together to answer compositional questions. Additionally, we find that in-context VQA examples do not improve VQA performance or compositional reasoning in these models.", "topic": "\"Evaluating AI Compositional Reasoning\""}, {"title": "Testing Causal Models of Word Meaning in GPT-3 and -4", "authors": "Sam Musker, Ellie Pavlick", "year": 2023, "url": "https://www.semanticscholar.org/paper/1578505cbec178c7eeffd21074bb6f23abb63800", "abstract": "Large Language Models (LLMs) have driven extraordinary improvements in NLP. However, it is unclear how such models represent lexical concepts-i.e., the meanings of the words they use. This paper evaluates the lexical representations of GPT-3 and GPT-4 through the lens of HIPE theory, a theory of concept representations which focuses on representations of words describing artifacts (such as\"mop\",\"pencil\", and\"whistle\"). The theory posits a causal graph that relates the meanings of such words to the form, use, and history of the objects to which they refer. We test LLMs using the same stimuli originally used by Chaigneau et al. (2004) to evaluate the theory in humans, and consider a variety of prompt designs. Our experiments concern judgements about causal outcomes, object function, and object naming. We find no evidence that GPT-3 encodes the causal structure hypothesized by HIPE, but do find evidence that GPT-4 encodes such structure. The results contribute to a growing body of research characterizing the representational capacity of large language models.", "topic": "\"Evaluating AI Compositional Reasoning\""}, {"title": "Uncovering Intermediate Variables in Transformers using Circuit Probing", "authors": "Michael A. Lepori, Thomas Serre, Ellie Pavlick", "year": 2023, "url": "https://www.semanticscholar.org/paper/40c34e85dc4558d26bfef7fb54327dbd4a0bebc3", "abstract": "Neural network models have achieved high performance on a wide variety of complex tasks, but the algorithms that they implement are notoriously difficult to interpret. In order to understand these algorithms, it is often necessary to hypothesize intermediate variables involved in the network's computation. For example, does a language model depend on particular syntactic properties when generating a sentence? However, existing analysis tools make it difficult to test hypotheses of this type. We propose a new analysis technique -- circuit probing -- that automatically uncovers low-level circuits that compute hypothesized intermediate variables. This enables causal analysis through targeted ablation at the level of model parameters. We apply this method to models trained on simple arithmetic tasks, demonstrating its effectiveness at (1) deciphering the algorithms that models have learned, (2) revealing modular structure within a model, and (3) tracking the development of circuits over training. We compare circuit probing to other methods across these three experiments, and find it on par or more effective than existing analysis methods. Finally, we demonstrate circuit probing on a real-world use case, uncovering circuits that are responsible for subject-verb agreement and reflexive anaphora in GPT2-Small and Medium.", "topic": "\"Neural Network Interpretability and Learning\""}, {"title": "Comparing Trajectory and Vision Modalities for Verb Representation", "authors": "Dylan Ebert, Chen Sun, Ellie Pavlick", "year": 2023, "url": "https://www.semanticscholar.org/paper/5aa49fb8ec1c2b24d7e317cbfb4b9d7a14165d09", "abstract": "Three-dimensional trajectories, or the 3D position and rotation of objects over time, have been shown to encode key aspects of verb semantics (e.g., the meanings of roll vs. slide). However, most multimodal models in NLP use 2D images as representations of the world. Given the importance of 3D space in formal models of verb semantics, we expect that these 2D images would result in impoverished representations that fail to capture nuanced differences in meaning. This paper tests this hypothesis directly in controlled experiments. We train self-supervised image and trajectory encoders, and then evaluate them on the extent to which each learns to differentiate verb concepts. Contrary to our initial expectations, we find that 2D visual modalities perform similarly well to 3D trajectories. While further work should be conducted on this question, our initial findings challenge the conventional wisdom that richer environment representations necessarily translate into better representation learning for language.", "topic": "\"Verb Semantics in Multimodal Models\""}, {"title": "Emergence of Abstract State Representations in Embodied Sequence Modeling", "authors": "Tian Yun, Zilai Zeng, Kunal Handa, Ashish V. Thapliyal, Bo Pang, Ellie Pavlick, Chen Sun", "year": 2023, "url": "https://www.semanticscholar.org/paper/62b0b511b62bf3c2a63d4f425128cc08e55ad8a1", "abstract": "Decision making via sequence modeling aims to mimic the success of language models, where actions taken by an embodied agent are modeled as tokens to predict. Despite their promising performance, it remains unclear if embodied sequence modeling leads to the emergence of internal representations that represent the environmental state information. A model that lacks abstract state representations would be liable to make decisions based on surface statistics which fail to generalize. We take the BabyAI environment, a grid world in which language-conditioned navigation tasks are performed, and build a sequence modeling Transformer, which takes a language instruction, a sequence of actions, and environmental observations as its inputs. In order to investigate the emergence of abstract state representations, we design a\"blindfolded\"navigation task, where only the initial environmental layout, the language instruction, and the action sequence to complete the task are available for training. Our probing results show that intermediate environmental layouts can be reasonably reconstructed from the internal activations of a trained model, and that language instructions play a role in the reconstruction accuracy. Our results suggest that many key features of state representations can emerge via embodied sequence modeling, supporting an optimistic outlook for applications of sequence modeling objectives to more complex embodied decision-making domains.", "topic": "\"Transformer Models and Cognitive Mechanisms\""}, {"title": "Instilling Inductive Biases with Subnetworks", "authors": "Enyan Zhang, Michael A. Lepori, Ellie Pavlick", "year": 2023, "url": "https://www.semanticscholar.org/paper/7b1c53e392dc322e7f9525b5c7fb8cdf0e357f8a", "abstract": "Despite the recent success of artificial neural networks on a variety of tasks, we have little knowledge or control over the exact solutions these models implement. Instilling inductive biases -- preferences for some solutions over others -- into these models is one promising path toward understanding and controlling their behavior. Much work has been done to study the inherent inductive biases of models and instill different inductive biases through hand-designed architectures or carefully curated training regimens. In this work, we explore a more mechanistic approach: Subtask Induction. Our method discovers a functional subnetwork that implements a particular subtask within a trained model and uses it to instill inductive biases towards solutions utilizing that subtask. Subtask Induction is flexible and efficient, and we demonstrate its effectiveness with two experiments. First, we show that Subtask Induction significantly reduces the amount of training data required for a model to adopt a specific, generalizable solution to a modular arithmetic task. Second, we demonstrate that Subtask Induction successfully induces a human-like shape bias while increasing data efficiency for convolutional and transformer-based image classification models.", "topic": "\"Neural Network Interpretability and Learning\""}, {"title": "Training Priors Predict Text-To-Image Model Performance", "authors": "Charles Lovering, Ellie Pavlick", "year": 2023, "url": "https://www.semanticscholar.org/paper/85c7c00ca6e9722ed55078a1b33a9b57df3b5e5e", "abstract": "Text-to-image models can often generate some relations, i.e.,\"astronaut riding horse\", but fail to generate other relations composed of the same basic parts, i.e.,\"horse riding astronaut\". These failures are often taken as evidence that models rely on training priors rather than constructing novel images compositionally. This paper tests this intuition on the stablediffusion 2.1 text-to-image model. By looking at the subject-verb-object (SVO) triads that underlie these prompts (e.g.,\"astronaut\",\"ride\",\"horse\"), we find that the more often an SVO triad appears in the training data, the better the model can generate an image aligned with that triad. Here, by aligned we mean that each of the terms appears in the generated image in the proper relation to each other. Surprisingly, this increased frequency also diminishes how well the model can generate an image aligned with the flipped triad. For example, if\"astronaut riding horse\"appears frequently in the training data, the image for\"horse riding astronaut\"will tend to be poorly aligned. Our results thus show that current models are biased to generate images with relations seen in training, and provide new data to the ongoing debate on whether these text-to-image models employ abstract compositional structure in a traditional sense, or rather, interpolate between relations explicitly seen in the training data.", "topic": "\"Evaluating AI Compositional Reasoning\""}, {"title": "Symbols and grounding in large language models", "authors": "Ellie Pavlick", "year": 2023, "url": "https://www.semanticscholar.org/paper/8b7a7bf91c5d7410273a24d245142acf08c55a3d", "abstract": "Large language models (LLMs) are one of the most impressive achievements of artificial intelligence in recent years. However, their relevance to the study of language more broadly remains unclear. This article considers the potential of LLMs to serve as models of language understanding in humans. While debate on this question typically centres around models\u2019 performance on challenging language understanding tasks, this article argues that the answer depends on models\u2019 underlying competence, and thus that the focus of the debate should be on empirical work which seeks to characterize the representations and processing algorithms that underlie model behaviour. From this perspective, the article offers counterarguments to two commonly cited reasons why LLMs cannot serve as plausible models of language in humans: their lack of symbolic structure and their lack of grounding. For each, a case is made that recent empirical trends undermine the common assumptions about LLMs, and thus that it is premature to draw conclusions about LLMs\u2019 ability (or lack thereof) to offer insights on human language representation and understanding. This article is part of a discussion meeting issue \u2018Cognitive artificial intelligence\u2019.", "topic": "\"Evaluating AI Compositional Reasoning\""}, {"title": "Language Models Implement Simple Word2Vec-style Vector Arithmetic", "authors": "Jack Merullo, Carsten Eickhoff, Ellie Pavlick", "year": 2023, "url": "https://www.semanticscholar.org/paper/95a0c8feccc01f2799c961ca850f75f9b054de6c", "abstract": "A primary criticism towards language models (LMs) is their inscrutability. This paper presents evidence that, despite their size and complexity, LMs sometimes exploit a simple vector arithmetic style mechanism to solve some relational tasks using regularities encoded in the hidden space of the model (e.g., Poland:Warsaw::China:Beijing). We investigate a range of language model sizes (from 124M parameters to 176B parameters) in an in-context learning setting, and find that for a variety of tasks (involving capital cities, uppercasing, and past-tensing) a key part of the mechanism reduces to a simple additive update typically applied by the feedforward (FFN) networks. We further show that this mechanism is specific to tasks that require retrieval from pretraining memory, rather than retrieval from local context. Our results contribute to a growing body of work on the interpretability of LMs, and offer reason to be optimistic that, despite the massive and non-linear nature of the models, the strategies they ultimately use to solve tasks can sometimes reduce to familiar and even intuitive algorithms.", "topic": "\"Transformer Models and Cognitive Mechanisms\""}, {"title": "Deep Neural Networks Can Learn Generalizable Same-Different Visual Relations", "authors": "Alexa R. Tartaglini, Sheridan Feucht, Michael A. Lepori, Wai Keen Vong, Charles Lovering, B. Lake, Ellie Pavlick", "year": 2023, "url": "https://www.semanticscholar.org/paper/962118058d72885b08a2514a98823e18dfd41c5c", "abstract": "Although deep neural networks can achieve human-level performance on many object recognition benchmarks, prior work suggests that these same models fail to learn simple abstract relations, such as determining whether two objects are the same or different. Much of this prior work focuses on training convolutional neural networks to classify images of two same or two different abstract shapes, testing generalization on within-distribution stimuli. In this article, we comprehensively study whether deep neural networks can acquire and generalize same-different relations both within and out-of-distribution using a variety of architectures, forms of pretraining, and fine-tuning datasets. We find that certain pretrained transformers can learn a same-different relation that generalizes with near perfect accuracy to out-of-distribution stimuli. Furthermore, we find that fine-tuning on abstract shapes that lack texture or color provides the strongest out-of-distribution generalization. Our results suggest that, with the right approach, deep neural networks can learn generalizable same-different visual relations.", "topic": "\"Transformer Models and Cognitive Mechanisms\""}, {"title": "NeuroSurgeon: A Toolkit for Subnetwork Analysis", "authors": "Michael A. Lepori, Ellie Pavlick, Thomas Serre", "year": 2023, "url": "https://www.semanticscholar.org/paper/b32a6770239aeeec31c782a2a25be3ffa6b7a051", "abstract": "Despite recent advances in the field of explainability, much remains unknown about the algorithms that neural networks learn to represent. Recent work has attempted to understand trained models by decomposing them into functional circuits (Csord\\'as et al., 2020; Lepori et al., 2023). To advance this research, we developed NeuroSurgeon, a python library that can be used to discover and manipulate subnetworks within models in the Huggingface Transformers library (Wolf et al., 2019). NeuroSurgeon is freely available at https://github.com/mlepori1/NeuroSurgeon.", "topic": "\"Neural Network Interpretability and Learning\""}, {"title": "Are Language Models Worse than Humans at Following Prompts? It's Complicated", "authors": "Albert Webson, Alyssa Marie Loo, Qinan Yu, Ellie Pavlick", "year": 2023, "url": "https://www.semanticscholar.org/paper/b7c4677cc6950d556f8d58084f87b2cb9cfe29b8", "abstract": "Prompts have been the center of progress in advancing language models' zero-shot and few-shot performance. However, recent work finds that models can perform surprisingly well when given intentionally irrelevant or misleading prompts. Such results may be interpreted as evidence that model behavior is not\"human like\". In this study, we challenge a central assumption in such work: that humans would perform badly when given pathological instructions. We find that humans are able to reliably ignore irrelevant instructions and thus, like models, perform well on the underlying task despite an apparent lack of signal regarding the task they are being asked to do. However, when given deliberately misleading instructions, humans follow the instructions faithfully, whereas models do not. Our findings caution that future research should not idealize human behaviors as a monolith and should not train or evaluate models to mimic assumptions about these behaviors without first validating humans' behaviors empirically.", "topic": "\"Evaluating AI Compositional Reasoning\""}, {"title": "Circuit Component Reuse Across Tasks in Transformer Language Models", "authors": "Jack Merullo, Carsten Eickhoff, Ellie Pavlick", "year": 2023, "url": "https://www.semanticscholar.org/paper/c0d9a48547d728dd320b453b01a0ab1ce2f96098", "abstract": "Recent work in mechanistic interpretability has shown that behaviors in language models can be successfully reverse-engineered through circuit analysis. A common criticism, however, is that each circuit is task-specific, and thus such analysis cannot contribute to understanding the models at a higher level. In this work, we present evidence that insights (both low-level findings about specific heads and higher-level findings about general algorithms) can indeed generalize across tasks. Specifically, we study the circuit discovered in Wang et al. (2022) for the Indirect Object Identification (IOI) task and 1.) show that it reproduces on a larger GPT2 model, and 2.) that it is mostly reused to solve a seemingly different task: Colored Objects (Ippolito&Callison-Burch, 2023). We provide evidence that the process underlying both tasks is functionally very similar, and contains about a 78% overlap in in-circuit attention heads. We further present a proof-of-concept intervention experiment, in which we adjust four attention heads in middle layers in order to 'repair' the Colored Objects circuit and make it behave like the IOI circuit. In doing so, we boost accuracy from 49.6% to 93.7% on the Colored Objects task and explain most sources of error. The intervention affects downstream attention heads in specific ways predicted by their interactions in the IOI circuit, indicating that this subcircuit behavior is invariant to the different task inputs. Overall, our results provide evidence that it may yet be possible to explain large language models' behavior in terms of a relatively small number of interpretable task-general algorithmic building blocks and computational components.", "topic": "\"Neural Network Interpretability and Learning\""}, {"title": "Simfluence: Modeling the Influence of Individual Training Examples by Simulating Training Runs", "authors": "Kelvin Guu, Albert Webson, Ellie Pavlick, Lucas Dixon, Ian Tenney, Tolga Bolukbasi", "year": 2023, "url": "https://www.semanticscholar.org/paper/c926bd6ebdbcf0bfabb55c1f0171055bc2dbe516", "abstract": "Training data attribution (TDA) methods offer to trace a model's prediction on any given example back to specific influential training examples. Existing approaches do so by assigning a scalar influence score to each training example, under a simplifying assumption that influence is additive. But in reality, we observe that training examples interact in highly non-additive ways due to factors such as inter-example redundancy, training order, and curriculum learning effects. To study such interactions, we propose Simfluence, a new paradigm for TDA where the goal is not to produce a single influence score per example, but instead a training run simulator: the user asks, ``If my model had trained on example $z_1$, then $z_2$, ..., then $z_n$, how would it behave on $z_{test}$?''; the simulator should then output a simulated training run, which is a time series predicting the loss on $z_{test}$ at every step of the simulated run. This enables users to answer counterfactual questions about what their model would have learned under different training curricula, and to directly see where in training that learning would occur. We present a simulator, Simfluence-Linear, that captures non-additive interactions and is often able to predict the spiky trajectory of individual example losses with surprising fidelity. Furthermore, we show that existing TDA methods such as TracIn and influence functions can be viewed as special cases of Simfluence-Linear. This enables us to directly compare methods in terms of their simulation accuracy, subsuming several prior TDA approaches to evaluation. In experiments on large language model (LLM) fine-tuning, we show that our method predicts loss trajectories with much higher accuracy than existing TDA methods (doubling Spearman's correlation and reducing mean-squared error by 75%) across several tasks, models, and training methods.", "topic": "\"Transformer Models and Cognitive Mechanisms\""}, {"title": "Analyzing Modular Approaches for Visual Question Decomposition", "authors": "Apoorv Khandelwal, Ellie Pavlick, Chen Sun", "year": 2023, "url": "https://www.semanticscholar.org/paper/cf7d69709bdeddd561c183178bbc1f0c2e156a08", "abstract": "Modular neural networks without additional training have recently been shown to surpass end-to-end neural networks on challenging vision-language tasks. The latest such methods simultaneously introduce LLM-based code generation to build programs and a number of skill-specific, task-oriented modules to execute them. In this paper, we focus on ViperGPT and ask where its additional performance comes from and how much is due to the (state-of-art, end-to-end) BLIP-2 model it subsumes vs. additional symbolic components. To do so, we conduct a controlled study (comparing end-to-end, modular, and prompting-based methods across several VQA benchmarks). We find that ViperGPT's reported gains over BLIP-2 can be attributed to its selection of task-specific modules, and when we run ViperGPT using a more task-agnostic selection of modules, these gains go away. Additionally, ViperGPT retains much of its performance if we make prominent alterations to its selection of modules: e.g. removing or retaining only BLIP-2. Finally, we compare ViperGPT against a prompting-based decomposition strategy and find that, on some benchmarks, modular approaches significantly benefit by representing subtasks with natural language, instead of code.", "topic": "\"Transformer Models and Cognitive Mechanisms\""}, {"title": "Break It Down: Evidence for Structural Compositionality in Neural Networks", "authors": "Michael A. Lepori, Thomas Serre, Ellie Pavlick", "year": 2023, "url": "https://www.semanticscholar.org/paper/d369fcd0d7652380e094425b98eae22a77cdcfdf", "abstract": "Many tasks can be described as compositions over subroutines. Though modern neural networks have achieved impressive performance on both vision and language tasks, we know little about the functions that they implement. One possibility is that neural networks implicitly break down complex tasks into subroutines, implement modular solutions to these subroutines, and compose them into an overall solution to a task -- a property we term structural compositionality. Or they may simply learn to match new inputs to memorized representations, eliding task decomposition entirely. Here, we leverage model pruning techniques to investigate this question in both vision and language, across a variety of architectures, tasks, and pretraining regimens. Our results demonstrate that models oftentimes implement solutions to subroutines via modular subnetworks, which can be ablated while maintaining the functionality of other subroutines. This suggests that neural networks may be able to learn to exhibit compositionality, obviating the need for specialized symbolic mechanisms.", "topic": "\"Neural Network Interpretability and Learning\""}, {"title": "Characterizing Mechanisms for Factual Recall in Language Models", "authors": "Qinan Yu, Jack Merullo, Ellie Pavlick", "year": 2023, "url": "https://www.semanticscholar.org/paper/f44c1d2f1764bbf186c95af1b3b5a624b9801d18", "abstract": "Language Models (LMs) often must integrate facts they memorized in pretraining with new information that appears in a given context. These two sources can disagree, causing competition within the model, and it is unclear how an LM will resolve the conflict. On a dataset that queries for knowledge of world capitals, we investigate both distributional and mechanistic determinants of LM behavior in such situations. Specifically, we measure the proportion of the time an LM will use a counterfactual prefix (e.g.,\"The capital of Poland is London\") to overwrite what it learned in pretraining (\"Warsaw\"). On Pythia and GPT2, the training frequency of both the query country (\"Poland\") and the in-context city (\"London\") highly affect the models' likelihood of using the counterfactual. We then use head attribution to identify individual attention heads that either promote the memorized answer or the in-context answer in the logits. By scaling up or down the value vector of these heads, we can control the likelihood of using the in-context answer on new data. This method can increase the rate of generating the in-context answer to 88\\% of the time simply by scaling a single head at runtime. Our work contributes to a body of evidence showing that we can often localize model behaviors to specific components and provides a proof of concept for how future methods might control model behavior dynamically at runtime.", "topic": "\"Evaluating AI Compositional Reasoning\""}, {"title": "Can Neural Networks Learn Implicit Logic from Physical Reasoning?", "authors": "Aaron Traylor, Roman Feiman, Ellie Pavlick", "year": 2023, "url": "https://www.semanticscholar.org/paper/fc80b1324187d5f7aa8466a7cb94d0d1640dfc2e", "abstract": "Despite the success of neural network models in a range of domains", "topic": "\"Neural Network Evaluation and Development\""}, {"title": "Hierarchical planning with state abstractions for temporal task specifications", "authors": "Yoonseon Oh, Roma Patel, Thao Nguyen, Baichuan Huang, Matthew Berg, Ellie Pavlick, Stefanie Tellex", "year": 2022, "url": "https://www.semanticscholar.org/paper/0734c412af9c4c01893f0550683b230d7f72afe0", "abstract": null, "topic": "Unknown"}, {"title": "Where, When & Which Concepts Does AlphaZero Learn? Lessons from the Game of Hex", "authors": "J. Forde, Charles Lovering, G. Konidaris, Ellie Pavlick, M. Littman", "year": 2022, "url": "https://www.semanticscholar.org/paper/140bbb09c80334c83476e7936fa80884c46e3608", "abstract": "AlphaZero, an approach to reinforcement learning that couples neural networks and Monte Carlo tree search (MCTS), has produced state-of-the-art strategies for traditional board games like Chess, Go, and Hex. While researchers and game commentators have suggested that AlphaZero uses concepts humans consider important, it is unclear how these concepts are represented in the network. We investigate AlphaZero\u2019s representations in Hex using both model probing and behavioral tests. We find that the MCTS search initially finds important concepts, and then the neural network learns to encode these concepts. Concepts related to short-term end-game planning are best encoded in the final layers of the model, whereas concepts related to long-term planning are encoded in the middle layers of the model.", "topic": "\"Neural Network Interpretability and Learning\""}, {"title": "Do Vision-Language Pretrained Models Learn Composable Primitive Concepts?", "authors": "Tian Yun, Usha Bhalla, Ellie Pavlick, Chen Sun", "year": 2022, "url": "https://www.semanticscholar.org/paper/222c1b5c2e883ea877c0b8f789585b1bb51f235e", "abstract": "Vision-language (VL) pretrained models have achieved impressive performance on multimodal reasoning and zero-shot recognition tasks. Many of these VL models are pretrained on unlabeled image and caption pairs from the internet. In this paper, we study whether representations of primitive concepts--such as colors, shapes, or the attributes of object parts--emerge automatically within these pretrained VL models. We propose a two-step framework, Compositional Concept Mapping (CompMap), to investigate this. CompMap first asks a VL model to generate primitive concept activations with text prompts, and then learns to construct a composition model that maps the primitive concept activations (e.g. the likelihood of black tail or red wing) to composite concepts (e.g. a red-winged blackbird). We show that a composition model can be reliably learn from ground truth primitive concepts. We thus hypothesize that if primitive concepts indeed emerge in a VL pretrained model, its primitive concept activations can be used to learn a composition model similar to the one designed by experts. We propose a quantitative metric to measure the degree of similarity, and refer to the metric as the interpretability metric. We also measure the classification accuracy when using the primitive concept activations and the learned composition model to predict the composite concepts, and refer to it as the usefulness metric. Our study reveals that state-of-the-art VL pretrained models learn primitive concepts that are highly useful for fine-grained visual recognition on the CUB dataset, and compositional generalization tasks on the MIT-States dataset. However, we observe that the learned composition models have low interpretability in our qualitative analyses. Our results reveal the limitations of existing VL models, and the necessity of pretraining objectives that encourage the acquisition of primitive concepts.", "topic": "\"Transformer Models and Cognitive Mechanisms\""}, {"title": "Does CLIP Bind Concepts? Probing Compositionality in Large Image Models", "authors": "Martha Lewis, Nihal V. Nayak, Qinan Yu, Jack Merullo, Ellie Pavlick", "year": 2022, "url": "https://www.semanticscholar.org/paper/2de7790ed868510c8001a90c11737fe4e8a01930", "abstract": "Large-scale neural network models combining text and images have made incredible progress in recent years. However, it remains an open question to what extent such models encode compositional representations of the concepts over which they operate, such as correctly identifying \u2018red cube\u2019 by reasoning over the constituents \u2018red\u2019 and \u2018cube\u2019. In this work, we focus on the ability of a large pretrained vision and language model (CLIP) to encode compositional concepts and to bind variables in a structure-sensitive way (e.g., differentiating \u2018cube behind sphere\u2019 from \u2018sphere behind cube\u2019). To inspect the performance of CLIP, we compare several architectures from research on compositional distributional semantics models (CDSMs), a line of research that attempts to implement traditional compositional linguistic structures within embedding spaces. We benchmark them on three synthetic datasets \u2013 single-object, two-object, and relational \u2013 designed to test concept binding. We find that CLIP can compose concepts in a single-object setting, but in situations where concept binding is needed, performance drops dramatically. At the same time, CDSMs also perform poorly, with best performance at chance level.", "topic": "\"Transformer Models and Cognitive Mechanisms\""}, {"title": "Mapping Language Models to Grounded Conceptual Spaces", "authors": "Roma Patel, Ellie Pavlick", "year": 2022, "url": "https://www.semanticscholar.org/paper/57db9833549247241decf442fcc30f6eb414981b", "abstract": "A fundamental criticism of text-only language models (LMs) is their lack of grounding \u2014that is, the ability to tie a word for which they have learned a representation to its referent in the non-linguistic world. However, despite this limitation, large pre-trained LMs have been shown to have a remarkable grasp of the conceptual structure of language, as demonstrated by their ability to answer questions, generate \ufb02uent text, or make inferences about entities, objects, and properties that they have never physically observed. In this work we investigate the extent to which the rich conceptual structure that LMs learn indeed re\ufb02ects the conceptual structure of the non-linguistic world\u2014which is something that LMs have never observed. We do this by testing whether the LMs can learn to map an entire conceptual domain (e.g., direction or colour) onto a grounded world representation given only a small number of examples. For example, we show a model what the word \u201cleft\u201d means using a textual depiction of a grid world, and assess how well it can generalise to related concepts, for example, the word \u201cright\u201d , in a similar grid world. We investigate a range of generative language models of varying sizes (including GPT-2 and GPT-3), and see that although the smaller models struggle to perform this mapping, the largest model can not only learn to ground the concepts that it is explicitly taught, but appears to generalise to several instances of unseen concepts as well. Our results suggest an alternative means of building grounded language models: rather than learning grounded representations \u201cfrom scratch\u201d, it is possible that large text-only models learn a suf\ufb01ciently rich conceptual structure that could allow them to be grounded in a data-ef\ufb01cient way.", "topic": "\"Evaluating AI Compositional Reasoning\""}, {"title": "Unit Testing for Concepts in Neural Networks", "authors": "Charles Lovering, Ellie Pavlick", "year": 2022, "url": "https://www.semanticscholar.org/paper/6876441071b8faeac106a28cad3790b8646dca3f", "abstract": "Abstract Many complex problems are naturally understood in terms of symbolic concepts. For example, our concept of \u201ccat\u201d is related to our concepts of \u201cears\u201d and \u201cwhiskers\u201d in a non-arbitrary way. Fodor (1998) proposes one theory of concepts, which emphasizes symbolic representations related via constituency structures. Whether neural networks are consistent with such a theory is open for debate. We propose unit tests for evaluating whether a system\u2019s behavior is consistent with several key aspects of Fodor\u2019s criteria. Using a simple visual concept learning task, we evaluate several modern neural architectures against this specification. We find that models succeed on tests of groundedness, modularity, and reusability of concepts, but that important questions about causality remain open. Resolving these will require new methods for analyzing models\u2019 internal states.", "topic": "\"Neural Network Evaluation and Development\""}, {"title": "Pretraining on Interactions for Learning Grounded Affordance Representations", "authors": "Jack Merullo, Dylan Ebert, Carsten Eickhoff, Ellie Pavlick", "year": 2022, "url": "https://www.semanticscholar.org/paper/6d21550acbadd87980421b1ed103df0fcd992a68", "abstract": "Lexical semantics and cognitive science point to affordances (i.e. the actions that objects support) as critical for understanding and representing nouns and verbs. However, study of these semantic features has not yet been integrated with the ?foundation? models that currently dominate language representation research. We hypothesize that predictive modeling of object state over time will result in representations that encode object affordance information ?for free?. We train a neural network to predict objects? trajectories in a simulated interaction and show that our network?s latent representations differentiate between both observed and unobserved affordances. We find that models trained using 3D simulations outperform conventional 2D computer vision models trained on a similar task, and, on initial inspection, that differences between concepts correspond to expected features (e.g., roll entails rotation) . Our results suggest a way in which modern deep learning approaches to grounded language learning can be integrated with traditional formal semantic notions of lexical representations.", "topic": "\"Verb Semantics in Multimodal Models\""}, {"title": "Do Trajectories Encode Verb Meaning?", "authors": "Dylan Ebert, Chen Sun, Ellie Pavlick", "year": 2022, "url": "https://www.semanticscholar.org/paper/7037d5ac26fbedf130cc729f0ce3971f1f927e92", "abstract": "Distributional models learn representations of words from text, but are criticized for their lack of grounding, or the linking of text to the non-linguistic world. Grounded language models have had success in learning to connect concrete categories like nouns and adjectives to the world via images and videos, but can struggle to isolate the meaning of the verbs themselves from the context in which they typically occur. In this paper, we investigate the extent to which trajectories (i.e. the position and rotation of objects over time) naturally encode verb semantics. We build a procedurally generated agent-object-interaction dataset, obtain human annotations for the verbs that occur in this data, and compare several methods for representation learning given the trajectories. We find that trajectories correlate as-is with some verbs (e.g., fall), and that additional abstraction via self-supervised pretraining can further capture nuanced differences in verb meaning (e.g., roll and slide).", "topic": "\"Verb Semantics in Multimodal Models\""}, {"title": "Evaluation Beyond Task Performance: Analyzing Concepts in AlphaZero in Hex", "authors": "Charles Lovering, J. Forde, G. Konidaris, Ellie Pavlick, M. Littman", "year": 2022, "url": "https://www.semanticscholar.org/paper/85dea9ee99e7d11bee0c82c857ab355e36638c13", "abstract": "AlphaZero, an approach to reinforcement learning that couples neural networks and Monte Carlo tree search (MCTS), has produced state-of-the-art strategies for traditional board games like chess, Go, shogi, and Hex. While researchers and game commentators have suggested that AlphaZero uses concepts that humans consider important, it is unclear how these concepts are captured in the network. We investigate AlphaZero's internal representations in the game of Hex using two evaluation techniques from natural language processing (NLP): model probing and behavioral tests. In doing so, we introduce new evaluation tools to the RL community and illustrate how evaluations other than task performance can be used to provide a more complete picture of a model's strengths and weaknesses. Our analyses in the game of Hex reveal interesting patterns and generate some testable hypotheses about how such models learn in general. For example, we find that MCTS discovers concepts before the neural network learns to encode them. We also find that concepts related to short-term end-game planning are best encoded in the final layers of the model, whereas concepts related to long-term planning are encoded in the middle layers of the model.", "topic": "\"Neural Network Interpretability and Learning\""}, {"title": "BLOOM: A 176B-Parameter Open-Access Multilingual Language Model", "authors": "Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili'c, Daniel Hesslow, Roman Castagn'e, A. Luccioni, Fran\u00e7ois Yvon, Matthias Gall\u00e9, J. Tow, Alexander M. Rush, Stella Biderman, Albert Webson, Pawan Sasanka Ammanamanchi, Thomas Wang, Beno\u00eet Sagot, Niklas Muennighoff, Albert Villanova del Moral, Olatunji Ruwase, Rachel Bawden, Stas Bekman, Angelina McMillan-Major, Iz Beltagy, Huu Nguyen, Lucile Saulnier, Samson Tan, Pedro Ortiz Suarez, Victor Sanh, Hugo Laurenccon, Yacine Jernite, Julien Launay, Margaret Mitchell, Colin Raffel, Aaron Gokaslan, Adi Simhi, Aitor Soroa Etxabe, Alham Fikri Aji, Amit Alfassy, Anna Rogers, Ariel Kreisberg Nitzav, Canwen Xu, Chenghao Mou, Chris C. Emezue, Christopher Klamm, Colin Leong, Daniel Alexander van Strien, David Ifeoluwa Adelani, Dragomir R. Radev, E. G. Ponferrada, Efrat Levkovizh, Ethan Kim, Eyal Natan, F. Toni, G\u00e9rard Dupont, Germ\u00e1n Kruszewski, Giada Pistilli, Hady ElSahar, Hamza Benyamina, H. Tran, Ian Yu, Idris Abdulmumin, Isaac Johnson, Itziar Gonzalez-Dios, Javier de la Rosa, Jenny Chim, Jesse Dodge, Jian Zhu, Jonathan Chang, Jorg Frohberg, Josephine Tobing, J. Bhattacharjee, Khalid Almubarak, Kimbo Chen, Kyle Lo, Leandro von Werra, Leon Weber, Long Phan, Loubna Ben Allal, Ludovic Tanguy, Manan Dey, M. Mu\u00f1oz, Maraim Masoud, Mar'ia Grandury, Mario vSavsko, Max Huang, Maximin Coavoux, Mayank Singh, Mike Tian-Jian Jiang, Minh Chien Vu, M. A. Jauhar, Mustafa Ghaleb, Nishant Subramani, Nora Kassner, Nurulaqilla Khamis, Olivier Nguyen, Omar Espejel, Ona de Gibert, Paulo Villegas, Peter Henderson, Pierre Colombo, Priscilla Amuok, Quentin Lhoest, Rheza Harliman, Rishi Bommasani, R. L'opez, Rui Ribeiro, Salomey Osei, Sampo Pyysalo, Sebastian Nagel, Shamik Bose, Shamsuddeen Hassan Muhammad, Shanya Sharma, S. Longpre, Somaieh Nikpoor, S. Silberberg, S. Pai, S. Zink, Tiago Timponi Torrent, Timo Schick, Tristan Thrush, V. Danchev, Vassilina Nikoulina, Veronika Laippala, Violette Lepercq, V. Prabhu, Zaid Alyafeai, Zeerak Talat, Arun Raja, Benjamin Heinzerling, Chenglei Si, Elizabeth Salesky, Sabrina J. Mielke, Wilson Y. Lee, Abheesht Sharma, Andrea Santilli, Antoine Chaffin, Arnaud Stiegler, Debajyoti Datta, Eliza Szczechla, Gunjan Chhablani, Han Wang, Harshit Pandey, Hendrik Strobelt, Jason Alan Fries, Jos Rozen, Leo Gao, Lintang Sutawika, M Saiful Bari, Maged S. Al-Shaibani, Matteo Manica, Nihal V. Nayak, Ryan Teehan, Samuel Albanie, Sheng Shen, Srulik Ben-David, Stephen H. Bach, Taewoon Kim, T. Bers, Thibault F\u00e9vry, Trishala Neeraj, Urmish Thakker, Vikas Raunak, Xiang Tang, Zheng-Xin Yong, Zhiqing Sun, Shaked Brody, Y. Uri, Hadar Tojarieh, Adam Roberts, Hyung Won Chung, Jaesung Tae, Jason Phang, Ofir Press, Conglong Li, D. Narayanan, Hatim Bourfoune, J. Casper, Jeff Rasley, Max Ryabinin, Mayank Mishra, Minjia Zhang, M. Shoeybi, Myriam Peyrounette, N. Patry, Nouamane Tazi, Omar Sanseviero, Patrick von Platen, Pierre Cornette, Pierre Franccois Lavall'ee, R. Lacroix, Samyam Rajbhandari, Sanchit Gandhi, Shaden Smith, S. Requena, Suraj Patil, Tim Dettmers, Ahmed Baruwa, Amanpreet Singh, Anastasia Cheveleva, Anne-Laure Ligozat, Arjun Subramonian, Aur'elie N'ev'eol, Charles Lovering, Daniel H Garrette, D. Tunuguntla, Ehud Reiter, Ekaterina Taktasheva, E. Voloshina, Eli Bogdanov, Genta Indra Winata, Hailey Schoelkopf, Jan-Christoph Kalo, Jekaterina Novikova, J. Forde, Xiangru Tang, Jungo Kasai, Ken Kawamura, Liam Hazan, Marine Carpuat, Miruna Clinciu, Najoung Kim, Newton Cheng, O. Serikov, Omer Antverg, Oskar van der Wal, Rui Zhang, Ruochen Zhang, Sebastian Gehrmann, Shachar Mirkin, S. Pais, Tatiana Shavrina, Thomas Scialom, Tian Yun, Tomasz Limisiewicz, Verena Rieser, Vitaly Protasov, V. Mikhailov, Yada Pruksachatkun, Yonatan Belinkov, Zachary Bamberger, Zdenvek Kasner, Zden\u011bk Kasner, A. Pestana, A. Feizpour, Ammar Khan, Amy Faranak, A. Santos, Anthony Hevia, Antigona Unldreaj, Arash Aghagol, Arezoo Abdollahi, A. Tammour, A. HajiHosseini, Bahareh Behroozi, Benjamin Ayoade Ajibade, B. Saxena, Carlos Mu\u00f1oz Ferrandis, Danish Contractor, D. Lansky, Davis David, Douwe Kiela, D. A. Nguyen, Edward Tan, Emi Baylor, Ezinwanne Ozoani, F. Mirza, Frankline Ononiwu, Habib Rezanejad, H.A. Jones, Indrani Bhattacharya, Irene Solaiman, Irina Sedenko, I. Nejadgholi, J. Passmore, Joshua Seltzer, Julio Bonis Sanz, Karen Fort, L\u00edvia Dutra, Mairon Samagaio, Maraim Elbadri, Margot Mieskes, Marissa Gerchick, Martha Akinlolu, Michael McKenna, Mike Qiu, M. Ghauri, Mykola Burynok, Nafis Abrar, Nazneen Rajani, Nour Elkott, N. Fahmy, Olanrewaju Samuel, Ran An, R. Kromann, Ryan Hao, S. Alizadeh, Sarmad Shubber, Silas L. Wang, Sourav Roy, S. Viguier, Thanh-Cong Le, Tobi Oyebade, T. Le, Yoyo Yang, Zach Nguyen, Abhinav Ramesh Kashyap, A. Palasciano, A. Callahan, Anima Shukla, Antonio Miranda-Escalada, A. Singh, Benjamin Beilharz, Bo Wang, C. Brito, Chenxi Zhou, Chirag Jain, Chuxin Xu, Cl\u00e9mentine Fourrier, Daniel Le'on Perin'an, Daniel Molano, Dian Yu, Enrique Manjavacas, Fabio Barth, Florian Fuhrimann, Gabriel Altay, Giyaseddin Bayrak, Gully Burns, Helena U. Vrabec, I. Bello, Isha Dash, J. Kang, John Giorgi, Jonas Golde, J. Posada, Karthi Sivaraman, Lokesh Bulchandani, Lu Liu, Luisa Shinzato, Madeleine Hahn de Bykhovetz, Maiko Takeuchi, Marc P\u00e0mies, M. A. Castillo, Marianna Nezhurina, Mario Sanger, M. Samwald, Michael Cullan, Michael Weinberg, M. Wolf, Mina Mihaljcic, Minna Liu, M. Freidank, Myungsun Kang, Natasha Seelam, N. Dahlberg, N. Broad, N. Muellner, Pascale Fung, Patricia Haller, Patrick Haller, R. Eisenberg, Robert Martin, Rodrigo Canalli, Rosaline Su, Ruisi Su, Samuel Cahyawijaya, Samuele Garda, Shlok S Deshmukh, Shubhanshu Mishra, Sid Kiblawi, Simon Ott, Sinee Sang-aroonsiri, Srishti Kumar, Stefan Schweter, S. Bharati, Tanmay Laud, Th\u00e9o Gigant, Tomoya Kainuma, Wojciech Kusa, Yanis Labrak, Yashasvi Bajaj, Y. Venkatraman, Yifan Xu, Ying Xu, Yu Xu, Z. Tan, Zhongli Xie, Zifan Ye, M. Bras, Younes Belkada, Thomas Wolf", "year": 2022, "url": "https://www.semanticscholar.org/paper/964bd39b546f0f6625ff3b9ef1083f797807ef2e", "abstract": "Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.", "topic": "\"Neural Network Evaluation and Development\""}, {"title": "Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models", "authors": "Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R. Brown, Adam Santoro, Aditya Gupta, Adri\u00e0 Garriga-Alonso, Agnieszka Kluska, Aitor Lewkowycz, Akshat Agarwal, Alethea Power, Alex Ray, Alex Warstadt, Alexander W. Kocurek, Ali Safaya, Ali Tazarv, Alice Xiang, Alicia Parrish, Allen Nie, Aman Hussain, Amanda Askell, Amanda Dsouza, Ambrose Slone, A. Rahane, Anantharaman S. Iyer, Anders Andreassen, Andrea Madotto, Andrea Santilli, Andreas Stuhlmuller, Andrew M. Dai, Andrew La, Andrew Kyle Lampinen, Andy Zou, Angela Jiang, Angelica Chen, Anh Vuong, Animesh Gupta, Anna Gottardi, Antonio Norelli, Anu Venkatesh, Arash Gholamidavoodi, Arfa Tabassum, Arul Menezes, Arun Kirubarajan, A. Mullokandov, Ashish Sabharwal, Austin Herrick, Avia Efrat, Aykut Erdem, Ayla Karakacs, B. R. Roberts, B. S. Loe, Barret Zoph, Bartlomiej Bojanowski, Batuhan Ozyurt, Behnam Hedayatnia, Behnam Neyshabur, Benjamin Inden, Benno Stein, Berk Ekmekci, Bill Yuchen Lin, B. Howald, Bryan Orinion, Cameron Diao, Cameron Dour, Catherine Stinson, Cedrick Argueta, C'esar Ferri Ram'irez, Chandan Singh, Charles Rathkopf, Chenlin Meng, Chitta Baral, Chiyu Wu, Chris Callison-Burch, Chris Waites, Christian Voigt, Christopher D. Manning, Christopher Potts, Cindy Ramirez, Clara Rivera, Clemencia Siro, Colin Raffel, Courtney Ashcraft, Cristina Garbacea, Damien Sileo, Daniel H Garrette, Dan Hendrycks, D. Kilman, Dan Roth, Daniel Freeman, Daniel Khashabi, Daniel Levy, D. Gonz'alez, Danielle R. Perszyk, Danny Hernandez, Danqi Chen, Daphne Ippolito, D. Gilboa, David Dohan, D. Drakard, David Jurgens, Debajyoti Datta, Deep Ganguli, Denis Emelin, Denis Kleyko, Deniz Yuret, Derek Chen, Derek Tam, Dieuwke Hupkes, Diganta Misra, Dilyar Buzan, Dimitri Coelho Mollo, Diyi Yang, Dong-Ho Lee, Dylan Schrader, Ekaterina Shutova, E. D. Cubuk, Elad Segal, Eleanor Hagerman, Elizabeth Barnes, E. Donoway, Ellie Pavlick, E. Rodol\u00e0, Emma Lam, Eric Chu, Eric Tang, Erkut Erdem, Ernie Chang, Ethan A. Chi, Ethan Dyer, E. Jerzak, Ethan Kim, Eunice Engefu Manyasi, Evgenii Zheltonozhskii, Fanyue Xia, F. Siar, Fernando Mart'inez-Plumed, Francesca Happ'e, Fran\u00e7ois Chollet, Frieda Rong, Gaurav Mishra, Genta Indra Winata, Gerard de Melo, Germ\u00e1n Kruszewski, Giambattista Parascandolo, Giorgio Mariani, Gloria Xinyue Wang, Gonzalo Jaimovitch-L'opez, Gregor Betz, Guy Gur-Ari, Hana Galijasevic, Hannah Kim, Hannah Rashkin, Hannaneh Hajishirzi, Harsh Mehta, H. Bogar, Henry Shevlin, Hinrich Schutze, H. Yakura, Hongming Zhang, Hugh Mee Wong, Ian Ng, Isaac Noble, Jaap Jumelet, Jack Geissinger, John Kernion, Jacob Hilton, Jaehoon Lee, J. Fisac, James B. Simon, James Koppel, James Zheng, James Zou, Jan Koco'n, Jana Thompson, Janelle Wingfield, Jared Kaplan, Jarema Radom, Jascha Narain Sohl-Dickstein, Jason Phang, Jason Wei, J. Yosinski, Jekaterina Novikova, Jelle Bosscher, Jennifer Marsh, Jeremy Kim, Jeroen Taal, Jesse Engel, Jesujoba Oluwadara Alabi, Jiacheng Xu, Jiaming Song, Jillian Tang, Jane W Waweru, John Burden, John Miller, John U. Balis, Jonathan Batchelder, Jonathan Berant, Jorg Frohberg, Jos Rozen, J. Hern\u00e1ndez-Orallo, Joseph Boudeman, Joseph Guerr, Joseph Jones, Joshua B Tenenbaum, Joshua S. Rule, Joyce Chua, Kamil Kanclerz, Karen Livescu, K. Krauth, Karthik Gopalakrishnan, Katerina Ignatyeva, K. Markert, Kaustubh D. Dhole, Kevin Gimpel, Kevin Omondi, K. Mathewson, Kristen Chiafullo, Ksenia Shkaruta, K. Shridhar, Kyle McDonell, Kyle Richardson, Laria Reynolds, Leo Gao, Li Zhang, Liam Dugan, Lianhui Qin, Lidia Contreras-Ochando, Louis-Philippe Morency, Luca Moschella, Luca Lam, Lucy Noble, Ludwig Schmidt, Luheng He, Luis Oliveros Col'on, Luke Metz, Lutfi Kerem cSenel, Maarten Bosma, Maarten Sap, Maartje ter Hoeve, Maheen Farooqi, Manaal Faruqui, Mantas Mazeika, Marco Baturan, Marco Marelli, Marco Maru, Maria Jose Ram'irez Quintana, Marie Tolkiehn, Mario Giulianelli, Martha Lewis, Martin Potthast, Matthew L. Leavitt, Matthias Hagen, M. Schubert, Medina Baitemirova, Melody Arnaud, M. McElrath, Michael Yee, Michael Cohen, Michael Gu, Michael Ivanitskiy, Michael Starritt, M. Strube, Michal Swkedrowski, Michele Bevilacqua, Michihiro Yasunaga, Mihir Kale, Mike Cain, Mimee Xu, Mirac Suzgun, Mitch Walker, Monica Tiwari, Mohit Bansal, Moin Aminnaseri, Mor Geva, Mozhdeh Gheini, T. MukundVarma, Nanyun Peng, Nathan A. Chi, Nayeon Lee, Neta Gur-Ari Krakover, Nicholas Cameron, Nicholas Roberts, Nick Doiron, Nicole Martinez, Nikita Nangia, Niklas Deckers, Niklas Muennighoff, N. Keskar, Niveditha Iyer, Noah Constant, Noah Fiedel, Nuan Wen, Oliver Zhang, Omar Agha, Omar Elbaghdadi, Omer Levy, Owain Evans, Pablo Antonio Moreno Casares, P. Doshi, Pascale Fung, P. Liang, Paul Vicol, Pegah Alipoormolabashi, Peiyuan Liao, Percy Liang, Peter Chang, P. Eckersley, Phu Mon Htut, Pi-Bei Hwang, P. Milkowski, P. Patil, Pouya Pezeshkpour, Priti Oli, Qiaozhu Mei, Qing Lyu, Qinlang Chen, Rabin Banjade, Rachel Etta Rudolph, Raefer Gabriel, Rahel Habacker, Ramon Risco, Raphael Milliere, Rhythm Garg, Richard Barnes, R. Saurous, Riku Arakawa, Robbe Raymaekers, Robert Frank, Rohan Sikand, Roman Novak, Roman Sitelew, Ronan Le Bras, Rosanne Liu, Rowan Jacobs, Rui Zhang, R. Salakhutdinov, Ryan Chi, Ryan Lee, Ryan Stovall, Ryan Teehan, Rylan Yang, Sahib Singh, Saif M. Mohammad, Sajant Anand, Sam Dillavou, Sam Shleifer, Sam Wiseman, Samuel Gruetter, Samuel R. Bowman, S. Schoenholz, Sanghyun Han, Sanjeev Kwatra, Sarah A. Rous, Sarik Ghazarian, Sayan Ghosh, Sean Casey, Sebastian Bischoff, Sebastian Gehrmann, Sebastian Schuster, Sepideh Sadeghi, Shadi S. Hamdan, Sharon Zhou, Shashank Srivastava, Sherry Shi, Shikhar Singh, Shima Asaadi, S. Gu, Shubh Pachchigar, Shubham Toshniwal, Shyam Upadhyay, Shyamolima Debnath, Siamak Shakeri, Simon Thormeyer, S. Melzi, Siva Reddy, S. Makini, Soo-Hwan Lee, Spencer Torene, Sriharsha Hatwar, S. Dehaene, Stefan Divic, Stefano Ermon, Stella Biderman, Stephanie Lin, Stephen Prasad, Steven T Piantadosi, Stuart M. Shieber, Summer Misherghi, S. Kiritchenko, Swaroop Mishra, Tal Linzen, Tal Schuster, Tao Li, Tao Yu, Tariq Ali, Tatsunori Hashimoto, Te-Lin Wu, T. Desbordes, Theodore Rothschild, Thomas Phan, Tianle Wang, Tiberius Nkinyili, Timo Schick, T. Kornev, T. Tunduny, Tobias Gerstenberg, T. Chang, Trishala Neeraj, Tushar Khot, Tyler Shultz, Uri Shaham, Vedant Misra, Vera Demberg, Victoria Nyamai, Vikas Raunak, V. Ramasesh, Vinay Uday Prabhu, Vishakh Padmakumar, Vivek Srikumar, W. Fedus, W. Saunders, William Zhang, Wout Vossen, Xiang Ren, Xiaoyu Tong, Xinran Zhao, Xinyi Wu, Xudong Shen, Yadollah Yaghoobzadeh, Yair Lakretz, Yangqiu Song, Yasaman Bahri, Yejin Choi, Yichi Yang, Yiding Hao, Yifu Chen, Yonatan Belinkov, Yu Hou, Yu Hou, Yuntao Bai, Zachary Seid, Zhuoye Zhao, Zijian Wang, Zijie J. Wang, Zirui Wang, Ziyi Wu", "year": 2022, "url": "https://www.semanticscholar.org/paper/bd1331b233e84bab7eba503abc60b31ac08e7881", "abstract": "Language models demonstrate both quantitative improvement and new qualitative capabilities with increasing scale. Despite their potentially transformative impact, these new capabilities are as yet poorly characterized. In order to inform future research, prepare for disruptive new model capabilities, and ameliorate socially harmful effects, it is vital that we understand the present and near-future capabilities and limitations of language models. To address this challenge, we introduce the Beyond the Imitation Game benchmark (BIG-bench). BIG-bench currently consists of 204 tasks, contributed by 450 authors across 132 institutions. Task topics are diverse, drawing problems from linguistics, childhood development, math, common-sense reasoning, biology, physics, social bias, software development, and beyond. BIG-bench focuses on tasks that are believed to be beyond the capabilities of current language models. We evaluate the behavior of OpenAI's GPT models, Google-internal dense transformer architectures, and Switch-style sparse transformers on BIG-bench, across model sizes spanning millions to hundreds of billions of parameters. In addition, a team of human expert raters performed all tasks in order to provide a strong baseline. Findings include: model performance and calibration both improve with scale, but are poor in absolute terms (and when compared with rater performance); performance is remarkably similar across model classes, though with benefits from sparsity; tasks that improve gradually and predictably commonly involve a large knowledge or memorization component, whereas tasks that exhibit\"breakthrough\"behavior at a critical scale often involve multiple steps or components, or brittle metrics; social bias typically increases with scale in settings with ambiguous context, but this can be improved with prompting.", "topic": "\"Evaluating AI Compositional Reasoning\""}, {"title": "Linearly Mapping from Image to Text Space", "authors": "Jack Merullo, Louis Castricato, Carsten Eickhoff, Ellie Pavlick", "year": 2022, "url": "https://www.semanticscholar.org/paper/c4cb3f7056f1216c1ddfbe4b9e55cbc07a1e43b9", "abstract": "The extent to which text-only language models (LMs) learn to represent features of the non-linguistic world is an open question. Prior work has shown that pretrained LMs can be taught to caption images when a vision model's parameters are optimized to encode images in the language space. We test a stronger hypothesis: that the conceptual representations learned by frozen text-only models and vision-only models are similar enough that this can be achieved with a linear map. We show that the image representations from vision models can be transferred as continuous prompts to frozen LMs by training only a single linear projection. Using these to prompt the LM achieves competitive performance on captioning and visual question answering tasks compared to models that tune both the image encoder and text decoder (such as the MAGMA model). We compare three image encoders with increasing amounts of linguistic supervision seen during pretraining: BEIT (no linguistic information), NF-ResNET (lexical category information), and CLIP (full natural language descriptions). We find that all three encoders perform equally well at transferring visual property information to the language model (e.g., whether an animal is large or small), but that image encoders pretrained with linguistic supervision more saliently encode category information (e.g., distinguishing hippo vs. elephant) and thus perform significantly better on benchmark language-and-vision tasks. Our results indicate that LMs encode conceptual information structurally similarly to vision-based models, even those that are solely trained on images. Code is available here: https://github.com/jmerullo/limber", "topic": "\"Evaluating AI Compositional Reasoning\""}, {"title": "Do Vision-Language Pretrained Models Learn Primitive Concepts?", "authors": "Tian Yun, Usha Bhalla, Ellie Pavlick, Chen Sun", "year": 2022, "url": "https://www.semanticscholar.org/paper/dbb012d143fdcbf8d02c0a5933c153767bd1cf12", "abstract": ". Vision-language pretrained models have achieved impressive performance on multimodal reasoning and zero-shot recognition tasks. Many of these VL models are pretrained on unlabeled image and caption pairs from the internet. In this paper, we study whether the notion of primitive concepts, such as color and shape attributes, emerges automatically from these pretrained VL models. We propose to learn compositional derivations that map primitive concept activations into composite concepts, a task which we demonstrate to be straightforward given true primitive concept annotations. This compositional derivation learning (CompDL) framework allows us to quantitively measure the usefulness and interpretability of the learned derivations, by jointly considering the entire set of candidate primitive concepts. Our study reveals that state-of-the-art VL pretrained models learn primitive concepts that are highly useful as visual descriptors, as demonstrated by their strong performance on fine-grained visual recognition tasks, but those concepts struggle to provide interpretable compositional derivations, which highlights limitations of existing VL models. Code and models will be released.", "topic": "\"Evaluating AI Compositional Reasoning\""}], "analysis": "1. **In-context learning in neural networks**: Investigating how neural networks, particularly large language models (LLMs) and metalearning transformers, exhibit in-context learning similar to human learning under different training curricula.\n\n2. **Cognitive mechanisms in Transformer models**: Analyzing how Transformer models develop mechanisms analogous to human frontostriatal gating mechanisms for selective memory updating and retrieval.\n\n3. **Compositional reasoning in vision-language models**: Evaluating the ability of vision-language models to perform compositional reasoning, particularly in visual question-answering tasks.\n\n4. **Lexical concept representation in large language models**: Assessing how large language models like GPT-3 and GPT-4 represent lexical concepts, especially those related to artifacts, through the lens of HIPE theory.\n\n5. **Circuit probing in neural networks**: Developing and applying a new analysis technique called circuit probing to uncover low-level circuits in neural networks that compute hypothesized intermediate variables.\n\n6. **Abstract state representation in sequence modeling**: Investigating the emergence of abstract state representations in sequence modeling Transformers trained on language.", "author_name": "Ellie Pavlick", "coauthors_histogram": {"Jacob Russin": 1, "Michael J. Frank": 2, "Aaron Traylor": 2, "Jack Merullo": 7, "Jessica Li": 1, "Apoorv Khandelwal": 2, "Sam Musker": 1, "Michael A. Lepori": 5, "Thomas Serre": 3, "Dylan Ebert": 3, "Chen Sun": 6, "Tian Yun": 4, "Zilai Zeng": 1, "Kunal Handa": 1, "Ashish V. Thapliyal": 1, "Bo Pang": 1, "Enyan Zhang": 1, "Charles Lovering": 6, "Carsten Eickhoff": 4, "Alexa R. Tartaglini": 1, "Sheridan Feucht": 1, "Wai Keen Vong": 1, "B. Lake": 1, "Albert Webson": 3, "Alyssa Marie Loo": 1, "Qinan Yu": 3, "Kelvin Guu": 1, "Lucas Dixon": 1, "Ian Tenney": 1, "Tolga Bolukbasi": 1, "Roman Feiman": 1, "Yoonseon Oh": 1, "Roma Patel": 2, "Thao Nguyen": 1, "Baichuan Huang": 1, "Matthew Berg": 1, "Stefanie Tellex": 1, "J. Forde": 3, "G. Konidaris": 2, "M. Littman": 2, "Usha Bhalla": 2, "Martha Lewis": 2, "Nihal V. Nayak": 2, "Teven Le Scao": 1, "Angela Fan": 1, "Christopher Akiki": 1, "Suzana Ili'c": 1, "Daniel Hesslow": 1, "Roman Castagn'e": 1, "A. Luccioni": 1, "Fran\u00e7ois Yvon": 1, "Matthias Gall\u00e9": 1, "J. Tow": 1, "Alexander M. Rush": 1, "Stella Biderman": 2, "Pawan Sasanka Ammanamanchi": 1, "Thomas Wang": 1, "Beno\u00eet Sagot": 1, "Niklas Muennighoff": 2, "Albert Villanova del Moral": 1, "Olatunji Ruwase": 1, "Rachel Bawden": 1, "Stas Bekman": 1, "Angelina McMillan-Major": 1, "Iz Beltagy": 1, "Huu Nguyen": 1, "Lucile Saulnier": 1, "Samson Tan": 1, "Pedro Ortiz Suarez": 1, "Victor Sanh": 1, "Hugo Laurenccon": 1, "Yacine Jernite": 1, "Julien Launay": 1, "Margaret Mitchell": 1, "Colin Raffel": 2, "Aaron Gokaslan": 1, "Adi Simhi": 1, "Aitor Soroa Etxabe": 1, "Alham Fikri Aji": 1, "Amit Alfassy": 1, "Anna Rogers": 1, "Ariel Kreisberg Nitzav": 1, "Canwen Xu": 1, "Chenghao Mou": 1, "Chris C. Emezue": 1, "Christopher Klamm": 1, "Colin Leong": 1, "Daniel Alexander van Strien": 1, "David Ifeoluwa Adelani": 1, "Dragomir R. Radev": 1, "E. G. Ponferrada": 1, "Efrat Levkovizh": 1, "Ethan Kim": 2, "Eyal Natan": 1, "F. Toni": 1, "G\u00e9rard Dupont": 1, "Germ\u00e1n Kruszewski": 2, "Giada Pistilli": 1, "Hady ElSahar": 1, "Hamza Benyamina": 1, "H. Tran": 1, "Ian Yu": 1, "Idris Abdulmumin": 1, "Isaac Johnson": 1, "Itziar Gonzalez-Dios": 1, "Javier de la Rosa": 1, "Jenny Chim": 1, "Jesse Dodge": 1, "Jian Zhu": 1, "Jonathan Chang": 1, "Jorg Frohberg": 2, "Josephine Tobing": 1, "J. Bhattacharjee": 1, "Khalid Almubarak": 1, "Kimbo Chen": 1, "Kyle Lo": 1, "Leandro von Werra": 1, "Leon Weber": 1, "Long Phan": 1, "Loubna Ben Allal": 1, "Ludovic Tanguy": 1, "Manan Dey": 1, "M. Mu\u00f1oz": 1, "Maraim Masoud": 1, "Mar'ia Grandury": 1, "Mario vSavsko": 1, "Max Huang": 1, "Maximin Coavoux": 1, "Mayank Singh": 1, "Mike Tian-Jian Jiang": 1, "Minh Chien Vu": 1, "M. A. Jauhar": 1, "Mustafa Ghaleb": 1, "Nishant Subramani": 1, "Nora Kassner": 1, "Nurulaqilla Khamis": 1, "Olivier Nguyen": 1, "Omar Espejel": 1, "Ona de Gibert": 1, "Paulo Villegas": 1, "Peter Henderson": 1, "Pierre Colombo": 1, "Priscilla Amuok": 1, "Quentin Lhoest": 1, "Rheza Harliman": 1, "Rishi Bommasani": 1, "R. L'opez": 1, "Rui Ribeiro": 1, "Salomey Osei": 1, "Sampo Pyysalo": 1, "Sebastian Nagel": 1, "Shamik Bose": 1, "Shamsuddeen Hassan Muhammad": 1, "Shanya Sharma": 1, "S. Longpre": 1, "Somaieh Nikpoor": 1, "S. Silberberg": 1, "S. Pai": 1, "S. Zink": 1, "Tiago Timponi Torrent": 1, "Timo Schick": 2, "Tristan Thrush": 1, "V. Danchev": 1, "Vassilina Nikoulina": 1, "Veronika Laippala": 1, "Violette Lepercq": 1, "V. Prabhu": 1, "Zaid Alyafeai": 1, "Zeerak Talat": 1, "Arun Raja": 1, "Benjamin Heinzerling": 1, "Chenglei Si": 1, "Elizabeth Salesky": 1, "Sabrina J. Mielke": 1, "Wilson Y. Lee": 1, "Abheesht Sharma": 1, "Andrea Santilli": 2, "Antoine Chaffin": 1, "Arnaud Stiegler": 1, "Debajyoti Datta": 2, "Eliza Szczechla": 1, "Gunjan Chhablani": 1, "Han Wang": 1, "Harshit Pandey": 1, "Hendrik Strobelt": 1, "Jason Alan Fries": 1, "Jos Rozen": 2, "Leo Gao": 2, "Lintang Sutawika": 1, "M Saiful Bari": 1, "Maged S. Al-Shaibani": 1, "Matteo Manica": 1, "Ryan Teehan": 2, "Samuel Albanie": 1, "Sheng Shen": 1, "Srulik Ben-David": 1, "Stephen H. Bach": 1, "Taewoon Kim": 1, "T. Bers": 1, "Thibault F\u00e9vry": 1, "Trishala Neeraj": 2, "Urmish Thakker": 1, "Vikas Raunak": 2, "Xiang Tang": 1, "Zheng-Xin Yong": 1, "Zhiqing Sun": 1, "Shaked Brody": 1, "Y. Uri": 1, "Hadar Tojarieh": 1, "Adam Roberts": 1, "Hyung Won Chung": 1, "Jaesung Tae": 1, "Jason Phang": 2, "Ofir Press": 1, "Conglong Li": 1, "D. Narayanan": 1, "Hatim Bourfoune": 1, "J. Casper": 1, "Jeff Rasley": 1, "Max Ryabinin": 1, "Mayank Mishra": 1, "Minjia Zhang": 1, "M. Shoeybi": 1, "Myriam Peyrounette": 1, "N. Patry": 1, "Nouamane Tazi": 1, "Omar Sanseviero": 1, "Patrick von Platen": 1, "Pierre Cornette": 1, "Pierre Franccois Lavall'ee": 1, "R. Lacroix": 1, "Samyam Rajbhandari": 1, "Sanchit Gandhi": 1, "Shaden Smith": 1, "S. Requena": 1, "Suraj Patil": 1, "Tim Dettmers": 1, "Ahmed Baruwa": 1, "Amanpreet Singh": 1, "Anastasia Cheveleva": 1, "Anne-Laure Ligozat": 1, "Arjun Subramonian": 1, "Aur'elie N'ev'eol": 1, "Daniel H Garrette": 2, "D. Tunuguntla": 1, "Ehud Reiter": 1, "Ekaterina Taktasheva": 1, "E. Voloshina": 1, "Eli Bogdanov": 1, "Genta Indra Winata": 2, "Hailey Schoelkopf": 1, "Jan-Christoph Kalo": 1, "Jekaterina Novikova": 2, "Xiangru Tang": 1, "Jungo Kasai": 1, "Ken Kawamura": 1, "Liam Hazan": 1, "Marine Carpuat": 1, "Miruna Clinciu": 1, "Najoung Kim": 1, "Newton Cheng": 1, "O. Serikov": 1, "Omer Antverg": 1, "Oskar van der Wal": 1, "Rui Zhang": 2, "Ruochen Zhang": 1, "Sebastian Gehrmann": 2, "Shachar Mirkin": 1, "S. Pais": 1, "Tatiana Shavrina": 1, "Thomas Scialom": 1, "Tomasz Limisiewicz": 1, "Verena Rieser": 1, "Vitaly Protasov": 1, "V. Mikhailov": 1, "Yada Pruksachatkun": 1, "Yonatan Belinkov": 2, "Zachary Bamberger": 1, "Zdenvek Kasner": 1, "Zden\u011bk Kasner": 1, "A. Pestana": 1, "A. Feizpour": 1, "Ammar Khan": 1, "Amy Faranak": 1, "A. Santos": 1, "Anthony Hevia": 1, "Antigona Unldreaj": 1, "Arash Aghagol": 1, "Arezoo Abdollahi": 1, "A. Tammour": 1, "A. HajiHosseini": 1, "Bahareh Behroozi": 1, "Benjamin Ayoade Ajibade": 1, "B. Saxena": 1, "Carlos Mu\u00f1oz Ferrandis": 1, "Danish Contractor": 1, "D. Lansky": 1, "Davis David": 1, "Douwe Kiela": 1, "D. A. Nguyen": 1, "Edward Tan": 1, "Emi Baylor": 1, "Ezinwanne Ozoani": 1, "F. Mirza": 1, "Frankline Ononiwu": 1, "Habib Rezanejad": 1, "H.A. Jones": 1, "Indrani Bhattacharya": 1, "Irene Solaiman": 1, "Irina Sedenko": 1, "I. Nejadgholi": 1, "J. Passmore": 1, "Joshua Seltzer": 1, "Julio Bonis Sanz": 1, "Karen Fort": 1, "L\u00edvia Dutra": 1, "Mairon Samagaio": 1, "Maraim Elbadri": 1, "Margot Mieskes": 1, "Marissa Gerchick": 1, "Martha Akinlolu": 1, "Michael McKenna": 1, "Mike Qiu": 1, "M. Ghauri": 1, "Mykola Burynok": 1, "Nafis Abrar": 1, "Nazneen Rajani": 1, "Nour Elkott": 1, "N. Fahmy": 1, "Olanrewaju Samuel": 1, "Ran An": 1, "R. Kromann": 1, "Ryan Hao": 1, "S. Alizadeh": 1, "Sarmad Shubber": 1, "Silas L. Wang": 1, "Sourav Roy": 1, "S. Viguier": 1, "Thanh-Cong Le": 1, "Tobi Oyebade": 1, "T. Le": 1, "Yoyo Yang": 1, "Zach Nguyen": 1, "Abhinav Ramesh Kashyap": 1, "A. Palasciano": 1, "A. Callahan": 1, "Anima Shukla": 1, "Antonio Miranda-Escalada": 1, "A. Singh": 1, "Benjamin Beilharz": 1, "Bo Wang": 1, "C. Brito": 1, "Chenxi Zhou": 1, "Chirag Jain": 1, "Chuxin Xu": 1, "Cl\u00e9mentine Fourrier": 1, "Daniel Le'on Perin'an": 1, "Daniel Molano": 1, "Dian Yu": 1, "Enrique Manjavacas": 1, "Fabio Barth": 1, "Florian Fuhrimann": 1, "Gabriel Altay": 1, "Giyaseddin Bayrak": 1, "Gully Burns": 1, "Helena U. Vrabec": 1, "I. Bello": 1, "Isha Dash": 1, "J. Kang": 1, "John Giorgi": 1, "Jonas Golde": 1, "J. Posada": 1, "Karthi Sivaraman": 1, "Lokesh Bulchandani": 1, "Lu Liu": 1, "Luisa Shinzato": 1, "Madeleine Hahn de Bykhovetz": 1, "Maiko Takeuchi": 1, "Marc P\u00e0mies": 1, "M. A. Castillo": 1, "Marianna Nezhurina": 1, "Mario Sanger": 1, "M. Samwald": 1, "Michael Cullan": 1, "Michael Weinberg": 1, "M. Wolf": 1, "Mina Mihaljcic": 1, "Minna Liu": 1, "M. Freidank": 1, "Myungsun Kang": 1, "Natasha Seelam": 1, "N. Dahlberg": 1, "N. Broad": 1, "N. Muellner": 1, "Pascale Fung": 2, "Patricia Haller": 1, "Patrick Haller": 1, "R. Eisenberg": 1, "Robert Martin": 1, "Rodrigo Canalli": 1, "Rosaline Su": 1, "Ruisi Su": 1, "Samuel Cahyawijaya": 1, "Samuele Garda": 1, "Shlok S Deshmukh": 1, "Shubhanshu Mishra": 1, "Sid Kiblawi": 1, "Simon Ott": 1, "Sinee Sang-aroonsiri": 1, "Srishti Kumar": 1, "Stefan Schweter": 1, "S. Bharati": 1, "Tanmay Laud": 1, "Th\u00e9o Gigant": 1, "Tomoya Kainuma": 1, "Wojciech Kusa": 1, "Yanis Labrak": 1, "Yashasvi Bajaj": 1, "Y. Venkatraman": 1, "Yifan Xu": 1, "Ying Xu": 1, "Yu Xu": 1, "Z. Tan": 1, "Zhongli Xie": 1, "Zifan Ye": 1, "M. Bras": 1, "Younes Belkada": 1, "Thomas Wolf": 1, "Aarohi Srivastava": 1, "Abhinav Rastogi": 1, "Abhishek Rao": 1, "Abu Awal Md Shoeb": 1, "Abubakar Abid": 1, "Adam Fisch": 1, "Adam R. Brown": 1, "Adam Santoro": 1, "Aditya Gupta": 1, "Adri\u00e0 Garriga-Alonso": 1, "Agnieszka Kluska": 1, "Aitor Lewkowycz": 1, "Akshat Agarwal": 1, "Alethea Power": 1, "Alex Ray": 1, "Alex Warstadt": 1, "Alexander W. Kocurek": 1, "Ali Safaya": 1, "Ali Tazarv": 1, "Alice Xiang": 1, "Alicia Parrish": 1, "Allen Nie": 1, "Aman Hussain": 1, "Amanda Askell": 1, "Amanda Dsouza": 1, "Ambrose Slone": 1, "A. Rahane": 1, "Anantharaman S. Iyer": 1, "Anders Andreassen": 1, "Andrea Madotto": 1, "Andreas Stuhlmuller": 1, "Andrew M. Dai": 1, "Andrew La": 1, "Andrew Kyle Lampinen": 1, "Andy Zou": 1, "Angela Jiang": 1, "Angelica Chen": 1, "Anh Vuong": 1, "Animesh Gupta": 1, "Anna Gottardi": 1, "Antonio Norelli": 1, "Anu Venkatesh": 1, "Arash Gholamidavoodi": 1, "Arfa Tabassum": 1, "Arul Menezes": 1, "Arun Kirubarajan": 1, "A. Mullokandov": 1, "Ashish Sabharwal": 1, "Austin Herrick": 1, "Avia Efrat": 1, "Aykut Erdem": 1, "Ayla Karakacs": 1, "B. R. Roberts": 1, "B. S. Loe": 1, "Barret Zoph": 1, "Bartlomiej Bojanowski": 1, "Batuhan Ozyurt": 1, "Behnam Hedayatnia": 1, "Behnam Neyshabur": 1, "Benjamin Inden": 1, "Benno Stein": 1, "Berk Ekmekci": 1, "Bill Yuchen Lin": 1, "B. Howald": 1, "Bryan Orinion": 1, "Cameron Diao": 1, "Cameron Dour": 1, "Catherine Stinson": 1, "Cedrick Argueta": 1, "C'esar Ferri Ram'irez": 1, "Chandan Singh": 1, "Charles Rathkopf": 1, "Chenlin Meng": 1, "Chitta Baral": 1, "Chiyu Wu": 1, "Chris Callison-Burch": 1, "Chris Waites": 1, "Christian Voigt": 1, "Christopher D. Manning": 1, "Christopher Potts": 1, "Cindy Ramirez": 1, "Clara Rivera": 1, "Clemencia Siro": 1, "Courtney Ashcraft": 1, "Cristina Garbacea": 1, "Damien Sileo": 1, "Dan Hendrycks": 1, "D. Kilman": 1, "Dan Roth": 1, "Daniel Freeman": 1, "Daniel Khashabi": 1, "Daniel Levy": 1, "D. Gonz'alez": 1, "Danielle R. Perszyk": 1, "Danny Hernandez": 1, "Danqi Chen": 1, "Daphne Ippolito": 1, "D. Gilboa": 1, "David Dohan": 1, "D. Drakard": 1, "David Jurgens": 1, "Deep Ganguli": 1, "Denis Emelin": 1, "Denis Kleyko": 1, "Deniz Yuret": 1, "Derek Chen": 1, "Derek Tam": 1, "Dieuwke Hupkes": 1, "Diganta Misra": 1, "Dilyar Buzan": 1, "Dimitri Coelho Mollo": 1, "Diyi Yang": 1, "Dong-Ho Lee": 1, "Dylan Schrader": 1, "Ekaterina Shutova": 1, "E. D. Cubuk": 1, "Elad Segal": 1, "Eleanor Hagerman": 1, "Elizabeth Barnes": 1, "E. Donoway": 1, "E. Rodol\u00e0": 1, "Emma Lam": 1, "Eric Chu": 1, "Eric Tang": 1, "Erkut Erdem": 1, "Ernie Chang": 1, "Ethan A. Chi": 1, "Ethan Dyer": 1, "E. Jerzak": 1, "Eunice Engefu Manyasi": 1, "Evgenii Zheltonozhskii": 1, "Fanyue Xia": 1, "F. Siar": 1, "Fernando Mart'inez-Plumed": 1, "Francesca Happ'e": 1, "Fran\u00e7ois Chollet": 1, "Frieda Rong": 1, "Gaurav Mishra": 1, "Gerard de Melo": 1, "Giambattista Parascandolo": 1, "Giorgio Mariani": 1, "Gloria Xinyue Wang": 1, "Gonzalo Jaimovitch-L'opez": 1, "Gregor Betz": 1, "Guy Gur-Ari": 1, "Hana Galijasevic": 1, "Hannah Kim": 1, "Hannah Rashkin": 1, "Hannaneh Hajishirzi": 1, "Harsh Mehta": 1, "H. Bogar": 1, "Henry Shevlin": 1, "Hinrich Schutze": 1, "H. Yakura": 1, "Hongming Zhang": 1, "Hugh Mee Wong": 1, "Ian Ng": 1, "Isaac Noble": 1, "Jaap Jumelet": 1, "Jack Geissinger": 1, "John Kernion": 1, "Jacob Hilton": 1, "Jaehoon Lee": 1, "J. Fisac": 1, "James B. Simon": 1, "James Koppel": 1, "James Zheng": 1, "James Zou": 1, "Jan Koco'n": 1, "Jana Thompson": 1, "Janelle Wingfield": 1, "Jared Kaplan": 1, "Jarema Radom": 1, "Jascha Narain Sohl-Dickstein": 1, "Jason Wei": 1, "J. Yosinski": 1, "Jelle Bosscher": 1, "Jennifer Marsh": 1, "Jeremy Kim": 1, "Jeroen Taal": 1, "Jesse Engel": 1, "Jesujoba Oluwadara Alabi": 1, "Jiacheng Xu": 1, "Jiaming Song": 1, "Jillian Tang": 1, "Jane W Waweru": 1, "John Burden": 1, "John Miller": 1, "John U. Balis": 1, "Jonathan Batchelder": 1, "Jonathan Berant": 1, "J. Hern\u00e1ndez-Orallo": 1, "Joseph Boudeman": 1, "Joseph Guerr": 1, "Joseph Jones": 1, "Joshua B Tenenbaum": 1, "Joshua S. Rule": 1, "Joyce Chua": 1, "Kamil Kanclerz": 1, "Karen Livescu": 1, "K. Krauth": 1, "Karthik Gopalakrishnan": 1, "Katerina Ignatyeva": 1, "K. Markert": 1, "Kaustubh D. Dhole": 1, "Kevin Gimpel": 1, "Kevin Omondi": 1, "K. Mathewson": 1, "Kristen Chiafullo": 1, "Ksenia Shkaruta": 1, "K. Shridhar": 1, "Kyle McDonell": 1, "Kyle Richardson": 1, "Laria Reynolds": 1, "Li Zhang": 1, "Liam Dugan": 1, "Lianhui Qin": 1, "Lidia Contreras-Ochando": 1, "Louis-Philippe Morency": 1, "Luca Moschella": 1, "Luca Lam": 1, "Lucy Noble": 1, "Ludwig Schmidt": 1, "Luheng He": 1, "Luis Oliveros Col'on": 1, "Luke Metz": 1, "Lutfi Kerem cSenel": 1, "Maarten Bosma": 1, "Maarten Sap": 1, "Maartje ter Hoeve": 1, "Maheen Farooqi": 1, "Manaal Faruqui": 1, "Mantas Mazeika": 1, "Marco Baturan": 1, "Marco Marelli": 1, "Marco Maru": 1, "Maria Jose Ram'irez Quintana": 1, "Marie Tolkiehn": 1, "Mario Giulianelli": 1, "Martin Potthast": 1, "Matthew L. Leavitt": 1, "Matthias Hagen": 1, "M. Schubert": 1, "Medina Baitemirova": 1, "Melody Arnaud": 1, "M. McElrath": 1, "Michael Yee": 1, "Michael Cohen": 1, "Michael Gu": 1, "Michael Ivanitskiy": 1, "Michael Starritt": 1, "M. Strube": 1, "Michal Swkedrowski": 1, "Michele Bevilacqua": 1, "Michihiro Yasunaga": 1, "Mihir Kale": 1, "Mike Cain": 1, "Mimee Xu": 1, "Mirac Suzgun": 1, "Mitch Walker": 1, "Monica Tiwari": 1, "Mohit Bansal": 1, "Moin Aminnaseri": 1, "Mor Geva": 1, "Mozhdeh Gheini": 1, "T. MukundVarma": 1, "Nanyun Peng": 1, "Nathan A. Chi": 1, "Nayeon Lee": 1, "Neta Gur-Ari Krakover": 1, "Nicholas Cameron": 1, "Nicholas Roberts": 1, "Nick Doiron": 1, "Nicole Martinez": 1, "Nikita Nangia": 1, "Niklas Deckers": 1, "N. Keskar": 1, "Niveditha Iyer": 1, "Noah Constant": 1, "Noah Fiedel": 1, "Nuan Wen": 1, "Oliver Zhang": 1, "Omar Agha": 1, "Omar Elbaghdadi": 1, "Omer Levy": 1, "Owain Evans": 1, "Pablo Antonio Moreno Casares": 1, "P. Doshi": 1, "P. Liang": 1, "Paul Vicol": 1, "Pegah Alipoormolabashi": 1, "Peiyuan Liao": 1, "Percy Liang": 1, "Peter Chang": 1, "P. Eckersley": 1, "Phu Mon Htut": 1, "Pi-Bei Hwang": 1, "P. Milkowski": 1, "P. Patil": 1, "Pouya Pezeshkpour": 1, "Priti Oli": 1, "Qiaozhu Mei": 1, "Qing Lyu": 1, "Qinlang Chen": 1, "Rabin Banjade": 1, "Rachel Etta Rudolph": 1, "Raefer Gabriel": 1, "Rahel Habacker": 1, "Ramon Risco": 1, "Raphael Milliere": 1, "Rhythm Garg": 1, "Richard Barnes": 1, "R. Saurous": 1, "Riku Arakawa": 1, "Robbe Raymaekers": 1, "Robert Frank": 1, "Rohan Sikand": 1, "Roman Novak": 1, "Roman Sitelew": 1, "Ronan Le Bras": 1, "Rosanne Liu": 1, "Rowan Jacobs": 1, "R. Salakhutdinov": 1, "Ryan Chi": 1, "Ryan Lee": 1, "Ryan Stovall": 1, "Rylan Yang": 1, "Sahib Singh": 1, "Saif M. Mohammad": 1, "Sajant Anand": 1, "Sam Dillavou": 1, "Sam Shleifer": 1, "Sam Wiseman": 1, "Samuel Gruetter": 1, "Samuel R. Bowman": 1, "S. Schoenholz": 1, "Sanghyun Han": 1, "Sanjeev Kwatra": 1, "Sarah A. Rous": 1, "Sarik Ghazarian": 1, "Sayan Ghosh": 1, "Sean Casey": 1, "Sebastian Bischoff": 1, "Sebastian Schuster": 1, "Sepideh Sadeghi": 1, "Shadi S. Hamdan": 1, "Sharon Zhou": 1, "Shashank Srivastava": 1, "Sherry Shi": 1, "Shikhar Singh": 1, "Shima Asaadi": 1, "S. Gu": 1, "Shubh Pachchigar": 1, "Shubham Toshniwal": 1, "Shyam Upadhyay": 1, "Shyamolima Debnath": 1, "Siamak Shakeri": 1, "Simon Thormeyer": 1, "S. Melzi": 1, "Siva Reddy": 1, "S. Makini": 1, "Soo-Hwan Lee": 1, "Spencer Torene": 1, "Sriharsha Hatwar": 1, "S. Dehaene": 1, "Stefan Divic": 1, "Stefano Ermon": 1, "Stephanie Lin": 1, "Stephen Prasad": 1, "Steven T Piantadosi": 1, "Stuart M. Shieber": 1, "Summer Misherghi": 1, "S. Kiritchenko": 1, "Swaroop Mishra": 1, "Tal Linzen": 1, "Tal Schuster": 1, "Tao Li": 1, "Tao Yu": 1, "Tariq Ali": 1, "Tatsunori Hashimoto": 1, "Te-Lin Wu": 1, "T. Desbordes": 1, "Theodore Rothschild": 1, "Thomas Phan": 1, "Tianle Wang": 1, "Tiberius Nkinyili": 1, "T. Kornev": 1, "T. Tunduny": 1, "Tobias Gerstenberg": 1, "T. Chang": 1, "Tushar Khot": 1, "Tyler Shultz": 1, "Uri Shaham": 1, "Vedant Misra": 1, "Vera Demberg": 1, "Victoria Nyamai": 1, "V. Ramasesh": 1, "Vinay Uday Prabhu": 1, "Vishakh Padmakumar": 1, "Vivek Srikumar": 1, "W. Fedus": 1, "W. Saunders": 1, "William Zhang": 1, "Wout Vossen": 1, "Xiang Ren": 1, "Xiaoyu Tong": 1, "Xinran Zhao": 1, "Xinyi Wu": 1, "Xudong Shen": 1, "Yadollah Yaghoobzadeh": 1, "Yair Lakretz": 1, "Yangqiu Song": 1, "Yasaman Bahri": 1, "Yejin Choi": 1, "Yichi Yang": 1, "Yiding Hao": 1, "Yifu Chen": 1, "Yu Hou": 2, "Yuntao Bai": 1, "Zachary Seid": 1, "Zhuoye Zhao": 1, "Zijian Wang": 1, "Zijie J. Wang": 1, "Zirui Wang": 1, "Ziyi Wu": 1, "Louis Castricato": 1}, "paper_topics": {}, "cluster_trends": {"\"Neural Network Interpretability and Learning\"": {"2024": 1, "2023": 5, "2022": 2}, "\"Transformer Models and Cognitive Mechanisms\"": {"2024": 1, "2023": 5, "2022": 2}, "\"Evaluating AI Compositional Reasoning\"": {"2023": 6, "2022": 4, "2024": 0}, "\"Verb Semantics in Multimodal Models\"": {"2023": 1, "2022": 2, "2024": 0}, "\"Neural Network Evaluation and Development\"": {"2023": 1, "2022": 2, "2024": 0}}, "index": "1"}, "27026c5895966fee8b95cfc1fa1c6ead": {"authorId": "2126494069", "start_year": 2022, "end_year": 2024, "papers": [{"title": "Riding the rails \u2013 different modes for RNA complex transport in axons", "authors": "Ofri Abraham, M. Fainzilber", "year": 2022, "url": "https://www.semanticscholar.org/paper/54874c5a1a54dd8927f7b0693820be5d8e1e0ade", "abstract": "Neurons are highly polarized cells with axons that innervate distant targets. The distance of subcellular compartments from the nucleus requires sophisticated transport mechanisms and local action of vital processes for proper function and rapid responses to local stimuli (Terenzio et al., 2017). This is partially achieved by transport of mRNAs to subcellular locations and regulation of local translation for axonal growth, branching, synaptic plasticity, and regeneration, among other needs. Axonally synthesized proteins support neuronal survival, and axonal development, maintenance, and growth (Rishal and Fainzilber, 2014; Dalla Costa et al., 2021). Thus, understanding the mechanisms that promote RNA transport to subcellular locations in neurons will contribute to the development of novel strategies to enhance axon regeneration and survival.", "topic": "\"Neuronal RNA Transport\""}], "analysis": "1. Mechanisms of RNA transport in neurons\n2. Local translation regulation in axonal growth and branching\n3. Role of axonally synthesized proteins in neuronal survival\n4. Synaptic plasticity and local protein synthesis in neurons", "author_name": "Ofri Abraham", "coauthors_histogram": {"M. Fainzilber": 1}, "paper_topics": {}, "cluster_trends": {"\"Neuronal RNA Transport\"": {"2022": 1, "2023": 0, "2024": 0}}, "index": "2"}, "72b94b2b911bf87f4ec9eb9a7f65bfec": {"authorId": "8365411", "start_year": 2020, "end_year": 2024, "papers": [{"title": "Number in NPI licensing", "authors": "Luka Crni\u010d", "year": 2022, "url": "https://www.semanticscholar.org/paper/ff426cc4d41f53a00c8d18c7d88244c821b92a8e", "abstract": null, "topic": "Unknown"}, {"title": "Ignorance, Introspection, and Epistemic Modals", "authors": "Luka Crni\u010d, Tue Trinh", "year": 2021, "url": "https://www.semanticscholar.org/paper/174650fbdc47ad6b4c8c88a2c126e8d24963ddbd", "abstract": "Embedded epistemic modals are infelicitous under desire predicates when they are anchored to the belief state of the attitude holder (see, esp., Anand & Hacquard 2013 ). We present two ways of deriving this observation from an inde- pendently motivated property of desire predicates ( Heim 1992 ; von Fintel 1999 ).", "topic": "\"Epistemic Modals and Desire Predicates\""}, {"title": "Remarks on two approaches to NPI licensing", "authors": "Luka Crni\u010d", "year": 2021, "url": "https://www.semanticscholar.org/paper/79c39cc042f1a96f9b92eb538b42a5ed47971826", "abstract": "In relation to the notion of informativity, two types of approaches to DPs headed by any (any-DPs) have been distinguished. At a \ufb01rst approximation, one approach takes an any-DP to be accompanied by a requirement that a clause containing it be more informative than all its relevant alternatives (cf. Kadmon & Landman 1993), while the other approach requires a clause containing it to be more informative than all its relevant alternatives that are true (cf. Chierchia 2013). The goal of this paper is to compare these approaches with respect to their predictions about the distribution of plural any-DPs in modal environments.", "topic": "\"Plural Any-DPs Analysis\""}, {"title": "NPIs, intervention, and collectivity", "authors": "Brian Buccola, Luka Crni\u010d", "year": 2021, "url": "https://www.semanticscholar.org/paper/c8903e75dd860e7c3b7f85fbd032181ae882cb05", "abstract": "Negative polarity items are subject to so-called intervention effects (Linebarger 1980, 1987). Specifically, they are unacceptable in the immediate scope of certain non-downward-entailing operators, even if they occur in the scope of a (higher) downward-entailing operator. By studying the behavior of any in configurations with collective predicates, we provide new empirical arguments that the descriptive condition concerning intervention must be stated with reference to the content of the clausal constituents in which NPIs may occur, and not merely with reference to operators c-commanding them. This is in line with recent arguments for environment-based formulations of NPI licensing conditions (e.g., Homer 2008, Gajewski 2011). We conclude by discussing how the condition fits in with some recent theories of intervention (especially Guerzoni 2006, Chierchia 2013).", "topic": "\"Syntax and Semantics\""}, {"title": "A Puzzle for the Scope Theories of Indefinites and Intensionality", "authors": "Luka Crni\u010d", "year": 2020, "url": "https://www.semanticscholar.org/paper/98e33d9287c9e6b3572c40c58d81966421daa890", "abstract": "ing away from the specific details of the different implementations, the derivation of islands-respecting LFs relies on two types of operations. The first involves scoping the DPs to the edge of the island, which itself moves above the relevant operator. The second involves appropriately shifting the meanings of certain expressions in the sentence \u2013 these shifts allow the meaning of the sister of the moved DP within the island to be effectively fed to the meaning of the sister of the moved island. An example of a simplified derivation is provided in (6): there are two movements, one above an appropriate operator within the island and the other involving the island. The moved DP \u2018a friend (of mine from Texas)\u2019 is sandwiched between elements (a higher-type expression \u2018P\u2019 and an abstractor over it) that allow the meaning of the sister of the island to apply directly to the meaning of the clause out of which the DP has moved.2 1Keshet\u2019s (2010) variant of the scope theory of intensionality avoids the problem of exceptional scope in (3). More involved examples can be constructed, say, the indefinite could be further embedded in an adjunct, in which scoping out of an island is required even on that variant (e.g., Schwarz 2012, Demirok 2019, Elliott 2020). 2Charlow assigns sentence (1) the structure in (i-a), while Demirok assigns it the structure in (ii-b) (see their papers for the definitions of the pertinent lexical items). Crucially for our purposes, in both systems, the indefinite scopes at the edge of the island in order for the required operations (ID, \u03b7 , >=), which are needed to semantically combine the meaning of embedded clause with the matrix one, to be able to apply and effect apparent wide scope of the indefinite. In the main text, we mimic the respective operations by a higher-type \u2018P\u2019 that is abstracted over. (i) a. [>= [[>= a friend of mine] [\u03bbx [\u03b7 [x died]]]]] [\u03bbq [if [\u03b7 q] [\u03b7 I inherit a fortune]]] b. [\u2203 [\u03bbp [a friend of mine [\u03bbx [[ID p] x died]]]]] [\u03bbq [if q, I inherit a fortune]]", "topic": "\"Syntax and Semantics\""}], "analysis": "1. Infelicity of embedded epistemic modals under desire predicates.\n2. Informativity requirements for DPs headed by \"any\" in modal environments.\n3. Intervention effects on negative polarity items.\n4. Environment-based formulations of NPI licensing conditions.\n5. Scope theory of intensionality and its implications for indefinite DPs.", "author_name": "Luka Crni\u010d", "coauthors_histogram": {"Tue Trinh": 1, "Brian Buccola": 1}, "paper_topics": {}, "cluster_trends": {"\"Epistemic Modals and Desire Predicates\"": {"2021": 1, "2020": 0, "2022": 0, "2023": 0, "2024": 0}, "\"Plural Any-DPs Analysis\"": {"2021": 1, "2020": 0, "2022": 0, "2023": 0, "2024": 0}, "\"Syntax and Semantics\"": {"2021": 1, "2020": 1, "2022": 0, "2023": 0, "2024": 0}}, "index": -1}, "fff24c0954aa50ff0e3dfaa2cf4b11d5": {"authorId": "103861390", "start_year": 2019, "end_year": 2023, "papers": [{"title": "Sj\u00f6rs, Ambj\u00f6rn: Historical Aspects of Standard Negation in Semitic. Leiden/Boston: Brill 2018. XVI, 478 S. 8\u00b0 = Studies in Semitic Languages and Linguistics 91. Hartbd. \u20ac 121,00. ISBN 978-90-04-34854-7.", "authors": "E. B. Siegal", "year": 2021, "url": "https://www.semanticscholar.org/paper/539f11c4e24bd9f8c9baf6de66b8c96e2255221b", "abstract": null, "topic": "Unknown"}, {"title": "A formal approach to reanalysis: The case of a negative counterfactual marker", "authors": "E. B. Siegal", "year": 2020, "url": "https://www.semanticscholar.org/paper/10a32d89c75908c1966f958fa71d2cf181c3de6d", "abstract": "This paper proposes a formal definition of reanalysis, while emphasizing the importance of the distinction between two different kinds of reanalysis: those in which the change is confined to the grammatical level, and those in which it is confined to the semantic level. After tracing the history of a negative counterfactual conditional marker in Hebrew and Aramaic which underwent both syntactic and semantic reanalyses, the paper assesses the concept of reanalysis with focus on the following questions: Is reanalysis a single, clearly-defined phenomenon, and if so, what is its nature? Is it merely a descriptive label for a certain observable state of affairs, or does it explain diachronic changes? Alternatively, perhaps it is a theoretical constraint , a theoretical requirement that linguistic change must be associated with specific environments where reanalysis can take place? A detailed analysis of the marker and its evolution yields the following broad hypothesis: Reanalysis of a linguistic form does not change the truth conditions of the proposition that contains it, regardless of whether the reanalysis is on the grammatical level or on the semantic level.", "topic": "Diachronic Linguistic Reanalysis"}, {"title": "Are Literary Languages Artificial? The Case of the Aramaic of the Zohar", "authors": "E. B. Siegal", "year": 2020, "url": "https://www.semanticscholar.org/paper/1ab79fd767f1003ac3bd7d89ca907b3ef1d17c54", "abstract": "\n Few studies have focused on the Aramaic of the Zohar, and to this day, only one of these presents a completed grammatical analysis. Scholars have dealt at large, however, with the question of whether the Aramaic of the Zohar is artificial or not. I briefly review the history of the literature around this question, then propose my own criteria to examine whether a language of a given text is indeed artificial. Finally, I put this methodology into practice, as I investigate the nature of Zoharic Aramaic by examining specific linguistic phenomena in the relevant corpus.", "topic": "Diachronic Linguistic Reanalysis"}, {"title": "Direct Causation: A New Approach to an Old Question", "authors": "R. Baglini, E. B. Siegal", "year": 2020, "url": "https://www.semanticscholar.org/paper/725ef8d72546c4d466fbe4cd6ce50d1715974a9b", "abstract": "Causative constructions come in lexical and periphrastic variants, exemplified in English by Sam killed Lee and Sam caused Lee to die. While use of the former, the lexical causative, entails the truth of the latter, an entailment in the other direction does not hold. The source of this asymmetry is commonly ascribed to the lexical causative having an additional prerequisite of \u201cdirect causation\", such that the causative relation holds between a contiguous cause and effect (Fodor 1970, Katz 1970). However, this explanation encounters both empirical and theoretical problems (Nelleman & van der Koot 2012). To explain the source of the directness inferences (as well as other longstanding puzzles), we propose a formal analysis based on the framework of Structural Equation Models (SEMs) (Pearl 2000) which provides the necessary background for licensing causal inferences. Specifically, we provide a formalization of a 'sufficient set of conditions' within a model and demonstrate its role in the selectional parameters of causative descriptions. We argue that \u201ccausal sufficiency\u201d is not a property of singular conditions, but rather sets of conditions, which are individually necessary but only sufficient when taken together (a view originally motivated in the philosophical literature by Mackie 1965). We further introduce the notion of a \u201ccompletion event\u201d of a sufficient set, which is critical to explain the particular inferential profile of lexical causatives. This working paper is available in University of Pennsylvania Working Papers in Linguistics: https://repository.upenn.edu/pwpl/vol26/iss1/4 Direct causation: A new approach to an old question Rebekah Baglini and Elitzur A. Bar-Asher Siegal\u2217", "topic": "Causative Constructions Analysis"}, {"title": "The Interrogative-Indefinite Puzzle in the Context of Biblical Hebrew", "authors": "E. B. Siegal", "year": 2020, "url": "https://www.semanticscholar.org/paper/806a83cc21b0ce780ba3c39b4596d8678a92a7ea", "abstract": "The biblical corpus features a number of verses in which interrogative pronouns appear in non-interrogative contexts. The same phenomenon is observed in many other languages and gives rise to the question known in the linguistic literature as \u201cthe interrogative-indefinite puzzle,\u201d namely, what is the natural connection between the interrogative and indefinite functions. This paper seeks to explore how this question should be examined in the context of the Biblical Hebrew data. It will be argued that a consideration of typological observations can yield important insights into this question. Subsequently, it proposes a formal semantic analysis of the indefinite pronouns in question and shows how the proposed approach can help explain their distribution.", "topic": "\"Semantics and Syntax in Linguistics\""}, {"title": "The NP-strategy for Expressing Reciprocity", "authors": "E. B. Siegal", "year": 2020, "url": "https://www.semanticscholar.org/paper/a0700205589506576c55003e14c3d0357c139022", "abstract": "This book provides a comprehensive treatment of the syntax and semantics of a single linguistic phenomenon \u2013 the NP-strategy for expressing reciprocity \u2013 in synchronic, diachronic, and typological perspectives. It challenges the assumption common in the typological, syntactic, and semantic literature, namely that so-called reciprocal constructions encode symmetric relations. Instead, they are analyzed as constructions encoding unspecified relations. In effect, it provides a new proposal for the truth-conditional semantics of these constructions. More broadly, this book introduces new ways of bringing together historical linguistics and formal semantics, demonstrating how, on the one hand, the inclusion of historical data concerning the sources of reciprocal constructions enriches their synchronic analysis; and how, on the other hand, an analysis of the syntax and the semantics of these constructions serves as a key for understanding their historical origins.", "topic": "\"Semantics and Syntax in Linguistics\""}, {"title": "Causation: From Metaphysics to Semantics and Back", "authors": "E. B. Siegal, N. Boneh", "year": 2020, "url": "https://www.semanticscholar.org/paper/d1742675eb2c054596e8cdfbf6851ef717248204", "abstract": null, "topic": "Unknown"}, {"title": "From negative cleft to external negator", "authors": "E. B. Siegal, K. Clercq", "year": 2019, "url": "https://www.semanticscholar.org/paper/a833686b6d3e93d538993b7993dbc2f432e0d64b", "abstract": "This chapter discusses the diachronic development of the Jewish Babylonian Aramaic negator l\u0101w, which developed from the univerbation between the sentential negator and agreement morphology in negative clefts. Whereas the semantics of negative clefts is retained in the new negator, their biclausal structure is replaced by a monoclausal one with l\u0101w merged in the clausal left periphery. The negator then takes propositional scope and expresses the meaning of external negation (\u2018it is not the case\u2019). Syntactically, it is merged in SpecFocP in the extended CP-domain, argued to host English negative DPs/PPs and wh-words. Finally, the chapter extends the analysis to Sicilian neca, opening up the route to consider the development of an external negator from a negative cleft as a path of change that has hitherto been left unexplored. This chapter also demonstrates how a similar semantic interpretation associated with two different syntactic structures can be a trigger for syntactic reanalysis.", "topic": "Diachronic Linguistic Reanalysis"}, {"title": "From diachronic observations to synchronic semantic analyses : the case of the NP-strategy for expressing reciprocity", "authors": "E. B. Siegal", "year": 2019, "url": "https://www.semanticscholar.org/paper/c0d451437b8ca671173870462a16fbf6cd8e9b15", "abstract": null, "topic": "Unknown"}, {"title": "Sufficient and Necessary Conditions for a Non-unified Analysis of Causation", "authors": "E. B. Siegal, N. Boneh", "year": 2019, "url": "https://www.semanticscholar.org/paper/e5610b3b794644a5d44f4d267660f3d5b04eb12d", "abstract": "In studying the nature of (D), one should examine whether a one, all-encompassing, causative meaning component underlying the diverse linguistic phenomena is a justifiable position (cf. e.g. McCawley 1968, Copley et al. 2015), or rather different ones should be distinguished for the various causative constructions. In other disciplines such as philosophy and cognitive psychology, a pluralistic notion of causation has been recently advocated. Within philosophy, see for instance, Hitchcock (2003), Hall (2004), Psillos (2009), who argue in favor of theories of causal pluralism, allowing the co-existence of different notions of causation; within cognitive studies Waldmann & Hagmayer (2013), inter alia, also indicate that people have a pluralistic conception of causation. As for linguistics, it has been proposed by Lauer (2010), Lauer & Nadathur (2017) and Martin (2017) that the semantic content of (D) is different in various constructions, tracing whether the main verb encodes a necessary or a sufficient condition. This paper expands on this latter line of thought by focusing on the types of conditions underlying the dependency relation encoded within three verbal constructions in Modern Hebrew, paying particular attention to whether these dependencies are asserted and/or presupposed. The constructions are (i) those featuring an overt causative verb (e.g. garam \u2018cause/make\u2019);1 (ii) those featuring a lexical causative verb encoding a change of state (e.g. patax \u2018open\u2019); and (iii) those featuring a lexical causative verb encoding a caused activity, absent from English (e.g. hirkid \u2018dance.CAUSE\u2019).2 The structure of the paper is as follows: in \u00a72 we lay out the distinction between sufficient and necessary conditions within the framework of Lewis's counterfactual analysis of causation. In \u00a73-4 we compare the different causative constructions in Hebrew considering the way they may describe various relations between events. The picture that emerges from this comparison is that the (D) of the overt causative verb requires only that (c) is a necessary condition for the occurrence of the (e), whereas lexical causative verbs encode other types of dependencies and combinations thereof. Furthermore, the constructions also differ as to the way they pattern in negative statements, a fact that supports the claim that the semantics of the lexical causative verbs involves various presuppositions. Based on this", "topic": "Causative Constructions Analysis"}], "analysis": "1. **Syntactic and semantic reanalysis in Hebrew and Aramaic**: Examining the evolution of a negative counterfactual conditional marker and its implications for linguistic change.\n2. **Artificiality of Zoharic Aramaic**: Investigating whether the language of the Zohar is artificial through specific linguistic phenomena.\n3. **Interrogative-indefinite pronouns in Biblical Hebrew**: Exploring the connection between interrogative and indefinite functions in non-interrogative contexts.\n4. **Diachronic development of the Jewish Babylonian Aramaic negator l\u0101w**: Tracing the syntactic and semantic changes from negative clefts to external negation.", "author_name": "E. B. Siegal", "coauthors_histogram": {"R. Baglini": 1, "N. Boneh": 2, "K. Clercq": 1}, "paper_topics": {}, "cluster_trends": {"Diachronic Linguistic Reanalysis": {"2020": 2, "2019": 1, "2021": 0, "2022": 0, "2023": 0}, "Causative Constructions Analysis": {"2020": 1, "2019": 1, "2021": 0, "2022": 0, "2023": 0}, "\"Semantics and Syntax in Linguistics\"": {"2020": 2, "2019": 0, "2021": 0, "2022": 0, "2023": 0}}, "index": "1"}, "76d06b96ca2cbaa9fea175863f237924": {"authorId": "2067118898", "start_year": 2019, "end_year": 2024, "papers": [{"title": "How Well Do Large Language Models Perform on Faux Pas Tests?", "authors": "Natalie Shapira, Guy Zwirn, Yoav Goldberg", "year": 2023, "url": "https://www.semanticscholar.org/paper/016c8d91f8a102111dc5eb76ab4ce433b9e2ec53", "abstract": null, "topic": "Unknown"}, {"title": "Evaluating Humorous Response Generation to Playful Shopping Requests", "authors": "Natalie Shapira, Oren Kalinsky, Alex Libov, Chen Shani, Sofia Tolmach", "year": 2023, "url": "https://www.semanticscholar.org/paper/76589f5c5339a116aa604f67ab953c1662fc2460", "abstract": null, "topic": "Unknown"}, {"title": "Clever Hans or Neural Theory of Mind? Stress Testing Social Reasoning in Large Language Models", "authors": "Natalie Shapira, Mosh Levy, S. Alavi, Xuhui Zhou, Yejin Choi, Yoav Goldberg, Maarten Sap, Vered Shwartz", "year": 2023, "url": "https://www.semanticscholar.org/paper/ddcd2bcc809bd0c2755a4a9487473d61ac327c50", "abstract": "The escalating debate on AI\u2019s capabilities warrants developing reliable metrics to assess machine \u201cintelligence.\u201d Recently, many anecdotal examples were used to suggest that newer Large Language Models (LLMs) like ChatGPT and GPT-4 exhibit Neural Theory-of-Mind (N-ToM); however, prior work reached conflicting conclusions regarding those abilities. We investigate the extent of LLMs\u2019 N-ToM through an extensive evaluation of 6 tasks and find that while LLMs exhibit certain N-ToM abilities, this behavior is far from being robust. We further examine the factors impacting performance on N-ToM tasks and discover that LLMs struggle with adversarial examples, indicating reliance on shallow heuristics rather than robust ToM abilities. We caution against drawing conclusions from anecdotal examples, limited benchmark testing, and using human-designed psychological tests to evaluate models.", "topic": "\"Language Models in Psychotherapy\""}, {"title": "Measuring Linguistic Synchrony in Psychotherapy", "authors": "Natalie Shapira, Dana Atzil-Slonim, Rivka Tuval Mashiach, Ori Shapira", "year": 2022, "url": "https://www.semanticscholar.org/paper/c0adbe9186b44b5320dc7dfd8dd84c3e28a479f6", "abstract": "We study the phenomenon of linguistic synchrony between clients and therapists in a psychotherapy process. Linguistic Synchrony (LS) can be viewed as any observed interdependence or association between more than one person?s linguistic behavior. Accordingly, we establish LS as a methodological task. We suggest a LS function that applies a linguistic similarity measure based on the Jensen-Shannon distance across the observed part-of-speech tag distributions (JSDuPos) of the speakers in different time frames. We perform a study over a unique corpus of 872 transcribed sessions, covering 68 clients and 59 therapists. After establishing the presence of client-therapist LS, we verify its association with therapeutic alliance and treatment outcome (measured using WAI and ORS), and additionally analyse the behavior of JSDuPos throughout treatment.Results indicate that (1) higher linguistic similarity at the session level associates with higher therapeutic alliance as reported by the client and therapist at the end of the session, (2) higher linguistic similarity at the session level associates with higher level of treatment outcome as reported by the client at the beginnings of the next sessions, (3) there is a significant linear increase in linguistic similarity throughout treatment, (4) surprisingly, higher LS associates with lower treatment outcome. Finally, we demonstrate how the LS function can be used to interpret and explore the mechanism for synchrony.", "topic": "\"Linguistic Synchrony in Psychotherapy\""}, {"title": "Using topic models to identify clients' functioning levels and alliance ruptures in psychotherapy.", "authors": "Dana Atzil-Slonim, Daniel Juravski, E. Bar-Kalifa, E. Gilboa-Schechtman, Rivka Tuval-Mashiach, Natalie Shapira, Yoav Goldberg", "year": 2021, "url": "https://www.semanticscholar.org/paper/6087a4ae4413fbb24de6f9ceb072381a7e72ae09", "abstract": "Computerized natural language processing techniques can analyze psychotherapy sessions as texts, thus generating information about the therapy process and outcome and supporting the scaling-up of psychotherapy research. We used topic modeling to identify topics discussed in psychotherapy sessions and explored (a) which topics best identified clients' functioning and alliance ruptures and (b) whether changes in these topics were associated with changes in outcome. Transcripts of 873 sessions from 58 clients treated by 52 therapists were analyzed. Before each session, clients self-reported functioning and symptom level. After each session, therapists reported the extent of alliance rupture. Latent Dirichlet allocation was used to extract latent topics from psychotherapy textual data. Then a sparse multinomial logistic regression model was used to predict which topics best identified clients' functioning levels and the occurrence of alliance ruptures in psychotherapy sessions. Finally, we used multilevel growth models to explore the associations between changes in topics and changes in outcome. Session-based processing yielded a list of semantic topics. The model identified the labels above chance (65% to 75% accuracy). Change trajectories in topics were associated with change trajectories in outcome. The results suggest that topic models can exploit rich linguistic data within sessions to identify psychotherapy process and outcomes. (PsycInfo Database Record (c) 2021 APA, all rights reserved).", "topic": "\"Language Models in Psychotherapy\""}, {"title": "Hebrew Psychological Lexicons", "authors": "Natalie Shapira, Dana Atzil-Slonim, Daniel Juravski, Moran Baruch, Dana Stolowicz-Melman, Adar Paz, Tal Alfi-Yogev, R. Azoulay, Adi Singer, Maayan Revivo, Chen Dahbash, Limor Dayan, Tamar Naim, Lidar Gez, Boaz Yanai, Adva Maman, A. Nadaf, Elinor Sarfati, Amna Baloum, Tal Naor, Ephraim Mosenkis, Badreya Sarsour, Jany Gelfand Morgenshteyn, Yarden Elias, Liat Braun, Moria Rubin, M. Kenigsbuch, Noa Bergwerk, Noam Yosef, Sivan Peled, Coral Avigdor, Rahav Obercyger, R. Mann, Tomer Alper, Inbal Beka, Ori Shapira, Yoav Goldberg", "year": 2021, "url": "https://www.semanticscholar.org/paper/899fa470eb47afc75347484c1feedc1947cb52c5", "abstract": "We introduce a large set of Hebrew lexicons pertaining to psychological aspects. These lexicons are useful for various psychology applications such as detecting emotional state, well being, relationship quality in conversation, identifying topics (e.g., family, work) and many more. We discuss the challenges in creating and validating lexicons in a new language, and highlight our methodological considerations in the data-driven lexicon construction process. Most of the lexicons are publicly available, which will facilitate further research on Hebrew clinical psychology text analysis. The lexicons were developed through data driven means, and verified by domain experts, clinical psychologists and psychology students, in a process of reconciliation with three judges. Development and verification relied on a dataset of a total of 872 psychotherapy session transcripts. We describe the construction process of each collection, the final resource and initial results of research studies employing this resource.", "topic": "\"Hebrew Lexicons for Psychological Analysis\""}, {"title": "Automatic Identification of Ruptures in Transcribed Psychotherapy Sessions", "authors": "Adam Tsakalidis, Dana Atzil-Slonim, Asaf Polakovski, Natalie Shapira, Rivka Tuval-Mashiach, M. Liakata", "year": 2021, "url": "https://www.semanticscholar.org/paper/9a58d14ca41038455295ed245d8c1d27fd378c9b", "abstract": "We present the first work on automatically capturing alliance rupture in transcribed therapy sessions, trained on the text and self-reported rupture scores from both therapists and clients. Our NLP baseline outperforms a strong majority baseline by a large margin and captures client reported ruptures unidentified by therapists in 40% of such cases.", "topic": "\"Language Models in Psychotherapy\""}, {"title": "Using computerized text analysis to examine associations between linguistic features and clients' distress during psychotherapy.", "authors": "Natalie Shapira, Gal Lazarus, Yoav Goldberg, E. Gilboa-Schechtman, Rivka Tuval-Mashiach, Daniel Juravski, Dana Atzil-Slonim", "year": 2020, "url": "https://www.semanticscholar.org/paper/c823a7e8d0989868b39698e6cdf6be53c56a8b40", "abstract": "Raw linguistic data within psychotherapy sessions may provide important information about clients' progress and well-being. In the current study, computerized text analytic techniques were applied to examine whether linguistic features were associated with clients' experiences of distress within and between clients and whether changes in linguistic features were associated with changes in treatment outcome. Transcripts of 729 psychotherapy sessions from 58 clients treated by 52 therapists were analyzed. Prior to each session, clients reported their distress level. Linguistic features were extracted automatically by using natural language parser for first-person singular identification and using positive and negative emotion words lexicon. The association between linguistic features and levels of distress was examined using multilevel models. At the within-client level, fewer first-person singular words, fewer negative emotional words and more positive emotional words were associated with lower distress in the same session; and fewer negative emotion words were associated with lower next session distress (rather small f2 effect sizes = 0.011 < f2 < 0.022). At the between-client level, only first session use of positive emotion words was associated with first session distress (\u03b7p2 effect size = 0.08). A drop in the use of first-person singular words was associated with improved outcome from pre- to posttreatment (small \u03b7p2 effect size = 0.05). The findings provide preliminary support for the association between clients' linguistic features and their fluctuating experience of distress. They point to the potential value of computerized linguistic measures to track therapeutic outcomes. (PsycInfo Database Record (c) 2020 APA, all rights reserved).", "topic": "\"Language Models in Psychotherapy\""}], "analysis": "1. Linguistic synchrony between clients and therapists in psychotherapy.\n2. Association of linguistic synchrony with therapeutic alliance and treatment outcomes.\n3. Topic modeling of psychotherapy session transcripts to identify client functioning and alliance ruptures.\n4. Development and validation of Hebrew lexicons for psychological text analysis.\n5. Automatic detection of alliance ruptures in psychotherapy sessions using NLP techniques.\n6. Association between linguistic features and client distress in psychotherapy sessions.\n7. Use of computerized text analytic techniques to track therapeutic outcomes.", "author_name": "Natalie Shapira", "coauthors_histogram": {"Guy Zwirn": 1, "Yoav Goldberg": 5, "Oren Kalinsky": 1, "Alex Libov": 1, "Chen Shani": 1, "Sofia Tolmach": 1, "Mosh Levy": 1, "S. Alavi": 1, "Xuhui Zhou": 1, "Yejin Choi": 1, "Maarten Sap": 1, "Vered Shwartz": 1, "Dana Atzil-Slonim": 5, "Rivka Tuval Mashiach": 1, "Ori Shapira": 2, "Daniel Juravski": 3, "E. Bar-Kalifa": 1, "E. Gilboa-Schechtman": 2, "Rivka Tuval-Mashiach": 3, "Moran Baruch": 1, "Dana Stolowicz-Melman": 1, "Adar Paz": 1, "Tal Alfi-Yogev": 1, "R. Azoulay": 1, "Adi Singer": 1, "Maayan Revivo": 1, "Chen Dahbash": 1, "Limor Dayan": 1, "Tamar Naim": 1, "Lidar Gez": 1, "Boaz Yanai": 1, "Adva Maman": 1, "A. Nadaf": 1, "Elinor Sarfati": 1, "Amna Baloum": 1, "Tal Naor": 1, "Ephraim Mosenkis": 1, "Badreya Sarsour": 1, "Jany Gelfand Morgenshteyn": 1, "Yarden Elias": 1, "Liat Braun": 1, "Moria Rubin": 1, "M. Kenigsbuch": 1, "Noa Bergwerk": 1, "Noam Yosef": 1, "Sivan Peled": 1, "Coral Avigdor": 1, "Rahav Obercyger": 1, "R. Mann": 1, "Tomer Alper": 1, "Inbal Beka": 1, "Adam Tsakalidis": 1, "Asaf Polakovski": 1, "M. Liakata": 1, "Gal Lazarus": 1}, "paper_topics": {}, "cluster_trends": {"\"Language Models in Psychotherapy\"": {"2023": 1, "2021": 2, "2020": 1, "2019": 0, "2022": 0, "2024": 0}, "\"Linguistic Synchrony in Psychotherapy\"": {"2022": 1, "2019": 0, "2020": 0, "2021": 0, "2023": 0, "2024": 0}, "\"Hebrew Lexicons for Psychological Analysis\"": {"2021": 1, "2019": 0, "2020": 0, "2022": 0, "2023": 0, "2024": 0}}, "index": "2"}, "ad3c5387c1025f78b6aadea1cefc158d": {"authorId": "41016275", "start_year": 2022, "end_year": 2024, "papers": [{"title": "A Chain-of-Thought Is as Strong as Its Weakest Link: A Benchmark for Verifiers of Reasoning Chains", "authors": "Alon Jacovi, Yonatan Bitton, Bernd Bohnet, Jonathan Herzig, Or Honovich, Michael Tseng, Michael Collins, Roee Aharoni, Mor Geva", "year": 2024, "url": "https://www.semanticscholar.org/paper/0f51d47871d99cda3e9eaf4ae1c9c7025ae76325", "abstract": "Prompting language models to provide step-by-step answers (e.g.,\"Chain-of-Thought\") is the prominent approach for complex reasoning tasks, where more accurate reasoning chains typically improve downstream task performance. Recent literature discusses automatic methods to verify reasoning to evaluate and improve their correctness. However, no fine-grained step-level datasets are available to enable thorough evaluation of such verification methods, hindering progress in this direction. We introduce REVEAL: Reasoning Verification Evaluation, a dataset to benchmark automatic verifiers of complex Chain-of-Thought reasoning in open-domain question-answering settings. REVEAL includes comprehensive labels for the relevance, attribution to evidence passages, and logical correctness of each reasoning step in a language model's answer, across a variety of datasets and state-of-the-art language models. Evaluation on REVEAL shows that verifiers struggle at verifying reasoning chains - in particular, verifying logical correctness and detecting contradictions. Available at https://reveal-dataset.github.io/ .", "topic": "Large Language Models Research"}, {"title": "Can Few-shot Work in Long-Context? Recycling the Context to Generate Demonstrations", "authors": "Arie Cattan, Alon Jacovi, Alex Fabrikant, Jonathan Herzig, Roee Aharoni, Hannah Rashkin, Dror Marcus, Avinatan Hassidim, Yossi Matias, Idan Szpektor, Avi Caciularu", "year": 2024, "url": "https://www.semanticscholar.org/paper/48a00907faea2ccb56ee45495aaef8953ccf6535", "abstract": "Despite recent advancements in Large Language Models (LLMs), their performance on tasks involving long contexts remains sub-optimal. In-Context Learning (ICL) with few-shot examples may be an appealing solution to enhance LLM performance in this scenario; However, naively adding ICL examples with long context introduces challenges, including substantial token overhead added for each few-shot example and context mismatch between the demonstrations and the target query. In this work, we propose to automatically generate few-shot examples for long context QA tasks by recycling contexts. Specifically, given a long input context (1-3k tokens) and a query, we generate additional query-output pairs from the given context as few-shot examples, while introducing the context only once. This ensures that the demonstrations are leveraging the same context as the target query while only adding a small number of tokens to the prompt. We further enhance each demonstration by instructing the model to explicitly identify the relevant paragraphs before the answer, which improves performance while providing fine-grained attribution to the answer source. We apply our method on multiple LLMs and obtain substantial improvements (+23\\% on average across models) on various QA datasets with long context, especially when the answer lies within the middle of the context. Surprisingly, despite introducing only single-hop ICL examples, LLMs also successfully generalize to multi-hop long-context QA using our approach.", "topic": "Large Language Models Research"}, {"title": "TACT: Advancing Complex Aggregative Reasoning with Information Extraction Tools", "authors": "Avi Caciularu, Alon Jacovi, Eyal Ben-David, S. Goldshtein, Tal Schuster, Jonathan Herzig, G. Elidan, Amir Globerson", "year": 2024, "url": "https://www.semanticscholar.org/paper/72c5a61c034c3cce735d6280c43e084091c0edc3", "abstract": "Large Language Models (LLMs) often do not perform well on queries that require the aggregation of information across texts. To better evaluate this setting and facilitate modeling efforts, we introduce TACT - Text And Calculations through Tables, a dataset crafted to evaluate LLMs' reasoning and computational abilities using complex instructions. TACT contains challenging instructions that demand stitching information scattered across one or more texts, and performing complex integration on this information to generate the answer. We construct this dataset by leveraging an existing dataset of texts and their associated tables. For each such tables, we formulate new queries, and gather their respective answers. We demonstrate that all contemporary LLMs perform poorly on this dataset, achieving an accuracy below 38\\%. To pinpoint the difficulties and thoroughly dissect the problem, we analyze model performance across three components: table-generation, Pandas command-generation, and execution. Unexpectedly, we discover that each component presents substantial challenges for current LLMs. These insights lead us to propose a focused modeling framework, which we refer to as IE as a tool. Specifically, we propose to add\"tools\"for each of the above steps, and implement each such tool with few-shot prompting. This approach shows an improvement over existing prompting techniques, offering a promising direction for enhancing model capabilities in these tasks.", "topic": "Large Language Models Research"}, {"title": "Is It Really Long Context if All You Need Is Retrieval? Towards Genuinely Difficult Long Context NLP", "authors": "Omer Goldman, Alon Jacovi, Aviv Slobodkin, Aviya Maimon, Ido Dagan, Reut Tsarfaty", "year": 2024, "url": "https://www.semanticscholar.org/paper/ca862ab6bdfc5eee2f3764e028f9c6cb13683f21", "abstract": "Improvements in language models' capabilities have pushed their applications towards longer contexts, making long-context evaluation and development an active research area. However, many disparate use-cases are grouped together under the umbrella term of\"long-context\", defined simply by the total length of the model's input, including - for example - Needle-in-a-Haystack tasks, book summarization, and information aggregation. Given their varied difficulty, in this position paper we argue that conflating different tasks by their context length is unproductive. As a community, we require a more precise vocabulary to understand what makes long-context tasks similar or different. We propose to unpack the taxonomy of long-context based on the properties that make them more difficult with longer contexts. We propose two orthogonal axes of difficulty: (I) Diffusion: How hard is it to find the necessary information in the context? (II) Scope: How much necessary information is there to find? We survey the literature on long-context, provide justification for this taxonomy as an informative descriptor, and situate the literature with respect to it. We conclude that the most difficult and interesting settings, whose necessary information is very long and highly diffused within the input, is severely under-explored. By using a descriptive vocabulary and discussing the relevant properties of difficulty in long-context, we can implement more informed research in this area. We call for a careful design of tasks and benchmarks with distinctly long context, taking into account the characteristics that make it qualitatively different from shorter context.", "topic": "Large Language Models Research"}, {"title": "Neighboring Words Affect Human Interpretation of Saliency Explanations", "authors": "Alon Jacovi, Hendrik Schuff, Heike Adel, Ngoc Thang Vu, Yoav Goldberg", "year": 2023, "url": "https://www.semanticscholar.org/paper/1a149dde95fba12d349d7bf9539e6d1fb59fc134", "abstract": "Word-level saliency explanations (\"heat maps over words\") are often used to communicate feature-attribution in text-based models. Recent studies found that superficial factors such as word length can distort human interpretation of the communicated saliency scores. We conduct a user study to investigate how the marking of a word's neighboring words affect the explainee's perception of the word's importance in the context of a saliency explanation. We find that neighboring words have significant effects on the word's importance rating. Concretely, we identify that the influence changes based on neighboring direction (left vs. right) and a-priori linguistic and computational measures of phrases and collocations (vs. unrelated neighboring words). Our results question whether text-based saliency explanations should be continued to be communicated at word level, and inform future research on alternative saliency explanation methods.", "topic": "Large Language Models Research"}, {"title": "Unpacking Human-AI Interaction in Safety-Critical Industries: A Systematic Literature Review", "authors": "T. A. Bach, Jenny K. Kristiansen, Aleksandar Babic, Alon Jacovi", "year": 2023, "url": "https://www.semanticscholar.org/paper/1fbf67fd3fb8944cf7de5c50f3694281267fa62c", "abstract": "Ensuring quality human-AI interaction (HAII) in safety-critical industries is essential. Failure to do so can lead to catastrophic and deadly consequences. Despite this urgency, what little research there is on HAII is fragmented and inconsistent. We present here a survey of that literature and recommendations for research best practices that will improve the field. We divided our investigation into the following research areas: (1) terms used to describe HAII, (2) primary roles of AI-enabled systems, (3) factors that influence HAII, and (4) how HAII is measured. Additionally, we described the capabilities and maturity of the AI-enabled systems used in safety-critical industries discussed in these articles. We found that no single term is used across the literature to describe HAII and some terms have multiple meanings. According to our literature, five factors influence HAII: user characteristics and background (e.g., user personality, perceptions), AI interface and features (e.g., interactive UI design), AI output (e.g., accuracy, actionable recommendations), explainability and interpretability (e.g., level of detail, user understanding), and usage of AI (e.g., heterogeneity of environments and user needs). HAII is most commonly measured with user-related subjective metrics (e.g., user perception, trust, and attitudes), and AI-assisted decision-making is the most common primary role of AI-enabled systems. Based on this review, we conclude that there are substantial research gaps in HAII. Researchers and developers need to codify HAII terminology, involve users throughout the AI lifecycle (especially during development), and tailor HAII in safety-critical industries to the users and environments.", "topic": "\"Human-AI Interaction and Explainable AI\""}, {"title": "A Comprehensive Evaluation of Tool-Assisted Generation Strategies", "authors": "Alon Jacovi, Avi Caciularu, Jonathan Herzig, Roee Aharoni, Bernd Bohnet, Mor Geva", "year": 2023, "url": "https://www.semanticscholar.org/paper/bb9f3c9c8bcf78814740559ae6fb82ca1012ea90", "abstract": "A growing area of research investigates augmenting language models with tools (e.g., search engines, calculators) to overcome their shortcomings (e.g., missing or incorrect knowledge, incorrect logical inferences). Various few-shot tool-usage strategies have been proposed. However, there is no systematic and fair comparison across different strategies, or between these strategies and strong baselines that do not leverage tools. We conduct an extensive empirical analysis, finding that (1) across various datasets, example difficulty levels, and models, strong no-tool baselines are competitive to tool-assisted strategies, implying that effectively using tools with in-context demonstrations is a difficult unsolved problem; (2) for knowledge-retrieval tasks, strategies that *refine* incorrect outputs with tools outperform strategies that retrieve relevant information *ahead of* or *during generation*; (3) tool-assisted strategies are expensive in the number of tokens they require to work -- incurring additional costs by orders of magnitude -- which does not translate into significant improvement in performance. Overall, our findings suggest that few-shot tool integration is still an open challenge, emphasizing the need for comprehensive evaluations of future strategies to accurately assess their *benefits* and *costs*.", "topic": "\"Language Models and Data Integrity\""}, {"title": "Trends in Explainable AI (XAI) Literature", "authors": "Alon Jacovi", "year": 2023, "url": "https://www.semanticscholar.org/paper/cb778d6939b92fcbaf8f184a5a1db528de8b8031", "abstract": "TheXAIliteratureisdecentralized,bothinterminologyandinpublicationvenues,butrecentyearssawthecommunityconvergearound keywords that make it possible to more reliably discover papers automatically. We use keyword search using the SemanticScholar API and manual curation to collect a well-formatted and reasonably comprehensive set of 5199 XAI papers, available at https: //github.com/alonjacovi/XAI-Scholar. We use this collection to clarify and visualize trends about the size and scope of the literature, citation trends, cross-field trends, and collaboration trends. Overall, XAI is becoming increasingly multidisciplinary, with relative growth in papers belonging to increasingly diverse (non-CS) scientific fields, increasing cross-field collaborative authorship, increasing cross-field citation activity. The collection can additionally be used as a paper discovery engine, by retrieving XAI literature which is cited according to specific constraints (for example, papers that are influential outside of their field, or influential to non-XAI research).", "topic": "\"Human-AI Interaction and Explainable AI\""}, {"title": "Stop Uploading Test Data in Plain Text: Practical Strategies for Mitigating Data Contamination by Evaluation Benchmarks", "authors": "Alon Jacovi, Avi Caciularu, Omer Goldman, Yoav Goldberg", "year": 2023, "url": "https://www.semanticscholar.org/paper/fc30093e9f55ae1c0a1d2c4c4e5341998adede66", "abstract": "Data contamination has become prevalent and challenging with the rise of models pretrained on large automatically-crawled corpora. For closed models, the training data becomes a trade secret, and even for open models, it is not trivial to detect contamination. Strategies such as leaderboards with hidden answers, or using test data which is guaranteed to be unseen, are expensive and become fragile with time. Assuming that all relevant actors value clean test data and will cooperate to mitigate data contamination, what can be done? We propose three strategies that can make a difference: (1) Test data made public should be encrypted with a public key and licensed to disallow derivative distribution; (2) demand training exclusion controls from closed API holders, and protect your test data by refusing to evaluate without them; (3) avoid data which appears with its solution on the internet, and release the web-page context of internet-derived data along with the data. These strategies are practical and can be effective in preventing data contamination.", "topic": "\"Language Models and Data Integrity\""}, {"title": "Human Interpretation of Saliency-based Explanation Over Text", "authors": "Hendrik Schuff, Alon Jacovi, Heike Adel, Yoav Goldberg, Ngoc Thang Vu", "year": 2022, "url": "https://www.semanticscholar.org/paper/56528ea81b30610f7cff12987a63ea7b5bae5b6f", "abstract": "While a lot of research in explainable AI focuses on producing effective explanations, less work is devoted to the question of how people understand and interpret the explanation. In this work, we focus on this question through a study of saliency-based explanations over textual data. Feature-attribution explanations of text models aim to communicate which parts of the input text were more influential than others towards the model decision. Many current explanation methods, such as gradient-based or Shapley value-based methods, provide measures of importance which are well-understood mathematically. But how does a person receiving the explanation (the explainee) comprehend it? And does their understanding match what the explanation attempted to communicate? We empirically investigate the effect of various factors of the input, the feature-attribution explanation, and visualization procedure, on laypeople\u2019s interpretation of the explanation. We query crowdworkers for their interpretation on tasks in English and German, and fit a GAMM model to their responses considering the factors of interest. We find that people often mis-interpret the explanations: superficial and unrelated factors, such as word length, influence the explainees\u2019 importance assignment despite the explanation communicating importance directly. We then show that some of this distortion can be attenuated: we propose a method to adjust saliencies based on model estimates of over- and under-perception, and explore bar charts as an alternative to heatmap saliency visualization. We find that both approaches can attenuate the distorting effect of specific factors, leading to better-calibrated understanding of the explanation.", "topic": "\"Human-AI Interaction and Explainable AI\""}, {"title": "Diagnosing AI Explanation Methods with Folk Concepts of Behavior", "authors": "Alon Jacovi, Jasmijn Bastings, Sebastian Gehrmann, Yoav Goldberg, Katja Filippova", "year": 2022, "url": "https://www.semanticscholar.org/paper/9ecd12e963e7cfd2a59f1a4b7ae315193f025882", "abstract": "We investigate a formalism for the conditions of a successful explanation of AI. We consider \u201csuccess\u201d to depend not only on what information the explanation contains, but also on what information the human explainee understands from it. Theory of mind literature discusses the folk concepts that humans use to understand and generalize behavior. We posit that folk concepts of behavior provide us with a \u201clanguage\u201d that humans understand behavior with. We use these folk concepts as a framework of social attribution by the human explainee\u2014the information constructs that humans are likely to comprehend from explanations\u2014by introducing a blueprint for an explanatory narrative that explains AI behavior with these constructs. We then demonstrate that many XAI methods today can be mapped to folk concepts of behavior in a qualitative evaluation. This allows us to uncover their failure modes that prevent current methods from explaining successfully\u2014i.e., the information constructs that are missing for any given XAI method, and whose inclusion can decrease the likelihood of misunderstanding AI behavior.", "topic": "\"Human-AI Interaction and Explainable AI\""}], "analysis": "1. **Verification of reasoning chains in language models**\n2. **Few-shot learning for long-context question answering**\n3. **Aggregation of information across texts using language models**\n4. **Taxonomy and evaluation of long-context natural language processing tasks**\n5. **Tool-assisted generation strategies in language models**\n6. **Strategies to mitigate data contamination in evaluation benchmarks**", "author_name": "Alon Jacovi", "coauthors_histogram": {"Yonatan Bitton": 1, "Bernd Bohnet": 2, "Jonathan Herzig": 4, "Or Honovich": 1, "Michael Tseng": 1, "Michael Collins": 1, "Roee Aharoni": 3, "Mor Geva": 2, "Arie Cattan": 1, "Alex Fabrikant": 1, "Hannah Rashkin": 1, "Dror Marcus": 1, "Avinatan Hassidim": 1, "Yossi Matias": 1, "Idan Szpektor": 1, "Avi Caciularu": 4, "Eyal Ben-David": 1, "S. Goldshtein": 1, "Tal Schuster": 1, "G. Elidan": 1, "Amir Globerson": 1, "Omer Goldman": 2, "Aviv Slobodkin": 1, "Aviya Maimon": 1, "Ido Dagan": 1, "Reut Tsarfaty": 1, "Hendrik Schuff": 2, "Heike Adel": 2, "Ngoc Thang Vu": 2, "Yoav Goldberg": 4, "T. A. Bach": 1, "Jenny K. Kristiansen": 1, "Aleksandar Babic": 1, "Jasmijn Bastings": 1, "Sebastian Gehrmann": 1, "Katja Filippova": 1}, "paper_topics": {}, "cluster_trends": {"Large Language Models Research": {"2024": 4, "2023": 1, "2022": 0}, "\"Human-AI Interaction and Explainable AI\"": {"2023": 2, "2022": 2, "2024": 0}, "\"Language Models and Data Integrity\"": {"2023": 2, "2022": 0, "2024": 0}}, "index": -1}, "0047efaf6753b374d2712d10e5498211": {"authorId": "51914081", "start_year": 2021, "end_year": 2024, "papers": [{"title": "From temporal to concessive meanings: a semantic analysis of 'still'", "authors": "Aynat Rubinstein, E. Herburger", "year": 2024, "url": "https://www.semanticscholar.org/paper/50b9631db880abcb730bfaf1d3aced44ae2bd0cf", "abstract": "We develop a new proposal about the historical connection between the durative and concessive readings of English still and Hebrew \u0295adain, a connection that shows striking parallels in the two languages. Building on a corpus study of Hebrew (Rubinstein forthcoming), we argue that durative 'still' precedes the concessive 'still' and that the latter first arises in bridging contexts (and earlier than previously thought). In contrast to previous literature, our proposal places the temporal-to-concessive development squarely in the semantics. We argue that concessive 'still' emerges when an originally durative 'still' gets \"infected\" with a concessive meaning that is expressed explicitly in the rest of the sentence.", "topic": "\"Semantic Evolution of 'Still'\""}, {"title": "Eventive modal projection: the case of Spanish subjunctive relative clauses", "authors": "Luis Alonso-Ovalle, Paula Men\u00e9ndez-Benito, Aynat Rubinstein", "year": 2024, "url": "https://www.semanticscholar.org/paper/f71406aea5370040c50fe0f489ac95ce76d98d31", "abstract": null, "topic": "Unknown"}, {"title": "Machine classification of modal meanings: An empirical study and some consequences", "authors": "Aynat Rubinstein, Valentina Pyatkin, Shoval Sadde, Reut Tsarfaty, P. Portner", "year": 2022, "url": "https://www.semanticscholar.org/paper/ae79cf3f7de6db24a6e9c3c4464f30a056f4e4a4", "abstract": ",", "topic": "\"Modal Classification in NLP\""}, {"title": "NLP in the DH pipeline: Transfer-learning to a Chronolect", "authors": "Aynat Rubinstein, Avi Shmidman", "year": 2021, "url": "https://www.semanticscholar.org/paper/0a6d5e48aad3f08e7f10b39480f67b31c352bf66", "abstract": "A big unknown in Digital Humanities (DH) projects that seek to analyze previously untouched corpora is the question of how to adapt existing Natural Language Processing (NLP) resources to the specific nature of the target corpus. In this paper, we study the case of Emergent Modern Hebrew (EMH), an under-resourced chronolect of the Hebrew language. The resource we seek to adapt, a diacritizer, exists for both earlier and later chronolects of the language. Given a small annotated corpus of our target chronolect, we demonstrate that applying transfer-learning from either of the chronolects is preferable to training a new model from scratch. Furthermore, we consider just how much annotated data is necessary. For our task, we find that even a minimal corpus of 50K tokens provides a noticeable gain in accuracy. At the same time, we also evaluate accuracy at three additional increments, in order to quantify the gains that can be expected by investing in a larger annotated corpus.", "topic": "\"Hebrew NLP in Digital Humanities\""}, {"title": "The Possible, the Plausible, and the Desirable: Event-Based Modality Detection for Language Processing", "authors": "Valentina Pyatkin, Shoval Sadde, Aynat Rubinstein, P. Portner, Reut Tsarfaty", "year": 2021, "url": "https://www.semanticscholar.org/paper/94a7921fe01fe3a9b1f1f94bfb20e539b5dc1a48", "abstract": "Modality is the linguistic ability to describe vents with added information such as how desirable, plausible, or feasible they are. Modality is important for many NLP downstream tasks such as the detection of hedging, uncertainty, speculation, and more. Previous studies that address modality detection in NLP often restrict modal expressions to a closed syntactic class, and the modal sense labels are vastly different across different studies, lacking an accepted standard. Furthermore, these senses are often analyzed independently of the events that they modify. This work builds on the theoretical foundations of the Georgetown Gradable Modal Expressions (GME) work by Rubinstein et al. (2013) to propose an event-based modality detection task where modal expressions can be words of any syntactic class and sense labels are drawn from a comprehensive taxonomy which harmonizes the modal concepts contributed by the different studies. We present experiments on the GME corpus aiming to detect and classify fine-grained modal concepts and associate them with their modified events. We show that detecting and classifying modal expressions is not only feasible, it also improves the detection of modal events in their own right.", "topic": "\"Modal Classification in NLP\""}], "analysis": "1. Machine classification of modal meanings in language.\n2. Event-based modality detection in Natural Language Processing.\n3. Detection and classification of fine-grained modal concepts in text.", "author_name": "Aynat Rubinstein", "coauthors_histogram": {"E. Herburger": 1, "Luis Alonso-Ovalle": 1, "Paula Men\u00e9ndez-Benito": 1, "Valentina Pyatkin": 2, "Shoval Sadde": 2, "Reut Tsarfaty": 2, "P. Portner": 2, "Avi Shmidman": 1}, "paper_topics": {}, "cluster_trends": {"\"Semantic Evolution of 'Still'\"": {"2024": 1, "2021": 0, "2022": 0, "2023": 0}, "\"Modal Classification in NLP\"": {"2022": 1, "2021": 1, "2023": 0, "2024": 0}, "\"Hebrew NLP in Digital Humanities\"": {"2021": 1, "2022": 0, "2023": 0, "2024": 0}}, "index": "1"}, "84e89ef40db651cf5260f25db7975524": {"authorId": "1689080066", "start_year": 2020, "end_year": 2024, "papers": [{"title": "Class Distribution Shifts in Zero-Shot Learning: Learning Robust Representations", "authors": "Y. Slavutsky, Y. Benjamini", "year": 2023, "url": "https://www.semanticscholar.org/paper/158dfccf066ab2ca2fe540f25f91f1f2ae0419bd", "abstract": "Zero-shot learning methods typically assume that the new, unseen classes that are encountered at deployment, come from the same distribution as training classes. However, real-world scenarios often involve class distribution shifts (e.g., in age or gender for person identification), posing challenges for zero-shot classifiers that rely on learned representations from training classes. In this work, we propose a model that assumes that the attribute responsible for the shift is unknown in advance, and show that standard training may lead to non-robust representations. To mitigate this, we propose an algorithm for learning robust representations by (a) constructing synthetic data environments via hierarchical sampling and (b) applying environment balancing penalization, inspired by out-of-distribution problems. We show that our approach improves generalization on diverse class distributions in both simulations and real-world datasets.", "topic": "\"Robust Zero-Shot Learning\""}, {"title": "BitterMatch: recommendation systems for matching molecules with bitter taste receptors", "authors": "Eitan Margulis, Y. Slavutsky, Tatjana Lang, M. Behrens, Y. Benjamini, M. Niv", "year": 2022, "url": "https://www.semanticscholar.org/paper/578ea8f0c855eeeafcf6b0446e4eb1014769ebf0", "abstract": null, "topic": "Unknown"}, {"title": "Predicting Classification Accuracy when Adding New Unobserved Classes", "authors": "Y. Slavutsky, Y. Benjamini", "year": 2020, "url": "https://www.semanticscholar.org/paper/33391f5117b1e93548b0f297de1e36ee4a61157a", "abstract": "Multiclass classifiers are often designed and evaluated only on a sample from the classes on which they will eventually be applied. Hence, their final accuracy remains unknown. In this work we study how a classifier's performance over the initial class sample can be used to extrapolate its expected accuracy on a larger, unobserved set of classes. For this, we define a measure of separation between correct and incorrect classes that is independent of the number of classes: the reversed ROC (rROC), which is obtained by replacing the roles of classes and data-points in the common ROC. We show that the classification accuracy is a function of the rROC in multiclass classifiers, for which the learned representation of data from the initial class sample remains unchanged when new classes are added. Using these results we formulate a robust neural-network-based algorithm, CleaneX, which learns to estimate the accuracy of such classifiers on arbitrarily large sets of classes. Our method achieves remarkably better predictions than current state-of-the-art methods on both simulations and real datasets of object detection, face recognition, and brain decoding.", "topic": "\"Multiclass Classifier Prediction\""}, {"title": "Tracking COVID-19 using taste and smell loss Google searches is not a reliable strategy", "authors": "K. Asseo, F. Fierro, Y. Slavutsky, J. Frasnelli, M. Niv", "year": 2020, "url": "https://www.semanticscholar.org/paper/7a20688c157ff1863370968da5c82c5dcc6a678b", "abstract": null, "topic": "Unknown"}, {"title": "Utility and limitations of Google searches for tracking disease: the case of taste and smell loss as markers for COVID-19", "authors": "K. Asseo, F. Fierro, Y. Slavutsky, J. Frasnelli, M. Niv", "year": 2020, "url": "https://www.semanticscholar.org/paper/8f8d32e3982b7b69ebc465f02933c1b19d19d9dd", "abstract": "Web tools are widely used among population to obtain health related information, and these data are often employed for public health monitoring. Here we analyzed searches related to smell loss and taste loss, recently linked to COVID-19, as well as sight loss and hearing loss, not included in the COVID-19 symptoms list. Google Trends results per region (Italy) or state (United States) over several weeks were compared to the number of new cases prevalence in that geographical area. Taste and smell loss searches were correlated with each other, and, during a limited time window, with new COVID-19 cases. However, this correlation decreased with time, attributable, at least in part, to media coverage. As new symptoms are being discovered for COVID-19 and the pandemic continues to spread around the globe, the lesson learned here, that correlation between public interest in novel symptoms of infectious disease has an initial spike (the \u2033surprise rise\u2033) and subsequently goes to a new baseline due to \u2033knowledge saturation\u2033, is of general and practical value for the public.", "topic": "\"Google Trends for COVID-19 Symptom Tracking\""}, {"title": "Utility and limitations of Google searches on sensory loss as markers for new COVID-19 cases", "authors": "K. Asseo, F. Fierro, Y. Slavutsky, J. Frasnelli, M. Niv", "year": 2020, "url": "https://www.semanticscholar.org/paper/c65b38a16633a8616fbf59c709e9aa1fe6ea8cdb", "abstract": "Evidence of smell loss in COVID-19 is growing. Researchers and analysts have suggested to use Google searches on smell loss as indicators of COVID-19 cases. However, such searches may be due to interest elicited by media coverage of the COVID-19-related smell loss, rather than attempts to understand self-symptoms. We analyzed searches related to 4 senses: smell and taste (both recently shown to be impaired in some COVID-19 patients), vision and sight (senses not currently known to be impaired in COVID-19 patients), and an additional general control (\"COVID-19 symptoms\"). Focusing on two countries with a large number of cases, Italy and the United States, we have compared Google Trends results per region or state to the number of new cases prevalence in that region. The analysis was performed for each of the 8 weeks ranging from March 4th till April 28th. No correlation with vision loss or sight loss searches was identified, while taste and smell loss searches were correlated with new COVID-19 cases during a limited time window, that starts when the number of weekly new cases reached for the first time 21357 cases in Italy (11-17 March) and 47553 in the US (18-24 March). Media effect on the specific symptoms searches was also analyzed, establishing a different impact according to the country. Our results suggest that Google Trends for taste loss and smell loss searches captured a genuine connection between these symptoms and new COVID-19 cases prevalence in the population. However, due to variability in correlation from week to week, and overall decrease in correlation as taste and smell loss are becoming known COVID-19 symptoms, recognized now by CDC and World Health Organization, Google Trends is no longer a reliable marker for monitoring the disease spread. The \"surprise rise\" followed by decrease, probably attributable to knowledge saturation, should be kept in mind for future digital media analyses of potential new symptoms of COVID-19 or future pandemics.", "topic": "\"Google Trends for COVID-19 Symptom Tracking\""}], "analysis": "1. **Class distribution shifts in zero-shot learning**\n2. **Robust representation learning for zero-shot classifiers**\n3. **Extrapolating classifier accuracy to unobserved classes**\n4. **Reversed ROC (rROC) for multiclass classification accuracy prediction**\n5. **Neural-network-based accuracy estimation for multiclass classifiers**", "author_name": "Y. Slavutsky", "coauthors_histogram": {"Y. Benjamini": 3, "Eitan Margulis": 1, "Tatjana Lang": 1, "M. Behrens": 1, "M. Niv": 4, "K. Asseo": 3, "F. Fierro": 3, "J. Frasnelli": 3}, "paper_topics": {}, "cluster_trends": {"\"Robust Zero-Shot Learning\"": {"2023": 1, "2020": 0, "2021": 0, "2022": 0, "2024": 0}, "\"Multiclass Classifier Prediction\"": {"2020": 1, "2021": 0, "2022": 0, "2023": 0, "2024": 0}, "\"Google Trends for COVID-19 Symptom Tracking\"": {"2020": 2, "2021": 0, "2022": 0, "2023": 0, "2024": 0}}, "index": -1}, "7d21f337d91443e74bab05273e2ceffe": {"authorId": "3167681", "start_year": 2022, "end_year": 2024, "papers": [{"title": "A Surprising Failure? Multimodal LLMs and the NLVR Challenge", "authors": "Anne Wu, Kiant\u00e9 Brantley, Yoav Artzi", "year": 2024, "url": "https://www.semanticscholar.org/paper/e567d36ec1c8f4690fd8dec23b4ce5e804f0f4f4", "abstract": "This study evaluates three state-of-the-art MLLMs -- GPT-4V, Gemini Pro, and the open-source model IDEFICS -- on the compositional natural language vision reasoning task NLVR. Given a human-written sentence paired with a synthetic image, this task requires the model to determine the truth value of the sentence with respect to the image. Despite the strong performance demonstrated by these models, we observe they perform poorly on NLVR, which was constructed to require compositional and spatial reasoning, and to be robust for semantic and systematic biases.", "topic": "\"Multimodal Language and Vision Reasoning\""}, {"title": "IncDSI: Incrementally Updatable Document Retrieval", "authors": "Varsha Kishore, Chao-gang Wan, Justin Lovelace, Yoav Artzi, Kilian Q. Weinberger", "year": 2023, "url": "https://www.semanticscholar.org/paper/4c1015f70dfcdefead2905ebbd21993d68def3be", "abstract": "Differentiable Search Index is a recently proposed paradigm for document retrieval, that encodes information about a corpus of documents within the parameters of a neural network and directly maps queries to corresponding documents. These models have achieved state-of-the-art performances for document retrieval across many benchmarks. These kinds of models have a significant limitation: it is not easy to add new documents after a model is trained. We propose IncDSI, a method to add documents in real time (about 20-50ms per document), without retraining the model on the entire dataset (or even parts thereof). Instead we formulate the addition of documents as a constrained optimization problem that makes minimal changes to the network parameters. Although orders of magnitude faster, our approach is competitive with re-training the model on the whole dataset and enables the development of document retrieval systems that can be updated with new information in real-time. Our code for IncDSI is available at https://github.com/varshakishore/IncDSI.", "topic": "\"Dynamic AI Systems\""}, {"title": "Continually Improving Extractive QA via Human Feedback", "authors": "Ge Gao, Hung-Ting Chen, Yoav Artzi, Eunsol Choi", "year": 2023, "url": "https://www.semanticscholar.org/paper/627964b3dc7211f81902afeaa79d9357b7b2440c", "abstract": "We study continually improving an extractive question answering (QA) system via human user feedback. We design and deploy an iterative approach, where information-seeking users ask questions, receive model-predicted answers, and provide feedback. We conduct experiments involving thousands of user interactions under diverse setups to broaden the understanding of learning from feedback over time. Our experiments show effective improvement from user feedback of extractive QA models over time across different data regimes, including significant potential for domain adaptation.", "topic": "\"Dynamic AI Systems\""}, {"title": "A Joint Study of Phrase Grounding and Task Performance in Vision and Language Models", "authors": "Noriyuki Kojima, Hadar Averbuch-Elor, Yoav Artzi", "year": 2023, "url": "https://www.semanticscholar.org/paper/b38a634a2902a333cce1f97789df03ae3189ed76", "abstract": "Key to tasks that require reasoning about natural language in visual contexts is grounding words and phrases to image regions. However, observing this grounding in contemporary models is complex, even if it is generally expected to take place if the task is addressed in a way that is conductive to generalization. We propose a framework to jointly study task performance and phrase grounding, and propose three benchmarks to study the relation between the two. Our results show that contemporary models demonstrate inconsistency between their ability to ground phrases and solve tasks. We show how this can be addressed through brute-force training on ground phrasing annotations, and analyze the dynamics it creates. Code and at available at https://github.com/lil-lab/phrase_grounding.", "topic": "\"Multimodal Language and Vision Reasoning\""}, {"title": "CB2: Collaborative Natural Language Interaction Research Platform", "authors": "Jacob Sharf, Mustafa Omer Gul, Yoav Artzi", "year": 2023, "url": "https://www.semanticscholar.org/paper/c8a5ab637457db0e38636d9322117a341999519b", "abstract": "CB2 is a multi-agent platform to study collaborative natural language interaction in a grounded task-oriented scenario. It includes a 3D game environment, a backend server designed to serve trained models to human agents, and various tools and processes to enable scalable studies. We deploy CB2 at https://cb2.ai as a system demonstration with a learned instruction following model.", "topic": "\"Language Interaction in AI\""}, {"title": "Semantic uncertainty guides the extension of conventions to new referents", "authors": "Ron Eliav, Anya Ji, Yoav Artzi, Robert D. Hawkins", "year": 2023, "url": "https://www.semanticscholar.org/paper/dd0ca00e2561f18191b046366d4c5ea553aae069", "abstract": "A long tradition of studies in psycholinguistics has examined the formation and generalization of ad hoc conventions in reference games, showing how newly acquired conventions for a given target transfer to new referential contexts. However, another axis of generalization remains understudied: how do conventions formed for one target transfer to completely distinct targets, when specific lexical choices are unlikely to repeat? This paper presents two dyadic studies (N = 240) that address this axis of generalization, focusing on the role of nameability -- the a priori likelihood that two individuals will share the same label. We leverage the recently-released KiloGram dataset, a collection of abstract tangram images that is orders of magnitude larger than previously available, exhibiting high diversity of properties like nameability. Our first study asks how nameability shapes convention formation, while the second asks how new conventions generalize to entirely new targets of reference. Our results raise new questions about how ad hoc conventions extend beyond target-specific re-use of specific lexical choices.", "topic": "\"Language Interaction in AI\""}, {"title": "Simulating Bandit Learning from User Feedback for Extractive Question Answering", "authors": "Ge Gao, Eunsol Choi, Yoav Artzi", "year": 2022, "url": "https://www.semanticscholar.org/paper/56152658a3bb42e0afd0231e6fd942ea3dc3604f", "abstract": "We study learning from user feedback for extractive question answering by simulating feedback using supervised data. We cast the problem as contextual bandit learning, and analyze the characteristics of several learning scenarios with focus on reducing data annotation. We show that systems initially trained on few examples can dramatically improve given feedback from users on model-predicted answers, and that one can use existing datasets to deploy systems in new domains without any annotation effort, but instead improving the system on-the-fly via user feedback.", "topic": "\"Dynamic AI Systems\""}, {"title": "Proceedings of the Society for Computation in Linguistics", "authors": "Anna Effenberger, Eva Yan, Rhia Singh, Alane Suhr, Yoav Artzi", "year": 2022, "url": "https://www.semanticscholar.org/paper/59fb58648b18662f7ceb486d17155ca420bae0cf", "abstract": null, "topic": "Unknown"}, {"title": "Continual Learning for Instruction Following from Realtime Feedback", "authors": "Alane Suhr, Yoav Artzi", "year": 2022, "url": "https://www.semanticscholar.org/paper/6ba3e4172e5c22c8c3ace05a31e9b119a2e3c33c", "abstract": "We propose and deploy an approach to continually train an instruction-following agent from feedback provided by users during collaborative interactions. During interaction, human users instruct an agent using natural language, and provide realtime binary feedback as they observe the agent following their instructions. We design a contextual bandit learning approach, converting user feedback to immediate reward. We evaluate through thousands of human-agent interactions, demonstrating 15.4% absolute improvement in instruction execution accuracy over time. We also show our approach is robust to several design variations, and that the feedback signal is roughly equivalent to the learning signal of supervised demonstration data.", "topic": "\"Dynamic AI Systems\""}, {"title": "Wav2Seq: Pre-Training Speech-to-Text Encoder-Decoder Models Using Pseudo Languages", "authors": "Felix Wu, Kwangyoun Kim, Shinji Watanabe, Kyu J. Han, Ryan T. McDonald, Kilian Q. Weinberger, Yoav Artzi", "year": 2022, "url": "https://www.semanticscholar.org/paper/708f52050bf9dc134124e8b9e4dbc1ecc65f0401", "abstract": "We introduce Wav2Seq, the first self-supervised approach to pre-train both parts of encoder-decoder models for speech data. We induce a pseudo language as a compact discrete representation, and formulate a self-supervised pseudo speech recognition task \u2014 transcribing audio inputs into pseudo subword sequences. This process stands on its own, or can be applied as low-cost second-stage pre-training. We experiment with automatic speech recognition (ASR), spoken named entity recognition, and speech-to-text translation. We set new state-of-the-art results for end-to-end spoken named entity recognition, and show consistent improvements on 8 language pairs for speech-to-text translation, even when competing methods use additional text data for training. On ASR, our approach enables encoder-decoder methods to benefit from pre-training for all parts of the network, and shows comparable performance to highly optimized recent methods.", "topic": "\"Multimodal Language and Vision Reasoning\""}, {"title": "Why are NLP Models Fumbling at Elementary Math? A Survey of Automatic Word Problem Solvers", "authors": "Karl Cobbe, V. Kosaraju, Mo Bavarian, Jacob Hilton, Reiichiro Nakano, Chris Hesse, J. Devlin, Ming-Wei Chang, Kenton Lee, Robert Geirhos, J. Jacobsen, Richard Michaelis, Wieland Zemel, Brendel, Matthias Bethge, Felix A. Wichmann, Mohammad Javad Hosseini, Hanna Hajishirzi, Bugeun Kim, Kyung Seo Ki, Donggeon Lee, Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, Yoav Artzi, Luke Zettlemoyer, Shen-Yun Miao, Chao-Chun Liang, Keh-Yih Su, Shuai Peng, Ke Yuan, Liangcai Gao, Zhi Tang, Jeffrey Pennington, R. Socher, Matthew E. Peters, Mohit Iyyer Matt Mark Neumann, Christopher Gardner, Kenton Clark, Lee Luke, Piotr Pi\u02dbekos, M. Malinowski", "year": 2022, "url": "https://www.semanticscholar.org/paper/82b2ac18473b0d01c10cbd7cee45bbb2fa9b505b", "abstract": "From the latter half of the last decade, there 001 has been growing interest in developing algo-002 rithms for automatically solving mathematical 003 word problems (MWP). It is an exciting lan-004 guage problem which demands not only sur-005 face level text pattern recognition but requires 006 coupling with mathematical reasoning as well. 007 In spite of the dedicated effort, we are still 008 miles away from building robust representa-009 tions of elementary math word problems. In 010 this paper, we critically examine the various 011 models that have been developed for solving 012 word problems, their pros and cons and the 013 challenges ahead. In the last two years, a lot of 014 deep learning models have come out with com-015 peting results on benchmark datasets. We take 016 a step back and analyse why, in spite of this, 017 the predominantly used experiment and dataset 018 designs are a stumbling block and provide a 019 road-map for the future. 020", "topic": "\"Multimodal Language and Vision Reasoning\""}, {"title": "Abstract Visual Reasoning with Tangram Shapes", "authors": "Anya Ji, Noriyuki Kojima, N. Rush, Alane Suhr, Wai Keen Vong, Robert D. Hawkins, Yoav Artzi", "year": 2022, "url": "https://www.semanticscholar.org/paper/cdef738dd79364d7f12ccad4eef0f38e2d37dd1c", "abstract": "We introduce KiloGram, a resource for studying abstract visual reasoning in humans and machines. Drawing on the history of tangram puzzles as stimuli in cognitive science, we build a richly annotated dataset that, with >1k distinct stimuli, is orders of magnitude larger and more diverse than prior resources. It is both visually and linguistically richer, moving beyond whole shape descriptions to include segmentation maps and part labels. We use this resource to evaluate the abstract visual reasoning capacities of recent multi-modal models. We observe that pre-trained weights demonstrate limited abstract reasoning, which dramatically improves with fine-tuning. We also observe that explicitly describing parts aids abstract reasoning for both humans and models, especially when jointly encoding the linguistic and visual inputs.", "topic": "\"Multimodal Language and Vision Reasoning\""}, {"title": "lilGym: Natural Language Visual Reasoning with Reinforcement Learning", "authors": "Anne Wu, Kiant\u00e9 Brantley, Noriyuki Kojima, Yoav Artzi", "year": 2022, "url": "https://www.semanticscholar.org/paper/f8d18d5b655d8ac6a8fcda8afc6ce81f1e9c5768", "abstract": "We present lilGym, a new benchmark for language-conditioned reinforcement learning in visual environments. lilGym is based on 2,661 highly-compositional human-written natural language statements grounded in an interactive visual environment. We introduce a new approach for exact reward computation in every possible world state by annotating all statements with executable Python programs. Each statement is paired with multiple start states and reward functions to form thousands of distinct Markov Decision Processes of varying difficulty. We experiment with lilGym with different models and learning regimes. Our results and analysis show that while existing methods are able to achieve non-trivial performance, lilGym forms a challenging open problem. lilGym is available at https://lil.nlp.cornell.edu/lilgym/.", "topic": "\"Language Interaction in AI\""}], "analysis": "1. **Evaluation of multimodal language models on compositional vision reasoning tasks**\n2. **Phrase grounding and task performance in vision-language models**\n3. **Collaborative natural language interaction in multi-agent environments**\n4. **Generalization of ad hoc conventions in psycholinguistic reference games**\n5. **Continual learning for instruction-following agents from real-time user feedback**\n6. **Language-conditioned reinforcement learning in visual environments**", "author_name": "Yoav Artzi", "coauthors_histogram": {"Anne Wu": 2, "Kiant\u00e9 Brantley": 2, "Varsha Kishore": 1, "Chao-gang Wan": 1, "Justin Lovelace": 1, "Kilian Q. Weinberger": 2, "Ge Gao": 2, "Hung-Ting Chen": 1, "Eunsol Choi": 2, "Noriyuki Kojima": 3, "Hadar Averbuch-Elor": 1, "Jacob Sharf": 1, "Mustafa Omer Gul": 1, "Ron Eliav": 1, "Anya Ji": 2, "Robert D. Hawkins": 2, "Anna Effenberger": 1, "Eva Yan": 1, "Rhia Singh": 1, "Alane Suhr": 3, "Felix Wu": 1, "Kwangyoun Kim": 1, "Shinji Watanabe": 1, "Kyu J. Han": 1, "Ryan T. McDonald": 1, "Karl Cobbe": 1, "V. Kosaraju": 1, "Mo Bavarian": 1, "Jacob Hilton": 1, "Reiichiro Nakano": 1, "Chris Hesse": 1, "J. Devlin": 1, "Ming-Wei Chang": 1, "Kenton Lee": 1, "Robert Geirhos": 1, "J. Jacobsen": 1, "Richard Michaelis": 1, "Wieland Zemel": 1, "Brendel": 1, "Matthias Bethge": 1, "Felix A. Wichmann": 1, "Mohammad Javad Hosseini": 1, "Hanna Hajishirzi": 1, "Bugeun Kim": 1, "Kyung Seo Ki": 1, "Donggeon Lee": 1, "Rik Koncel-Kedziorski": 1, "Subhro Roy": 1, "Aida Amini": 1, "Nate Kushman": 1, "Luke Zettlemoyer": 1, "Shen-Yun Miao": 1, "Chao-Chun Liang": 1, "Keh-Yih Su": 1, "Shuai Peng": 1, "Ke Yuan": 1, "Liangcai Gao": 1, "Zhi Tang": 1, "Jeffrey Pennington": 1, "R. Socher": 1, "Matthew E. Peters": 1, "Mohit Iyyer Matt Mark Neumann": 1, "Christopher Gardner": 1, "Kenton Clark": 1, "Lee Luke": 1, "Piotr Pi\u02dbekos": 1, "M. Malinowski": 1, "N. Rush": 1, "Wai Keen Vong": 1}, "paper_topics": {}, "cluster_trends": {"\"Multimodal Language and Vision Reasoning\"": {"2024": 1, "2023": 1, "2022": 3}, "\"Dynamic AI Systems\"": {"2023": 2, "2022": 2, "2024": 0}, "\"Language Interaction in AI\"": {"2023": 2, "2022": 1, "2024": 0}}, "index": "1"}}